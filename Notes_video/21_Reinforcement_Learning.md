### [ğŸ“š] è§†é¢‘å­¦ä¹ è„šæ‰‹æ¶: Lecture 21: Reinforcement Learning

### ä¸€ã€æ ¸å¿ƒå†…å®¹å¤§çº² (Core Content Outline)

-   **å¯¼è®º (Introduction)**
    -   æœ¬èŠ‚è¯¾ä¸»é¢˜ï¼šå¼ºåŒ–å­¦ä¹  (Reinforcement Learning)
    -   å›é¡¾ä¹‹å‰è¯¾ç¨‹å†…å®¹ï¼š
        -   ç›‘ç£å­¦ä¹  (Supervised Learning)
            -   æ•°æ® (Data): (x, y)ï¼Œxæ˜¯æ•°æ®ï¼Œyæ˜¯æ ‡ç­¾
            -   ç›®æ ‡ (Goal): å­¦ä¹ ä¸€ä¸ªå‡½æ•° $f: x \to y$
            -   ä¾‹å­ (Examples): åˆ†ç±» (Classification)ã€å›å½’ (Regression)ã€ç›®æ ‡æ£€æµ‹ (Object Detection)ã€è¯­ä¹‰åˆ†å‰² (Semantic Segmentation)ã€å›¾åƒæ ‡æ³¨ (Image Captioning)
        -   æ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)
            -   æ•°æ® (Data): x (åªæœ‰æ•°æ®ï¼Œæ²¡æœ‰æ ‡ç­¾)
            -   ç›®æ ‡ (Goal): å­¦ä¹ æ•°æ®ä¸­æ½œåœ¨çš„éšè—ç»“æ„ (underlying hidden structure)
            -   ä¾‹å­ (Examples): èšç±» (Clustering)ã€é™ç»´ (Dimensionality Reduction)ã€ç‰¹å¾å­¦ä¹  (Feature Learning) (ä¾‹å¦‚ï¼šè‡ªç¼–ç å™¨ Autoencoders)ã€å¯†åº¦ä¼°è®¡ (Density Estimation)ã€ç”Ÿæˆæ¨¡å‹ (Generative Models)
-   **ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹  (What is Reinforcement Learning)?**
    -   é—®é¢˜å®šä¹‰ (Problem Definition):
        -   ä¸€ä¸ª**æ™ºèƒ½ä½“ (agent)** åœ¨**ç¯å¢ƒ (environment)** ä¸­æ‰§è¡Œ**åŠ¨ä½œ (actions)**ï¼Œå¹¶æ¥æ”¶**å¥–åŠ± (rewards)**ã€‚
        -   ç›®æ ‡ (Goal): å­¦ä¹ å¦‚ä½•é‡‡å–æœ€å¤§åŒ–å¥–åŠ±çš„åŠ¨ä½œ (Learn how to take actions that maximize reward)ã€‚
    -   å¼ºåŒ–å­¦ä¹ äº¤äº’å¾ªç¯ (Reinforcement Learning Interaction Loop):
        1.  ç¯å¢ƒ (Environment) å‘ æ™ºèƒ½ä½“ (Agent) å‘é€**çŠ¶æ€ ($s_t$) (State)**ã€‚
        2.  æ™ºèƒ½ä½“ (Agent) åŸºäºçŠ¶æ€é€‰æ‹©å¹¶æ‰§è¡Œ**åŠ¨ä½œ ($a_t$) (Action)**ã€‚
        3.  ç¯å¢ƒ (Environment) å‘ æ™ºèƒ½ä½“ (Agent) å‘é€**å¥–åŠ± ($r_t$) (Reward)** (è¡¨æ˜åŠ¨ä½œçš„å¥½å)ã€‚
        4.  åŠ¨ä½œå¯¼è‡´ç¯å¢ƒå˜åŒ–ï¼Œæ™ºèƒ½ä½“æ¥æ”¶å¥–åŠ±å¹¶å­¦ä¹  (Action causes environment change, Agent learns)ã€‚
        5.  è¿‡ç¨‹é‡å¤è¿›è¡Œ (Process repeats)ã€‚
-   **å¼ºåŒ–å­¦ä¹ çš„ä¾‹å­ (Examples of Reinforcement Learning)**
    -   **å°è½¦-æ†å­é—®é¢˜ (Cart-Pole Problem)**
        -   ç›®æ ‡ (Objective): å¹³è¡¡ç§»åŠ¨å°è½¦é¡¶éƒ¨çš„æ†å­ã€‚
        -   çŠ¶æ€ (State): æ†å­è§’åº¦ (angle)ã€è§’é€Ÿåº¦ (angular speed)ã€å°è½¦ä½ç½® (position)ã€æ°´å¹³é€Ÿåº¦ (horizontal velocity)ã€‚
        -   åŠ¨ä½œ (Action): å¯¹å°è½¦æ–½åŠ æ°´å¹³åŠ› (horizontal force)ã€‚
        -   å¥–åŠ± (Reward): æ¯ä¸€æ­¥æ†å­ä¿æŒç›´ç«‹å¾—åˆ° 1 åˆ†ã€‚
    -   **æœºå™¨äººè¿åŠ¨ (Robot Locomotion)**
        -   ç›®æ ‡ (Objective): ä½¿æœºå™¨äººå‘å‰ç§»åŠ¨ã€‚
        -   çŠ¶æ€ (State): æ‰€æœ‰å…³èŠ‚çš„è§’åº¦ (angle)ã€ä½ç½® (position)ã€é€Ÿåº¦ (velocity)ã€‚
        -   åŠ¨ä½œ (Action): æ–½åŠ åœ¨å…³èŠ‚ä¸Šçš„æ‰­çŸ© (Torques applied on joints)ã€‚
        -   å¥–åŠ± (Reward): æ¯ä¸€æ­¥ä¿æŒç›´ç«‹å’Œå‘å‰ç§»åŠ¨å¾—åˆ° 1 åˆ†ã€‚
    -   **Atari æ¸¸æˆ (Atari Games)**
        -   ç›®æ ‡ (Objective): ä»¥æœ€é«˜åˆ†å®Œæˆæ¸¸æˆã€‚
        -   çŠ¶æ€ (State): æ¸¸æˆç”»é¢çš„åŸå§‹åƒç´ è¾“å…¥ (raw pixel inputs of the game screen)ã€‚
        -   åŠ¨ä½œ (Action): æ¸¸æˆæ§åˆ¶ (Game controls) (ä¾‹å¦‚ï¼šå·¦ Leftã€å³ Rightã€ä¸Š Upã€ä¸‹ Down)ã€‚
        -   å¥–åŠ± (Reward): æ¯ä¸€æ­¥å¾—åˆ†çš„å¢åŠ /å‡å°‘ (Score increase/decrease)ã€‚
        -   æ³¨æ„ (Note): çŠ¶æ€å¯èƒ½æ˜¯ä¸å®Œå…¨ä¿¡æ¯ (noisy or incomplete information)ã€‚
    -   **å›´æ£‹ (Go)**
        -   ç›®æ ‡ (Objective): èµ¢å¾—æ¯”èµ›ã€‚
        -   çŠ¶æ€ (State): æ‰€æœ‰æ£‹å­çš„ä½ç½® (Position of all pieces)ã€‚
        -   åŠ¨ä½œ (Action): æ”¾ç½®ä¸‹ä¸€é¢—æ£‹å­çš„ä½ç½® (Where to put the next piece down)ã€‚
        -   å¥–åŠ± (Reward): åœ¨æœ€åä¸€å›åˆï¼šå¦‚æœèµ¢äº†å¾— 1 åˆ†ï¼Œå¦‚æœè¾“äº†å¾— 0 åˆ† (ç¨€ç–å¥–åŠ± Sparse Reward)ã€‚
-   **å¼ºåŒ–å­¦ä¹ ä¸ç›‘ç£å­¦ä¹ çš„å¯¹æ¯” (Reinforcement Learning vs. Supervised Learning)**
    -   **å…±åŒç‚¹ (Similarities)**: éƒ½å¯ä»¥ç”¨è¿­ä»£æ›´æ–°æ¨¡å‹ã€‚
    -   **ä¸»è¦åŒºåˆ« (Key Differences)**:
        1.  **éšæœºæ€§ (Stochasticity)**:
            -   RLä¸­ï¼Œå¥–åŠ±å’ŒçŠ¶æ€è½¬ç§»å¯èƒ½æ˜¯éšæœºçš„ (Rewards and state transitions may be random)ã€‚
            -   ç›‘ç£å­¦ä¹ ä¸­ï¼ŒæŸå¤±å‡½æ•°é€šå¸¸æ˜¯ç¡®å®šæ€§çš„ (Loss is typically deterministic)ã€‚
        2.  **ä¿¡ç”¨åˆ†é… (Credit Assignment)**:
            -   RLä¸­ï¼Œå¥–åŠ±å¯èƒ½ä¸æ˜¯å³æ—¶åé¦ˆï¼Œä¸€ä¸ªåŠ¨ä½œçš„å¥–åŠ±å¯èƒ½åœ¨å¾ˆä¹…ä¹‹åæ‰ä½“ç° (Reward may not directly depend on action, long-term dependency)ã€‚
            -   ç›‘ç£å­¦ä¹ ä¸­ï¼Œåé¦ˆæ˜¯å³æ—¶çš„ (Immediate feedback)ã€‚
        3.  **ä¸å¯å¾® (Nondifferentiable)**:
            -   RLä¸­ï¼Œæ— æ³•é€šè¿‡ç¯å¢ƒè¿›è¡Œåå‘ä¼ æ’­ (Cannot backpropagate through the world)ã€‚
            -   ç›‘ç£å­¦ä¹ é€šå¸¸é€šè¿‡æŸå¤±å‡½æ•°è®¡ç®—æ¢¯åº¦ (Compute gradients from loss function)ã€‚
        4.  **éå¹³ç¨³æ€§ (Nonstationary)**:
            -   æ™ºèƒ½ä½“æ‰€ç»å†çš„çŠ¶æ€å–å†³äºå…¶è‡ªèº«çš„è¡Œä¸ºï¼Œæ•°æ®åˆ†å¸ƒä¼šéšç€æ™ºèƒ½ä½“å­¦ä¹ å’Œç­–ç•¥å˜åŒ–è€Œå˜åŒ– (What the agent experiences depends on how it acts)ã€‚
            -   ç›‘ç£å­¦ä¹ é€šå¸¸å‡è®¾æ•°æ®åˆ†å¸ƒæ˜¯é™æ€çš„ (Static data distribution)ã€‚
-   **é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (Markov Decision Process - MDP)**
    -   å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„æ•°å­¦å½¢å¼åŒ– (Mathematical formalization of the RL problem)ã€‚
    -   **äº”å…ƒç»„ (A tuple)**: (S, A, R, P, $\gamma$)
        -   **S (Set of possible states)**: çŠ¶æ€ç©ºé—´ã€‚
        -   **A (Set of possible actions)**: åŠ¨ä½œç©ºé—´ã€‚
        -   **R (Distribution of reward given (state, action) pair)**: å¥–åŠ±å‡½æ•°ã€‚
        -   **P (Transition probability: distribution over next state given (state, action))**: çŠ¶æ€è½¬ç§»å‡½æ•°ã€‚
        -   **$\gamma$ (Discount factor)**: æŠ˜æ‰£å› å­ (tradeoff between future and present rewards)ã€‚
    -   **é©¬å°”å¯å¤«æ€§è´¨ (Markov Property)**: å½“å‰çŠ¶æ€å®Œå…¨åˆ»ç”»äº†ä¸–ç•Œçš„çŠ¶æ€ã€‚å¥–åŠ±å’Œä¸‹ä¸€ä¸ªçŠ¶æ€åªå–å†³äºå½“å‰çŠ¶æ€å’Œå½“å‰åŠ¨ä½œï¼Œè€Œä¸ä¹‹å‰çš„å†å²æ— å…³ã€‚
-   **ä»£ç†çš„ç­–ç•¥ (Agent's Policy)**
    -   æ™ºèƒ½ä½“æ‰§è¡Œä¸€ä¸ª**ç­–ç•¥ $\pi$ (policy $\pi$)**ï¼Œå®ƒç»™å‡ºåœ¨ç»™å®šçŠ¶æ€ä¸‹åŠ¨ä½œçš„åˆ†å¸ƒã€‚
    -   **ç®€å• MDP: ç½‘æ ¼ä¸–ç•Œ (Grid World)** ç¤ºä¾‹:
        -   **ç³Ÿç³•çš„ç­–ç•¥ (Bad policy)**: æ™ºèƒ½ä½“åœ¨æ¯ä¸ªçŠ¶æ€éƒ½éšæœºå‘ä¸Šæˆ–å‘ä¸‹ç§»åŠ¨ (50/50 æ¦‚ç‡)ï¼Œæ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥è¾¾åˆ°ç›®æ ‡çŠ¶æ€ã€‚
        -   **æœ€ä¼˜ç­–ç•¥ (Optimal Policy)**: æ™ºèƒ½ä½“åœ¨æ¯ä¸ªçŠ¶æ€éƒ½é€‰æ‹©æœ€èƒ½å¼•å¯¼å…¶è¾¾åˆ°ç›®æ ‡çŠ¶æ€çš„åŠ¨ä½œï¼Œä»¥æœ€å¤§åŒ–é¢„æœŸå¥–åŠ±ã€‚
    -   ç›®æ ‡ (Goal): æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ $\pi^*$ (optimal policy $\pi^*$)ï¼Œä½¿ç´¯è®¡æŠ˜æ‰£å¥–åŠ±æœ€å¤§åŒ– (maximizes (discounted) sum of rewards)ã€‚
        $\pi^* = \arg \max_{\pi} E \left[ \sum_{t \ge 0} \gamma^t r_t \right]$
        -   éšæœºæ€§æ¥æº (Sources of randomness): åˆå§‹çŠ¶æ€ ($s_0 \sim p(s_0)$)ã€åŠ¨ä½œé€‰æ‹© ($a_t \sim \pi(a | s_t)$)ã€çŠ¶æ€è½¬ç§» ($s_{t+1} \sim P(s | s_t, a_t)$) å’Œå¥–åŠ± ($r_t \sim R(r | s_t, a_t)$)ã€‚
-   **ä»·å€¼å‡½æ•° (Value Function) ä¸ Q å‡½æ•° (Q Function)**
    -   éµå¾ªç­–ç•¥ $\pi$ ä¼šäº§ç”Ÿæ ·æœ¬è½¨è¿¹ (æˆ–è·¯å¾„) (sample trajectories or paths)ï¼š$s_0, a_0, r_0, s_1, a_1, r_1, \dots$
    -   **çŠ¶æ€ä»·å€¼å‡½æ•° (Value function) $V^{\pi}(s)$**:
        -   è¡¡é‡ä¸€ä¸ªçŠ¶æ€æœ‰å¤šå¥½ (How good is a state)?
        -   å®šä¹‰ (Definition): $V^{\pi}(s) = E \left[ \sum_{t \ge 0} \gamma^t r_t | s_0 = s, \pi \right]$
        -   è¡¨ç¤ºä»çŠ¶æ€ $s$ å¼€å§‹å¹¶éµå¾ªç­–ç•¥ $\pi$ æ‰€æœŸæœ›çš„ç´¯è®¡å¥–åŠ±ã€‚
    -   **çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•° (Q function) $Q^{\pi}(s, a)$**:
        -   è¡¡é‡ä¸€ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹æœ‰å¤šå¥½ (How good is a state-action pair)?
        -   å®šä¹‰ (Definition): $Q^{\pi}(s, a) = E \left[ \sum_{t \ge 0} \gamma^t r_t | s_0 = s, a_0 = a, \pi \right]$
        -   è¡¨ç¤ºåœ¨çŠ¶æ€ $s$ é‡‡å–åŠ¨ä½œ $a$ åï¼Œå†éµå¾ªç­–ç•¥ $\pi$ æ‰€æœŸæœ›çš„ç´¯è®¡å¥–åŠ±ã€‚Q å‡½æ•°æ¯”ä»·å€¼å‡½æ•°åœ¨æ•°å­¦ä¸Šæ›´æ–¹ä¾¿ç”¨äºå­¦ä¹ ç®—æ³•ã€‚
-   **è´å°”æ›¼æ–¹ç¨‹ (Bellman Equation)**
    -   **æœ€ä¼˜ Q å‡½æ•° (Optimal Q-function) $Q^*(s, a)$**:
        -   æ˜¯ä¼˜åŒ–ç­–ç•¥ $\pi^*$ çš„ Q å‡½æ•°ã€‚å®ƒè¡¨ç¤ºåœ¨çŠ¶æ€ $s$ é‡‡å–åŠ¨ä½œ $a$ åèƒ½è·å¾—çš„æœ€å¤§å¯èƒ½æœªæ¥å¥–åŠ± (It gives the max possible future reward when taking action a in state s)ã€‚
        -   $Q^*(s, a) = \max_{\pi} E \left[ \sum_{t \ge 0} \gamma^t r_t | s_0 = s, a_0 = a, \pi \right]$
        -   **$Q^*$ ç¼–ç æœ€ä¼˜ç­–ç•¥ (encodes the optimal policy)**: $\pi^*(s) = \arg \max_{a'} Q(s, a')$
    -   **è´å°”æ›¼æ–¹ç¨‹ (Bellman Equation) - é€’å½’å…³ç³» (recurrence relation)**:
        -   $Q^*(s, a) = E_{r,s'} [r + \gamma \max_{a'} Q^*(s', a')]$
            -   å…¶ä¸­ $r \sim R(s, a)$, $s' \sim P(s, a)$
        -   **ç›´è§‚ç†è§£ (Intuition)**: åœ¨çŠ¶æ€ $s$ é‡‡å–åŠ¨ä½œ $a$ åï¼Œæˆ‘ä»¬è·å¾—å¥–åŠ± $r$ å¹¶ç§»åŠ¨åˆ°æ–°çŠ¶æ€ $s'$ã€‚ä¹‹åï¼Œæˆ‘ä»¬èƒ½è·å¾—çš„æœ€å¤§å¯èƒ½å¥–åŠ±æ˜¯ $\max_{a'} Q^*(s', a')$ã€‚
-   **æ±‚è§£æœ€ä¼˜ç­–ç•¥ï¼šä»·å€¼è¿­ä»£ (Solving for the Optimal Policy: Value Iteration)**
    -   æ€æƒ³ (Idea): å¦‚æœæˆ‘ä»¬æ‰¾åˆ°ä¸€ä¸ªå‡½æ•° $Q(s, a)$ æ»¡è¶³è´å°”æ›¼æ–¹ç¨‹ï¼Œé‚£ä¹ˆå®ƒä¸€å®šæ˜¯ $Q^*$ã€‚
    -   å¼€å§‹æ—¶ä½¿ç”¨éšæœºçš„ $Q$ å‡½æ•°ï¼Œå¹¶ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹ä½œä¸ºæ›´æ–°è§„åˆ™ (Start with a random $Q$, and use the Bellman Equation as an update rule):
        -   $Q_{i+1}(s, a) = E_{r,s'} [r + \gamma \max_{a'} Q_i(s', a')]$
            -   å…¶ä¸­ $r \sim R(s, a)$, $s' \sim P(s, a)$
    -   æƒŠäººçš„äº‹å® (Amazing fact): å½“ $i \to \infty$ æ—¶ï¼Œ$Q_i$ æ”¶æ•›äº $Q^*$ã€‚
    -   é—®é¢˜ (Problem): éœ€è¦è·Ÿè¸ªæ‰€æœ‰ (çŠ¶æ€, åŠ¨ä½œ) å¯¹çš„ $Q(s, a)$ å€¼ï¼Œå¦‚æœçŠ¶æ€ç©ºé—´æ— é™åˆ™ä¸å¯èƒ½ (Need to keep track of $Q(s, a)$ for all (state, action) pairs â€“ impossible if infinite)ã€‚
-   **æ±‚è§£æœ€ä¼˜ç­–ç•¥ï¼šæ·±åº¦ Q å­¦ä¹  (Solving for the Optimal Policy: Deep Q-Learning)**
    -   è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œ (å¸¦æœ‰æƒé‡ $\theta$) æ¥è¿‘ä¼¼ $Q^*$ (Train a neural network (with weights $\theta$) to approximate $Q^*$): $Q^*(s, a) \approx Q(s, a; \theta)$ã€‚
    -   ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹æ¥å®šä¹‰è®­ç»ƒ $Q$ çš„æŸå¤±å‡½æ•° (Use Bellman Equation to define loss function for training $Q$):
        -   $y_{s,a,\theta} = E_{r,s'} [r + \gamma \max_{a'} Q(s', a'; \theta')]$ (ç›®æ ‡å€¼ï¼Œç”±ç›®æ ‡ç½‘ç»œ $\theta'$ è®¡ç®—)
        -   æŸå¤±å‡½æ•° (Loss function): $L(s, a) = (Q(s, a; \theta) - y_{s,a,\theta})^2$
    -   é—®é¢˜ (Problem): éå¹³ç¨³æ€§ (Nonstationary)! $Q(s, a)$ çš„â€œç›®æ ‡â€å–å†³äºå½“å‰çš„æƒé‡ $\theta$ (The "target" for $Q(s, a)$ depends on the current weights $\theta$!)ã€‚
    -   é—®é¢˜ (Problem): å¦‚ä½•é‡‡æ ·æ‰¹æ•°æ®è¿›è¡Œè®­ç»ƒï¼Ÿ(How to sample batches of data for training?)
    -   **æ¡ˆä¾‹ç ”ç©¶ï¼šç© Atari æ¸¸æˆ (Case Study: Playing Atari Games)**
        -   ç½‘ç»œè¾“å…¥ (Network input): çŠ¶æ€ $s_t$: æœ€å 4 å¸§çš„ $4 \times 84 \times 84$ å †å å›¾åƒ (ç»è¿‡ RGB->ç°åº¦è½¬æ¢ã€ä¸‹é‡‡æ ·å’Œè£å‰ªå)ã€‚
        -   ç½‘ç»œè¾“å‡º (Network output): æ‰€æœ‰åŠ¨ä½œçš„ Q å€¼ (Q-values for all actions)ã€‚
        -   ç½‘ç»œæ¶æ„ (Network architecture): Conv(4->16, 8x8, stride 4) -> Conv(16->32, 4x4, stride 2) -> FC-256 -> FC-A (Q-values)ã€‚
        -   ç»“æœ (Results): ç»è¿‡è®­ç»ƒï¼Œç®—æ³•èƒ½åƒä¸“å®¶ä¸€æ ·ç© Atari Breakout æ¸¸æˆï¼Œç”šè‡³èƒ½å‘ç°äººç±»ç©å®¶æœªæ›¾å‘ç°çš„ç­–ç•¥ã€‚
-   **Q-Learning vs ç­–ç•¥æ¢¯åº¦ (Policy Gradients)**
    -   Q-Learning: è®­ç»ƒç½‘ç»œ $Q_{\theta}(s, a)$ æ¥ä¼°è®¡æ¯ä¸ª (çŠ¶æ€, åŠ¨ä½œ) å¯¹çš„æœªæ¥å¥–åŠ±ã€‚
        -   é—®é¢˜ (Problem): å¯¹äºæŸäº›é—®é¢˜ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªéš¾ä»¥å­¦ä¹ çš„å‡½æ•°ã€‚
    -   ç­–ç•¥æ¢¯åº¦ (Policy Gradients): è®­ç»ƒä¸€ä¸ªç½‘ç»œ $\pi_{\theta}(a | s)$ï¼Œå®ƒå°†çŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç»™å‡ºåœ¨è¯¥çŠ¶æ€ä¸‹é‡‡å–å“ªä¸ªåŠ¨ä½œçš„åˆ†å¸ƒã€‚
    -   ç›®æ ‡å‡½æ•° (Objective function): éµå¾ªç­–ç•¥ $\pi_{\theta}$ æ—¶çš„æœŸæœ›æœªæ¥å¥–åŠ± (Expected future rewards when following policy $\pi_{\theta}$):
        -   $J(\theta) = E_{x \sim p_{\theta}}[ \sum_{t \ge 0} \gamma^t r_t ]$
    -   é€šè¿‡æœ€å¤§åŒ–æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ (Find the optimal policy by maximizing): $\theta^* = \arg \max_{\theta} J(\theta)$ã€‚
        -   ä½¿ç”¨æ¢¯åº¦ä¸Šå‡ (Use gradient ascent!)ã€‚
    -   é—®é¢˜ (Problem): ä¸å¯å¾®æ€§ (Nondifferentiability)! ä¸çŸ¥é“å¦‚ä½•è®¡ç®— $\frac{\partial J}{\partial \theta}$ã€‚
    -   **REINFORCE ç®—æ³• (REINFORCE Algorithm)**:
        -   **æ•°å­¦æ¨å¯¼ (Mathematical Derivation)**:
            -   $\frac{\partial J}{\partial \theta} = E_{x \sim p_{\theta}} [ (\sum_{t \ge 0} \gamma^t r_t) \sum_{t \ge 0} \frac{\partial}{\partial \theta} \log \pi_{\theta}(a_t | s_t) ]$
            -   è¿™ä¸ªå…¬å¼å°†æ¢¯åº¦è®¡ç®—è½¬æ¢ä¸ºæœŸæœ›ï¼Œå¯ä»¥é€šè¿‡é‡‡æ ·è½¨è¿¹æ¥è¿‘ä¼¼ã€‚
        -   **ç®—æ³•æ­¥éª¤ (Algorithm Steps)**:
            1.  éšæœºåˆå§‹åŒ–æƒé‡ $\theta$ (Initialize random weights $\theta$)ã€‚
            2.  é€šè¿‡è¿è¡Œç­–ç•¥ $\pi_{\theta}$ åœ¨ç¯å¢ƒä¸­æ”¶é›†è½¨è¿¹ $x$ å’Œå¥–åŠ± $f(x)$ (Collect trajectories $x$ and rewards $f(x)$ using policy $\pi_{\theta}$)ã€‚
            3.  è®¡ç®— $\frac{\partial J}{\partial \theta}$ (Compute $\frac{\partial J}{\partial \theta}$)ã€‚
            4.  å¯¹ $\theta$ æ‰§è¡Œæ¢¯åº¦ä¸Šå‡æ­¥ (Gradient ascent step on $\theta$)ã€‚
            5.  è·³è½¬åˆ°æ­¥éª¤ 2 (GOTO 2)ã€‚
        -   **ç›´è§‚ç†è§£ (Intuition)**:
            -   å½“ $f(x)$ å¾ˆé«˜ (é«˜å¥–åŠ±) æ—¶ï¼šå¢åŠ æˆ‘ä»¬é‡‡å–çš„åŠ¨ä½œçš„æ¦‚ç‡ (Increase the probability of the actions we took)ã€‚
            -   å½“ $f(x)$ å¾ˆä½ (ä½å¥–åŠ±) æ—¶ï¼šé™ä½æˆ‘ä»¬é‡‡å–çš„åŠ¨ä½œçš„æ¦‚ç‡ (Decrease the probability of the actions we took)ã€‚
        -   **é—®é¢˜ (Problem)**: æ¢¯åº¦ä¼°è®¡å™¨æ–¹å·®å¤§ (High variance of gradient estimator)ã€‚
        -   **æ”¹è¿›ç­–ç•¥æ¢¯åº¦ (Improving Policy Gradients)**: æ·»åŠ åŸºçº¿ (Add baseline) ä»¥å‡å°‘æ¢¯åº¦ä¼°è®¡å™¨çš„æ–¹å·®ã€‚
-   **å…¶ä»–æ–¹æ³•ï¼šåŸºäºæ¨¡å‹å¼ºåŒ–å­¦ä¹  (Other approaches: Model Based RL)**
    -   **Actor-Critic (æ¼”å‘˜-è¯„è®ºå®¶)**: è®­ç»ƒä¸€ä¸ªé¢„æµ‹åŠ¨ä½œçš„æ¼”å‘˜ (actor) (å¦‚ç­–ç•¥æ¢¯åº¦) å’Œä¸€ä¸ªé¢„æµ‹ä»è¿™äº›åŠ¨ä½œä¸­è·å¾—çš„æœªæ¥å¥–åŠ±çš„è¯„è®ºå®¶ (critic) (å¦‚ Q-Learning)ã€‚
    -   **åŸºäºæ¨¡å‹ (Model-Based)**: å­¦ä¹ ä¸–ç•Œçš„çŠ¶æ€è½¬ç§»å‡½æ•° $P(s_{t+1} | s_t, a_t)$ çš„æ¨¡å‹ï¼Œç„¶åé€šè¿‡æ¨¡å‹è¿›è¡Œè§„åˆ’ä»¥åšå‡ºå†³ç­–ã€‚
    -   **æ¨¡ä»¿å­¦ä¹  (Imitation Learning)**: æ”¶é›†ä¸“å®¶åœ¨ç¯å¢ƒä¸­æ‰§è¡Œæ“ä½œçš„æ•°æ®ï¼›å­¦ä¹ ä¸€ä¸ªå‡½æ•°æ¥æ¨¡ä»¿ä»–ä»¬çš„è¡Œä¸º (ç›‘ç£å­¦ä¹ æ–¹æ³•)ã€‚
    -   **é€†å¼ºåŒ–å­¦ä¹  (Inverse Reinforcement Learning)**: æ”¶é›†ä¸“å®¶åœ¨ç¯å¢ƒä¸­æ‰§è¡Œæ“ä½œçš„æ•°æ®ï¼›å­¦ä¹ ä¸€ä¸ªä»–ä»¬ä¼¼ä¹æ­£åœ¨ä¼˜åŒ–çš„å¥–åŠ±å‡½æ•°ï¼Œç„¶ååˆ©ç”¨è¯¥å¥–åŠ±å‡½æ•°è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚
    -   **å¯¹æŠ—æ€§å­¦ä¹  (Adversarial Learning)**: å­¦ä¹ ä¸€ä¸ªé‰´åˆ«å™¨æ¥åˆ¤æ–­åŠ¨ä½œæ˜¯çœŸå®çš„è¿˜æ˜¯å‡çš„ (é€šå¸¸ç”¨äºç”Ÿæˆå¯¹æŠ—ç½‘ç»œ)ã€‚
-   **æ¡ˆä¾‹ç ”ç©¶ï¼šç©æ¸¸æˆ (Case Study: Playing Games)** (æœ€æ–°è¿›å±•)
    -   **AlphaGo (2016 å¹´ 1 æœˆ)**:
        -   ä½¿ç”¨æ¨¡ä»¿å­¦ä¹  (Imitation Learning) + æ ‘æœç´¢ (Tree Search) + å¼ºåŒ–å­¦ä¹  (RL)ã€‚
        -   å‡»è´¥äº† 18 å±Šå›´æ£‹ä¸–ç•Œå† å†›æä¸–çŸ³ (Lee Sedol)ã€‚
    -   **AlphaGo Zero (2017 å¹´ 10 æœˆ)**:
        -   AlphaGo çš„ç®€åŒ–ç‰ˆæœ¬ã€‚
        -   ä¸å†ä½¿ç”¨æ¨¡ä»¿å­¦ä¹  (No longer using imitation learning)ã€‚
        -   å‡»è´¥äº†å½“æ—¶æ’åç¬¬ä¸€çš„æŸ¯æ´ (Ke Jie)ã€‚
    -   **Alpha Zero (2018 å¹´ 12 æœˆ)**:
        -   æ¨å¹¿åˆ°å…¶ä»–æ¸¸æˆï¼šå›½é™…è±¡æ£‹ (Chess) å’Œå°†æ£‹ (Shogi)ã€‚
    -   **MuZero (2019 å¹´ 11 æœˆ)**:
        -   é€šè¿‡æ¸¸æˆçš„å­¦ä¹ æ¨¡å‹è¿›è¡Œè§„åˆ’ (Plans through a learned model of the game)ã€‚
        -   æä¸–çŸ³äº 2019 å¹´ 11 æœˆå®£å¸ƒé€€å½¹ï¼Œä»–è¡¨ç¤ºï¼šâ€œéšç€å›´æ£‹ AI çš„å‡ºç°ï¼Œæˆ‘æ„è¯†åˆ°å³ä½¿æˆ‘é€šè¿‡ä¸æ‡ˆåŠªåŠ›æˆä¸ºç¬¬ä¸€ï¼Œæˆ‘ä¹Ÿä¸æ˜¯å·…å³°ã€‚å³ä½¿æˆ‘æˆä¸ºç¬¬ä¸€ï¼Œä¹Ÿæœ‰ä¸€ä¸ªæ— æ³•è¢«å‡»è´¥çš„å®ä½“ã€‚â€
    -   **æ›´å¤æ‚çš„æ¸¸æˆ (More Complex Games)**:
        -   StarCraft II: AlphaStar (2019 å¹´ 10 æœˆ): ä½¿ç”¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è¾¾åˆ°æ˜Ÿé™…äº‰éœ¸ II çš„å®—å¸ˆçº§æ°´å¹³ã€‚
        -   Dota 2: OpenAI Five (2019 å¹´ 4 æœˆ): å­¦ä¹ ç© Dota 2 æ¸¸æˆã€‚
-   **å¼ºåŒ–å­¦ä¹ ï¼šä¸ä¸–ç•Œäº¤äº’ (Reinforcement Learning: Interacting With World)**
    -   é€šå¸¸ä½¿ç”¨ RL æ¥è®­ç»ƒä¸ (å™ªå£°ã€ä¸å¯å¾®åˆ†çš„) ç¯å¢ƒäº¤äº’çš„æ™ºèƒ½ä½“ (agents)ã€‚
    -   **å¼ºåŒ–å­¦ä¹ ï¼šéšæœºè®¡ç®—å›¾ (Reinforcement Learning: Stochastic Computation Graphs)**
        -   RL è¿˜å¯ä»¥ç”¨æ¥è®­ç»ƒå…·æœ‰ä¸å¯å¾®åˆ†ç»„ä»¶çš„ç¥ç»ç½‘ç»œï¼
        -   **ç¤ºä¾‹ (Example)**: å°å‹â€œè·¯ç”±â€ç½‘ç»œå°†å›¾åƒå‘é€åˆ° K ä¸ªç½‘ç»œä¹‹ä¸€ã€‚
            -   ç¬¬ä¸€ä¸ª CNN è¾“å‡ºåº”è¯¥ä½¿ç”¨å“ªä¸ªç½‘ç»œçš„æ¦‚ç‡ã€‚
            -   ä»è¯¥åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªç½‘ç»œ (ä¾‹å¦‚ï¼Œç»¿è‰²ç½‘ç»œ)ã€‚
            -   å°†å›¾åƒè¾“å…¥åˆ°é‡‡æ ·çš„ç½‘ç»œï¼Œå¾—åˆ°æŸå¤±ã€‚
            -   å°†è¯¥æŸå¤±ä½œä¸ºå¥–åŠ±ï¼Œå¹¶ä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ›´æ–°ç¬¬ä¸€ä¸ªâ€œè·¯ç”±â€ç½‘ç»œã€‚

### äºŒã€å…³é”®æœ¯è¯­å®šä¹‰ (Key Term Definitions)

-   **å¼ºåŒ–å­¦ä¹  (Reinforcement Learning)**: ä¸€ç§æœºå™¨å­¦ä¹ èŒƒå¼ï¼Œå…¶ä¸­æ™ºèƒ½ä½“é€šè¿‡åœ¨ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œå¹¶æ¥æ”¶å¥–åŠ±æ¥å­¦ä¹ å¦‚ä½•æœ€å¤§åŒ–é•¿æœŸå›æŠ¥ã€‚
-   **æ™ºèƒ½ä½“ (Agent)**: åœ¨ç¯å¢ƒä¸­é‡‡å–è¡ŒåŠ¨å¹¶å­¦ä¹ çš„å®ä½“ã€‚
-   **ç¯å¢ƒ (Environment)**: æ™ºèƒ½ä½“ä¸ä¹‹äº¤äº’çš„ç³»ç»Ÿæˆ–ä¸–ç•Œã€‚
-   **çŠ¶æ€ (State)**: ç¯å¢ƒåœ¨æŸä¸ªæ—¶é—´ç‚¹çš„æè¿°ï¼Œæ™ºèƒ½ä½“é€šè¿‡å®ƒæ„ŸçŸ¥ä¸–ç•Œã€‚
-   **åŠ¨ä½œ (Action)**: æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­å¯ä»¥æ‰§è¡Œçš„æ“ä½œã€‚
-   **å¥–åŠ± (Reward)**: ç¯å¢ƒå¯¹æ™ºèƒ½ä½“åŠ¨ä½œçš„å³æ—¶åé¦ˆï¼Œè¡¨ç¤ºè¯¥åŠ¨ä½œçš„å¥½åã€‚
-   **ç­–ç•¥ (Policy) ($\pi$)**: æ™ºèƒ½ä½“åœ¨ç»™å®šçŠ¶æ€ä¸‹é€‰æ‹©åŠ¨ä½œçš„è§„åˆ™æˆ–å‡½æ•°ã€‚é€šå¸¸è¡¨ç¤ºä¸º $\pi(a|s)$ï¼Œå³åœ¨çŠ¶æ€ $s$ ä¸‹é‡‡å–åŠ¨ä½œ $a$ çš„æ¦‚ç‡ã€‚
-   **æŠ˜æ‰£å› å­ (Discount Factor) ($\gamma$)**: ä¸€ä¸ªä»‹äº 0 å’Œ 1 ä¹‹é—´çš„å€¼ï¼Œç”¨äºæƒè¡¡æœªæ¥å¥–åŠ±çš„é‡è¦æ€§ã€‚
-   **é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (Markov Decision Process - MDP)**: å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„æ•°å­¦å½¢å¼åŒ–ï¼Œç”±çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€çŠ¶æ€è½¬ç§»å‡½æ•°å’ŒæŠ˜æ‰£å› å­å®šä¹‰ã€‚
-   **é©¬å°”å¯å¤«æ€§è´¨ (Markov Property)**: ç³»ç»Ÿçš„æœªæ¥çŠ¶æ€ä»…å–å†³äºå½“å‰çŠ¶æ€å’Œå½“å‰åŠ¨ä½œï¼Œè€Œä¸ä¹‹å‰çš„å†å²æ— å…³ã€‚
-   **éšæœºæ€§ (Stochasticity)**: ç¯å¢ƒçš„ç‰¹æ€§ï¼ŒæŒ‡å¥–åŠ±å’ŒçŠ¶æ€è½¬ç§»å¯èƒ½åŒ…å«éšæœºæ€§ã€‚
-   **ä¿¡ç”¨åˆ†é… (Credit Assignment)**: åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå°†æœ€ç»ˆçš„å¥–åŠ±å½’å› äºå¯¼è‡´è¯¥å¥–åŠ±çš„ä¸€ç³»åˆ—åŠ¨ä½œå’ŒçŠ¶æ€çš„è¿‡ç¨‹ã€‚
-   **ä¸å¯å¾® (Nondifferentiable)**: æŒ‡ç¯å¢ƒçš„åŠ¨æ€æˆ–å¥–åŠ±å‡½æ•°å¯èƒ½æ— æ³•é€šè¿‡ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œæ±‚å¯¼ã€‚
-   **éå¹³ç¨³æ€§ (Nonstationary)**: æŒ‡åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ™ºèƒ½ä½“å­¦ä¹ çš„æ•°æ®åˆ†å¸ƒä¼šéšç€æ™ºèƒ½ä½“ç­–ç•¥çš„å˜åŒ–è€Œå˜åŒ–ã€‚
-   **ä»·å€¼å‡½æ•° (Value Function) ($V^{\pi}(s)$)**: è¡¡é‡ä»æŸä¸ªçŠ¶æ€ $s$ å¼€å§‹å¹¶éµå¾ªç­–ç•¥ $\pi$ æ‰€èƒ½è·å¾—çš„æœŸæœ›ç´¯è®¡å¥–åŠ±ã€‚
-   **Q å‡½æ•° (Q Function) ($Q^{\pi}(s, a)$)**: è¡¡é‡åœ¨æŸä¸ªçŠ¶æ€ $s$ é‡‡å–ç‰¹å®šåŠ¨ä½œ $a$ åï¼Œå†éµå¾ªç­–ç•¥ $\pi$ æ‰€èƒ½è·å¾—çš„æœŸæœ›ç´¯è®¡å¥–åŠ±ã€‚
-   **è´å°”æ›¼æ–¹ç¨‹ (Bellman Equation)**: å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸€ç»„æ ¸å¿ƒæ–¹ç¨‹ï¼Œç”¨äºæè¿°ä»·å€¼å‡½æ•°æˆ– Q å‡½æ•°ä¸å…¶æœªæ¥å€¼ä¹‹é—´çš„å…³ç³»ï¼Œæ˜¯åŠ¨æ€è§„åˆ’å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„åŸºç¡€ã€‚
-   **æœ€ä¼˜ Q å‡½æ•° (Optimal Q-function) ($Q^*(s, a)$)**: ä»£è¡¨é€šè¿‡é‡‡å–æœ€ä¼˜è¡ŒåŠ¨æ‰€èƒ½è¾¾åˆ°çš„æœ€å¤§æœŸæœ›å¥–åŠ±ã€‚
-   **ä»·å€¼è¿­ä»£ (Value Iteration)**: ä¸€ç§æ±‚è§£æœ€ä¼˜ç­–ç•¥çš„ç®—æ³•ï¼Œé€šè¿‡è¿­ä»£æ›´æ–° Q å‡½æ•°ç›´åˆ°æ”¶æ•›åˆ°æœ€ä¼˜ Q å‡½æ•°ã€‚
-   **æ·±åº¦ Q å­¦ä¹  (Deep Q-Learning - DQN)**: ç»“åˆæ·±åº¦ç¥ç»ç½‘ç»œå’Œ Q-Learning çš„æ–¹æ³•ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè¿‘ä¼¼ Q å‡½æ•°ã€‚
-   **ç­–ç•¥æ¢¯åº¦ (Policy Gradients)**: ä¸€ç±»å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç›´æ¥ä¼˜åŒ–ç­–ç•¥å‡½æ•°ï¼Œé€šå¸¸é€šè¿‡æ¢¯åº¦ä¸Šå‡æ¥æœ€å¤§åŒ–æœŸæœ›å›æŠ¥ã€‚
-   **REINFORCE ç®—æ³• (REINFORCE Algorithm)**: ä¸€ç§åŸºäºç­–ç•¥æ¢¯åº¦çš„è’™ç‰¹å¡æ´›ç®—æ³•ï¼Œé€šè¿‡é‡‡æ ·è½¨è¿¹æ¥ä¼°è®¡ç­–ç•¥æ¢¯åº¦ã€‚
-   **åŸºçº¿ (Baseline)**: åœ¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸­ï¼Œç”¨äºå‡å°‘æ¢¯åº¦ä¼°è®¡å™¨æ–¹å·®çš„å‚è€ƒå€¼ã€‚
-   **Actor-Critic (æ¼”å‘˜-è¯„è®ºå®¶)**: ä¸€ç±»å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç»“åˆäº†ç­–ç•¥æ¢¯åº¦ (actor) å’Œ Q-Learning (critic) çš„æ€æƒ³ã€‚
-   **åŸºäºæ¨¡å‹å¼ºåŒ–å­¦ä¹  (Model-Based Reinforcement Learning)**: å­¦ä¹ ç¯å¢ƒçš„åŠ¨æ€æ¨¡å‹ï¼Œç„¶ååˆ©ç”¨è¯¥æ¨¡å‹è¿›è¡Œè§„åˆ’æ¥åšå‡ºå†³ç­–ã€‚
-   **æ¨¡ä»¿å­¦ä¹  (Imitation Learning)**: ä»ä¸“å®¶æ¼”ç¤ºä¸­å­¦ä¹ ç­–ç•¥ï¼Œé€šå¸¸é‡‡ç”¨ç›‘ç£å­¦ä¹ çš„æ–¹å¼ã€‚
-   **é€†å¼ºåŒ–å­¦ä¹  (Inverse Reinforcement Learning)**: ä»ä¸“å®¶è¡Œä¸ºä¸­æ¨æ–­å…¶æ½œåœ¨çš„å¥–åŠ±å‡½æ•°ã€‚
-   **éšæœºè®¡ç®—å›¾ (Stochastic Computation Graphs)**: å…è®¸åœ¨ç¥ç»ç½‘ç»œä¸­åŒ…å«éšæœºçš„ã€é€šå¸¸æ˜¯ä¸å¯å¾®åˆ†çš„ç»„ä»¶ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æŠ€æœ¯è¿›è¡Œè®­ç»ƒã€‚
-   **ç¡¬æ³¨æ„åŠ› (Hard Attention)**: åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œæ¨¡å‹æ˜ç¡®åœ°ä»ä¸€ç»„ç¦»æ•£çš„ä½ç½®ä¸­é€‰æ‹©ä¸€ä¸ªè¿›è¡Œå…³æ³¨ï¼Œè¿™é€šå¸¸æ˜¯ä¸å¯å¾®åˆ†çš„æ“ä½œï¼Œéœ€è¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚

### ä¸‰ã€æ ¸å¿ƒç®—æ³•ä¸ä»£ç ç‰‡æ®µ (Core Algorithms & Code Snippets)

-   **æœ€ä¼˜ç­–ç•¥ç›®æ ‡å‡½æ•° (Objective Function for Optimal Policy)**:
    $\pi^* = \arg \max_{\pi} E \left[ \sum_{t \ge 0} \gamma^t r_t \right]$

-   **ä»·å€¼å‡½æ•° (Value Function)**:
    $V^{\pi}(s) = E \left[ \sum_{t \ge 0} \gamma^t r_t | s_0 = s, \pi \right]$

-   **Q å‡½æ•° (Q Function)**:
    $Q^{\pi}(s, a) = E \left[ \sum_{t \ge 0} \gamma^t r_t | s_0 = s, a_0 = a, \pi \right]$

-   **æœ€ä¼˜ Q å‡½æ•°çš„è´å°”æ›¼æ–¹ç¨‹ (Bellman Equation for Optimal Q-function)**:
    $Q^*(s, a) = E_{r,s'} [r + \gamma \max_{a'} Q^*(s', a')]$
    å…¶ä¸­ $r \sim R(s, a)$, $s' \sim P(s, a)$

-   **ä»·å€¼è¿­ä»£æ›´æ–°è§„åˆ™ (Value Iteration Update Rule)**:
    $Q_{i+1}(s, a) = E_{r,s'} [r + \gamma \max_{a'} Q_i(s', a')]$
    å…¶ä¸­ $r \sim R(s, a)$, $s' \sim P(s, a)$

-   **æ·±åº¦ Q å­¦ä¹ æŸå¤±å‡½æ•° (Deep Q-Learning Loss Function)**:
    $L(s, a) = (Q(s, a; \theta) - y_{s,a,\theta})^2$
    å…¶ä¸­ $y_{s,a,\theta} = E_{r,s'} [r + \gamma \max_{a'} Q(s', a'; \theta')]$
    $r \sim R(s, a)$, $s' \sim P(s, a)$

-   **ç­–ç•¥æ¢¯åº¦ç›®æ ‡å‡½æ•° (Policy Gradients Objective Function)**:
    $J(\theta) = E_{x \sim p_{\theta}}[ \sum_{t \ge 0} \gamma^t r_t ]$

-   **ç­–ç•¥æ¢¯åº¦è®¡ç®—å…¬å¼ (Policy Gradient Computation Formula)**:
    $\frac{\partial J}{\partial \theta} = E_{x \sim p_{\theta}} [ (\sum_{t \ge 0} \gamma^t r_t) \sum_{t \ge 0} \frac{\partial}{\partial \theta} \log \pi_{\theta}(a_t | s_t) ]$
    å…¶ä¸­ $\frac{\partial}{\partial \theta} \log p_{\theta}(x) = \sum_{t \ge 0} \frac{\partial}{\partial \theta} \log \pi_{\theta}(a_t | s_t)$

-   **REINFORCE ç®—æ³• (REINFORCE Algorithm)**:
    1.  éšæœºåˆå§‹åŒ–æƒé‡ $\theta$ (Initialize random weights $\theta$)ã€‚
    2.  é€šè¿‡è¿è¡Œç­–ç•¥ $\pi_{\theta}$ åœ¨ç¯å¢ƒä¸­æ”¶é›†è½¨è¿¹ $x$ å’Œå¥–åŠ± $f(x)$ (Collect trajectories $x$ and rewards $f(x)$ using policy $\pi_{\theta}$)ã€‚
    3.  è®¡ç®— $\frac{\partial J}{\partial \theta}$ã€‚
    4.  å¯¹ $\theta$ æ‰§è¡Œæ¢¯åº¦ä¸Šå‡æ­¥ (Gradient ascent step on $\theta$)ã€‚
    5.  è·³è½¬åˆ°æ­¥éª¤ 2 (GOTO 2)ã€‚

-   **Atari æ¸¸æˆæ·±åº¦ Q å­¦ä¹ ç½‘ç»œæ¶æ„ (Atari Games Deep Q-Learning Network Architecture)**:
    -   ç½‘ç»œè¾“å…¥ (Network input): çŠ¶æ€ $s_t$: $4 \times 84 \times 84$ å †å çš„æœ€å 4 å¸§å›¾åƒ (ç»è¿‡ RGB->ç°åº¦è½¬æ¢ã€ä¸‹é‡‡æ ·å’Œè£å‰ªå)ã€‚
    -   ç½‘ç»œè¾“å‡º (Network output): æ‰€æœ‰åŠ¨ä½œçš„ Q å€¼ (Q-values for all actions)ã€‚
    -   å±‚ (Layers):
        -   Conv(4->16, 8x8, stride 4)
        -   Conv(16->32, 4x4, stride 2)
        -   FC-256
        -   FC-A (è¾“å‡º Q å€¼ï¼ŒA ä¸ºåŠ¨ä½œæ•°é‡)

### å››ã€è®²å¸ˆæå‡ºçš„æ€è€ƒé¢˜ (Questions Posed by the Instructor)

-   ä¸ºä»€ä¹ˆå¼ºåŒ–å­¦ä¹ ä¸ç›‘ç£å­¦ä¹ ä¸åŒï¼Ÿ(Why is RL different from normal supervised learning?)
-   (åœ¨è§£é‡Š Q å‡½æ•°å’Œä»·å€¼å‡½æ•°å) åˆ°ç›®å‰ä¸ºæ­¢æ˜¯å¦æ¸…æ¥šäº†ï¼Ÿæœ‰ä»»ä½•å…³äº Q å‡½æ•°ã€ä»·å€¼å‡½æ•°æˆ–ä»»ä½•å…¶ä»–å†…å®¹çš„ç–‘é—®å—ï¼Ÿ(Are we maybe clear up to this point? Any questions on these Q functions, these value functions, any any of the stuff up to this point?)