### [ğŸ“š] è§†é¢‘å­¦ä¹ è„šæ‰‹æ¶: Lecture 3: Linear Classifiers

### ä¸€ã€æ ¸å¿ƒå†…å®¹å¤§çº² (Core Content Outline)
-   **è®²åº§ä»‹ç» (Lecture Introduction)**
    -   ä¸»é¢˜ï¼šçº¿æ€§åˆ†ç±»å™¨ (Linear Classifiers)
-   **ä¸ŠèŠ‚å›é¡¾ï¼šå›¾åƒåˆ†ç±» (Last Time: Image Classification Recap)**
    -   **å›¾åƒåˆ†ç±»é—®é¢˜ (Image Classification Problem)**
        -   è¾“å…¥ï¼šå›¾åƒ (Input: image)
        -   è¾“å‡ºï¼šå°†å›¾åƒåˆ†é…åˆ°å›ºå®šç±»åˆ«çš„å…¶ä¸­ä¸€ä¸ª (Output: Assign image to one of a fixed set of categories)
        -   åŸºç¡€é—®é¢˜ (Foundational Problem)
    -   **è¯†åˆ«çš„æŒ‘æˆ˜ (Challenges of Recognition)**
        -   è§†ç‚¹å˜åŒ– (Viewpoint changes)
        -   å…‰ç…§å˜åŒ– (Illumination changes)
        -   å½¢å˜ (Deformation)
        -   é®æŒ¡ (Occlusion)
        -   æ‚ä¹± (Clutter)
        -   ç±»å†…å·®å¼‚ (Intraclass Variation)
        -   éœ€è¦æ„å»ºå¯¹è¿™äº›å˜åŒ–é²æ£’çš„åˆ†ç±»å™¨ (Need classifiers robust to variations)
    -   **æ•°æ®é©±åŠ¨æ–¹æ³•ï¼škNN (Data-Driven Approach: kNN)**
        -   ä¸å°è¯•å†™å‡ºæ˜¾å¼å‡½æ•°æ¥å¤„ç†æ‰€æœ‰è§†è§‰ç»†èŠ‚ (Rather than explicit function for visual details)
        -   æ”¶é›†å¤§é‡æ•°æ® (Collect a big dataset)
        -   ä½¿ç”¨å­¦ä¹ ç®—æ³•ä»æ•°æ®ä¸­å­¦ä¹  (Use a learning algorithm to learn from data)
        -   kNN åˆ†ç±»å™¨ (kNN classifier)
            -   è®°å¿†è®­ç»ƒæ•°æ® (Memorize training data)
            -   æµ‹è¯•æ—¶è¾“å‡ºæœ€ç›¸ä¼¼è®­ç»ƒå›¾åƒçš„æ ‡ç­¾ (Output label of most similar training image at test time)
        -   kNNçš„ç¼ºç‚¹ (Limitations of kNN)
            -   è®­ç»ƒå¿«ï¼Œè¯„ä¼°æ…¢ (Fast at training, slow to evaluate)
            -   L1/L2è·ç¦»åœ¨åƒç´ å€¼ä¸Šä¸å…·æœ‰æ„ŸçŸ¥æ„ä¹‰ (L1/L2 distances on raw pixel values not perceptually meaningful)
-   **ä»Šæ—¥ä¸»é¢˜ï¼šçº¿æ€§åˆ†ç±»å™¨ (Today: Linear Classifiers)**
    -   é‡è¦æ€§ï¼šä½œä¸ºç¥ç»ç½‘ç»œçš„åŸºæœ¬æ„å»ºæ¨¡å— (Very important as basic blocks in neural networks)
    -   **å›é¡¾ CIFAR-10 æ•°æ®é›† (Recall CIFAR-10 Dataset)**
        -   50,000å¼ è®­ç»ƒå›¾åƒ (50,000 training images)
        -   10,000å¼ æµ‹è¯•å›¾åƒ (10,000 test images)
        -   æ¯å¼ å›¾åƒå°ºå¯¸ï¼š32x32x3 (Each image is 32x32x3 numbers)
            -   å…±3072ä¸ªåƒç´ å€¼ (3072 numbers total)
    -   **å‚æ•°åŒ–æ–¹æ³• (Parametric Approach)**
        -   æ ¸å¿ƒæ€æƒ³ï¼šä¸€ä¸ªå‡½æ•° $f(x,W)$ï¼Œè¾“å…¥å›¾åƒ $x$ å’Œå¯å­¦ä¹ çš„æƒé‡ $W$ (learnable weights)ï¼Œè¾“å‡ºåˆ†ç±»å¾—åˆ† (class scores)ã€‚
        -   è¾“å‡ºä¸º10ä¸ªæ•°å­—ï¼Œå¯¹åº”10ä¸ªç±»åˆ«å¾—åˆ† (10 numbers giving class scores)
        -   **çº¿æ€§åˆ†ç±»å™¨å…¬å¼ (Linear Classifier Formula)**: $f(x,W) = Wx + b$
            -   $x$ï¼šè¾“å…¥å›¾åƒçš„åƒç´ å€¼å±•å¹³ä¸ºåˆ—å‘é‡ (Input image pixels stretched into a column vector, e.g., 3072 dimensions for CIFAR-10).
            -   $W$ï¼šæƒé‡çŸ©é˜µ (Weight matrix)
                -   å½¢çŠ¶ï¼š(ç±»åˆ«æ•°, åƒç´ ç»´åº¦) (Shape: e.g., (10, 3072) for CIFAR-10)
            -   $b$ï¼šåç½®å‘é‡ (Bias vector)
                -   å½¢çŠ¶ï¼š(ç±»åˆ«æ•°,) (Shape: e.g., (10,) for CIFAR-10)
            -   è¾“å‡ºï¼šåˆ†ç±»å¾—åˆ†å‘é‡ (Output: Vector of scores, e.g., (10,) for CIFAR-10)
        -   **åç½®æŠ€å·§ (Bias Trick)**
            -   å°†åç½® $b$ å¸æ”¶åˆ°æƒé‡çŸ©é˜µ $W$ ä¸­ (Bias absorbed into weight matrix $W$)
            -   é€šè¿‡åœ¨è¾“å…¥å‘é‡ $x$ çš„æœ«å°¾æ·»åŠ ä¸€ä¸ªå¸¸æ•°1 (Add extra one to data vector)
            -   ç›¸åº”åœ°å¢åŠ  $W$ çš„åˆ—æ•° (Increase last column of weight matrix)
    -   **è§£é‡Šçº¿æ€§åˆ†ç±»å™¨ (Interpreting a Linear Classifier)**
        -   **ä»£æ•°è§†è§’ (Algebraic Viewpoint)**: $f(x,W) = Wx + b$
            -   å°†å›¾åƒåƒç´ å±•å¹³ä¸ºåˆ—å‘é‡è¿›è¡ŒçŸ©é˜µå‘é‡ä¹˜æ³• (Stretch pixels into column for matrix-vector multiplication).
        -   **è§†è§‰è§†è§’ (Visual Viewpoint)**: çº¿æ€§åˆ†ç±»å™¨æœ‰ä¸€ä¸ªâ€œæ¨¡æ¿â€æ¯ä¸ªç±»åˆ« (Linear classifier has one "template" per category)
            -   æƒé‡çŸ©é˜µ $W$ çš„æ¯ä¸€è¡Œå¯ä»¥è¢«é‡æ–°è§£é‡Šä¸ºå¯¹åº”ç±»åˆ«çš„å›¾åƒæ¨¡æ¿ (Each row of $W$ can be reshaped to match image dimensions).
            -   åˆ†ç±»å¾—åˆ†æ˜¯é€šè¿‡è¾“å…¥å›¾åƒå’Œæ¯ä¸ªç±»åˆ«æ¨¡æ¿çš„å†…ç§¯ï¼ˆåŒ¹é…åº¦ï¼‰è®¡ç®—çš„ (Scores are inner products/matches between input image and class templates).
            -   **å±€é™æ€§ (Limitations)**:
                -   å•ä¸ªæ¨¡æ¿æ— æ³•æ•æ‰æ•°æ®ä¸­çš„å¤šç§æ¨¡å¼ (A single template cannot capture multiple modes of the data).
                -   ä¾‹å¦‚ï¼šé©¬å¯èƒ½æœå‘ä¸åŒæ–¹å‘ï¼Œå¯¼è‡´æ¨¡æ¿èåˆäº†å·¦å³æœå‘çš„ç‰¹å¾ï¼Œçœ‹èµ·æ¥åƒæœ‰ä¸¤åªå¤´ (e.g., horse template has 2 heads due to averaging different poses).
                -   å›¾åƒçš„èƒŒæ™¯æˆ–ä¸Šä¸‹æ–‡ä¿¡æ¯ä¼šå¼ºçƒˆå½±å“åˆ†ç±» (Relies heavily on context cues, e.g., blue sky for airplane, green background for deer).
                -   å¯¹é¢œè‰²ç­‰ç®€å•ç¼©æ”¾æ•æ„Ÿ (Predictions are linear, scaling image pixels by c scales scores by c, which is unintuitive for humans).
        -   **å‡ ä½•è§†è§’ (Geometric Viewpoint)**: è¶…å¹³é¢åˆ‡å‰²é«˜ç»´ç©ºé—´ (Hyperplanes carving up a high-dimensional space)
            -   æ¯ä¸ªç±»åˆ«çš„åˆ†ç±»å¾—åˆ†å‡½æ•°åœ¨åƒç´ ç©ºé—´ä¸­æ˜¯çº¿æ€§çš„ (Classifier score is a linear function of pixel values).
            -   å¾—åˆ†ä¸ºé›¶çš„åƒç´ å½¢æˆä¸€ä¸ªè¶…å¹³é¢ (The set of pixels where the score is zero forms a hyperplane).
            -   æ¯ä¸ªç±»åˆ«ç”±ä¸€ä¸ªè¶…å¹³é¢å®šä¹‰ (Each class is defined by a hyperplane).
            -   **å±€é™æ€§ (Limitations)**:
                -   å‡ ä½•ç›´è§‰åœ¨é«˜ç»´ç©ºé—´ä¸­å¯èƒ½å¤±æ•ˆ (Geometry gets really weird in high dimensions).
                -   çº¿æ€§åˆ†ç±»å™¨æ— æ³•å¤„ç†éçº¿æ€§å¯åˆ†çš„æ•°æ® (Cannot learn XOR function, concentric circles, or classes with multiple disjoint modes).
                -   è¿™æ˜¯æ„ŸçŸ¥å™¨ (Perceptron) å¤±è´¥çš„åŸå› ä¹‹ä¸€ (Historical context: Perceptron couldn't learn XOR because it's a linear classifier).
-   **é€‰æ‹©å¥½çš„æƒé‡ $W$ (Choosing a good W)**
    -   ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åªå®šä¹‰äº†ä¸€ä¸ªçº¿æ€§å¾—åˆ†å‡½æ•° $f(x,W) = Wx + b$ (So far: Defined a linear score function).
    -   ä½†å¦‚ä½•é€‰æ‹©ä¸€ä¸ªå¥½çš„ $W$ å‘¢ï¼Ÿ (But how can we actually choose a good W?)
    -   **å¾…åŠäº‹é¡¹ (TODO)**:
        1.  ä½¿ç”¨ä¸€ä¸ªæŸå¤±å‡½æ•° (Loss Function) æ¥é‡åŒ–å½“å‰ $W$ çš„å¥½å (Use a loss function to quantify how good a value of W is).
            -   ä½æŸå¤± = å¥½çš„åˆ†ç±»å™¨ (Low loss = good classifier)
            -   é«˜æŸå¤± = åçš„åˆ†ç±»å™¨ (High loss = bad classifier)
            -   ä¹Ÿç§°ä¸ºï¼šç›®æ ‡å‡½æ•° (objective function)ã€æˆæœ¬å‡½æ•° (cost function)
            -   è´ŸæŸå¤±å‡½æ•°æœ‰æ—¶ç§°ä¸ºå¥–åŠ±å‡½æ•° (reward function)ã€åˆ©æ¶¦å‡½æ•° (profit function)ã€æ•ˆç”¨å‡½æ•° (utility function)ã€é€‚åº”åº¦å‡½æ•° (fitness function) ç­‰ (Negative loss function sometimes called reward function, profit function, utility function, fitness function, etc.).
        2.  æ‰¾åˆ°ä¸€ä¸ªæœ€å°åŒ–æŸå¤±å‡½æ•° $W$ (ä¼˜åŒ–) (Find a W that minimizes the loss function (optimization)).
    -   **æŸå¤±å‡½æ•°å®šä¹‰ (Loss Function Definition)**:
        -   ç»™å®šä¸€ä¸ªç¤ºä¾‹æ•°æ®é›† $\{(x_i, y_i)\}_{i=1}^N$ (Given a dataset of examples).
            -   $x_i$ æ˜¯å›¾åƒ (where $x_i$ is image)
            -   $y_i$ æ˜¯æ•´æ•°æ ‡ç­¾ (and $y_i$ is integer label)
        -   å•ä¸ªç¤ºä¾‹çš„æŸå¤± $L_i = L_i(f(x_i, W), y_i)$ (Loss for a single example is $L_i$).
        -   æ•´ä¸ªæ•°æ®é›†çš„æŸå¤±æ˜¯æ¯ä¸ªç¤ºä¾‹æŸå¤±çš„å¹³å‡å€¼ $L = \frac{1}{N} \sum_{i} L_i(f(x_i, W), y_i)$ (Loss for the dataset is average of per-example losses).
    -   **å¤šåˆ†ç±»æ”¯æŒå‘é‡æœºæŸå¤± (Multiclass SVM Loss)**
        -   æ ¸å¿ƒæ€æƒ³ï¼šæ­£ç¡®ç±»åˆ«çš„å¾—åˆ†åº”è¯¥é«˜äºæ‰€æœ‰å…¶ä»–ç±»åˆ«çš„å¾—åˆ† (The score of the correct class should be higher than all the other scores).
        -   **å…¬å¼ (Formula)**: $L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + 1)$
            -   $s = f(x_i, W)$ æ˜¯æ¨¡å‹è¾“å‡ºçš„å¾—åˆ†å‘é‡ ($s$ are scores).
            -   $s_j$ æ˜¯é”™è¯¯ç±»åˆ«çš„å¾—åˆ† ($s_j$ is score for incorrect class).
            -   $s_{y_i}$ æ˜¯æ­£ç¡®ç±»åˆ«çš„å¾—åˆ† ($s_{y_i}$ is score for correct class).
            -   $+1$ æ˜¯ä¸€ä¸ªâ€œè¾¹ç•Œâ€æˆ–â€œé—´éš”â€ (a "margin").
        -   **ç‰¹æ€§ (Properties)**:
            -   å½“æ­£ç¡®ç±»åˆ«çš„å¾—åˆ†æ˜¾è‘—é«˜äºæœ€é«˜ä¸æ­£ç¡®ç±»åˆ«çš„å¾—åˆ†ï¼ˆè¶…è¿‡1çš„é—´éš”ï¼‰æ—¶ï¼ŒæŸå¤±ä¸º0 (If correct score is sufficiently higher than others, loss is 0).
            -   å¦åˆ™ï¼ŒæŸå¤±çº¿æ€§å¢åŠ  (Otherwise, loss increases linearly).
            -   ä¸€æ—¦æ­£ç¡®åˆ†ç±»ï¼Œå¾®å°çš„åˆ†æ•°å˜åŒ–ä¸ä¼šå½±å“æŸå¤± (If correctly classified, small score changes don't affect loss).
            -   æœ€å°å¯èƒ½æŸå¤±ä¸º0 (Min possible loss = 0).
            -   æœ€å¤§å¯èƒ½æŸå¤±æ— ä¸Šé™ (Max possible loss is unbounded).
    -   **æ­£åˆ™åŒ– (Regularization)**
        -   ç›®çš„ï¼šè¶…è¶Šè®­ç»ƒè¯¯å·®ï¼Œé˜²æ­¢æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°â€œè¿‡å¥½â€ (Prevent the model from doing *too* well on training data).
        -   **å…¨æŸå¤±å‡½æ•° (Full Loss Function)**: $L(W) = \frac{1}{N} \sum_{i=1}^N L_i(f(x_i, W), y_i) + \lambda R(W)$
            -   `æ•°æ®æŸå¤± (Data loss)`: æ¨¡å‹é¢„æµ‹åº”ä¸è®­ç»ƒæ•°æ®åŒ¹é… (Model predictions should match training data).
            -   `æ­£åˆ™åŒ–é¡¹ (Regularization term)`: æƒ©ç½šå¤æ‚æ¨¡å‹ (Penalizes complex models).
            -   $\lambda$: æ­£åˆ™åŒ–å¼ºåº¦ (regularization strength)ï¼Œä¸€ä¸ªè¶…å‚æ•° (hyperparameter)ã€‚
        -   **ç®€å•æ­£åˆ™åŒ–ç¤ºä¾‹ (Simple Regularization Examples for Linear Models)**:
            -   `L2æ­£åˆ™åŒ– (L2 regularization)`: $R(W) = \sum_k \sum_l W_{k,l}^2$ (å€¾å‘äºâ€œå¹³æ‘Šâ€æƒé‡ï¼Œä½¿ç”¨æ‰€æœ‰ç‰¹å¾).
            -   `L1æ­£åˆ™åŒ– (L1 regularization)`: $R(W) = \sum_k \sum_l |W_{k,l}|$ (å€¾å‘äºäº§ç”Ÿç¨€ç–æƒé‡ï¼Œåªç”¨å°‘é‡ç‰¹å¾).
            -   `å¼¹æ€§ç½‘ç»œ (Elastic Net)`: $R(W) = \sum_k \sum_l \beta W_{k,l}^2 + |\alpha W_{k,l}|$ (L1 + L2ç»„åˆ).
        -   **æ›´å¤æ‚æ­£åˆ™åŒ–ç¤ºä¾‹ (More Complex Regularization for Neural Networks)**:
            -   `Dropout` (éšæœºä¸¢å¼ƒç¥ç»å…ƒ).
            -   `æ‰¹æ ‡å‡†åŒ– (Batch Normalization)` (æ ‡å‡†åŒ–å±‚è¾“å…¥).
            -   `Cutout`, `Mixup`, `Stochastic depth` (æ•°æ®å¢å¼ºåŠç½‘ç»œç»“æ„éšæœºåŒ–æ–¹æ³•).
        -   **æ­£åˆ™åŒ–çš„ç›®çš„ (Purpose of Regularization)**:
            -   åœ¨â€œæœ€å°åŒ–è®­ç»ƒè¯¯å·®â€ä¹‹å¤–ï¼Œè¡¨è¾¾æ¨¡å‹é—´çš„åå¥½ (Express preferences in among models beyond "minimize training error").
            -   é¿å…**è¿‡æ‹Ÿåˆ (Overfitting)**: åå¥½ç®€å•æ¨¡å‹ï¼Œä½¿å…¶æ³›åŒ–èƒ½åŠ›æ›´å¥½ (Prefer simple models that generalize better).
            -   é€šè¿‡å¢åŠ æ›²ç‡æ”¹è¿›ä¼˜åŒ–è¿‡ç¨‹ (Improve optimization by adding curvature)ã€‚
    -   **äº¤å‰ç†µæŸå¤± (Cross-Entropy Loss)**
        -   ç›®çš„ï¼šå°†åŸå§‹åˆ†ç±»å™¨å¾—åˆ†è§£é‡Šä¸º**æ¦‚ç‡ (probabilities)** (Want to interpret raw classifier scores as probabilities).
        -   **Softmaxå‡½æ•° (Softmax Function)**: $P(Y=k|X=x_i) = \frac{e^{s_k}}{\sum_j e^{s_j}}$
            -   å°†å¾—åˆ† $s$ è½¬æ¢ä¸ºéè´Ÿçš„éå½’ä¸€åŒ–æ¦‚ç‡ (Take raw scores, apply exponential to make them non-negative).
            -   ç„¶åè¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å…¶æ€»å’Œä¸º1 (Normalize by dividing by the sum of exponentials).
            -   ç»“æœæ˜¯ä¸€ç»„å¯ä»¥è§£é‡Šä¸ºç±»åˆ«çš„æ¦‚ç‡ (Result is a probability distribution over classes).
        -   **äº¤å‰ç†µæŸå¤±å…¬å¼ (Cross-Entropy Loss Formula)**: $L_i = -\log P(Y=y_i|X=x_i)$
            -   æœ€å¤§åŒ–æ­£ç¡®ç±»åˆ«çš„æ¦‚ç‡ (Maximizes probability of correct class).
            -   æ˜¯**æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (Maximum Likelihood Estimation)** çš„ä¸€ä¸ªå®ä¾‹ (It's an instance of MLE).
        -   **ä¸ Kullback-Leibler (KL) æ•£åº¦çš„å…³ç³» (Relation to KL Divergence)**:
            -   $L_i$ ç­‰ä»·äºè®¡ç®—é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå’ŒçœŸå®æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„ KL æ•£åº¦ (The loss can be seen as minimizing the KL divergence between predicted and true distributions).
            -   $D_{KL}(P||Q) = \sum_y P(y) \log \frac{P(y)}{Q(y)}$ (Kullback-Leibler divergence formula).
            -   äº¤å‰ç†µ (Cross Entropy) ä¹Ÿæ˜¯ä¿¡æ¯è®ºä¸­çš„ä¸€ä¸ªæ¦‚å¿µï¼š$H(P, Q) = H(P) + D_{KL}(P||Q)$ã€‚
-   **æ¯”è¾ƒäº¤å‰ç†µæŸå¤±ä¸SVMæŸå¤± (Cross-Entropy Loss vs SVM Loss Comparison)**
    -   **SVM æŸå¤± (SVM Loss)**ï¼šä¸€æ—¦è¾¾åˆ°è¾¹è·ï¼ŒæŸå¤±å°±ä¸º0ï¼Œä¸å†å…³å¿ƒå¾—åˆ†çš„è¿›ä¸€æ­¥å¢åŠ  (Once the margin is met, loss is 0 and no further improvement is penalized).
    -   **äº¤å‰ç†µæŸå¤± (Cross-Entropy Loss)**ï¼šæ°¸ä¸æ»¡è¶³ï¼Œæ€»æ˜¯å¸Œæœ›æ­£ç¡®ç±»åˆ«çš„å¾—åˆ†æ— é™é«˜ (Never satisfied, always wants the score of the correct class to be infinitely higher).

### äºŒã€å…³é”®æœ¯è¯­å®šä¹‰ (Key Term Definitions)
-   **çº¿æ€§åˆ†ç±»å™¨ (Linear Classifiers)**: ä¸€ç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡è¾“å…¥ç‰¹å¾çš„çº¿æ€§ç»„åˆæ¥åšå‡ºåˆ†ç±»å†³ç­–ï¼Œå…¶å†³ç­–è¾¹ç•Œæ˜¯ä¸€ä¸ªè¶…å¹³é¢ã€‚
-   **å›¾åƒåˆ†ç±» (Image Classification)**: è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œæ—¨åœ¨å°†å›¾åƒåˆ†é…åˆ°é¢„å®šä¹‰çš„ç±»åˆ«ä¹‹ä¸€ã€‚
-   **æ•°æ®é©±åŠ¨æ–¹æ³• (Data-Driven Approach)**: ä¸€ç§æœºå™¨å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡ä»å¤§é‡æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼æ¥æ„å»ºæ¨¡å‹ï¼Œè€Œä¸æ˜¯é€šè¿‡äººå·¥ç¼–å†™è§„åˆ™ã€‚
-   **Kæœ€è¿‘é‚» (k-Nearest Neighbors, kNN)**: ä¸€ç§ç®€å•çš„éå‚æ•°åˆ†ç±»ç®—æ³•ï¼Œé€šè¿‡æŸ¥æ‰¾è®­ç»ƒæ•°æ®é›†ä¸­ä¸æ–°æ•°æ®ç‚¹æœ€è¿‘çš„kä¸ªæ ·æœ¬æ¥è¿›è¡Œåˆ†ç±»ã€‚
-   **è¶…å‚æ•° (Hyperparameters)**: åœ¨æ¨¡å‹è®­ç»ƒå¼€å§‹å‰éœ€è¦æ‰‹åŠ¨è®¾å®šçš„å‚æ•°ï¼Œä¾‹å¦‚kNNä¸­çš„kå€¼ã€‚
-   **äº¤å‰éªŒè¯ (Cross-Validation)**: ä¸€ç§æ¨¡å‹éªŒè¯æŠ€æœ¯ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ç‹¬ç«‹æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡å°†æ•°æ®é›†åˆ†æˆå¤šä¸ªå­é›†è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚
-   **æ„ŸçŸ¥æ„ä¹‰ (Perceptually Meaningful)**: æŒ‡çš„æ˜¯æŸç§åº¦é‡æˆ–ç‰¹å¾ä¸äººç±»æ„ŸçŸ¥çš„ç›¸ä¼¼æˆ–ç›¸å…³ç¨‹åº¦ã€‚
-   **å‚æ•°åŒ–æ–¹æ³• (Parametric Approach)**: ä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå…¶ä¸­æ¨¡å‹å…·æœ‰å›ºå®šæ•°é‡çš„å‚æ•°ï¼ˆå¦‚æƒé‡å’Œåç½®ï¼‰ï¼Œè¿™äº›å‚æ•°ä»æ•°æ®ä¸­å­¦ä¹ ã€‚
-   **æƒé‡ (Weights)**: å‚æ•°åŒ–æ¨¡å‹ä¸­çš„å¯å­¦ä¹ æ•°å€¼ï¼Œå®ƒä»¬å†³å®šäº†è¾“å…¥ç‰¹å¾å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“ã€‚
-   **åç½® (Bias)**: å‚æ•°åŒ–æ¨¡å‹ä¸­çš„å¯å­¦ä¹ æ•°å€¼ï¼Œä½œä¸ºå¸¸æ•°é¡¹æ·»åŠ åˆ°çº¿æ€§ç»„åˆä¸­ï¼Œå…è®¸å†³ç­–è¾¹ç•Œä¸ç»è¿‡åŸç‚¹ã€‚
-   **CIFAR-10 æ•°æ®é›† (CIFAR-10 Dataset)**: ä¸€ä¸ªå¸¸ç”¨çš„è®¡ç®—æœºè§†è§‰æ•°æ®é›†ï¼ŒåŒ…å«10ä¸ªç±»åˆ«çš„60,000å¼ 32x32å½©è‰²å›¾åƒã€‚
-   **åç½®æŠ€å·§ (Bias Trick)**: ä¸€ç§ç®€åŒ–çº¿æ€§æ¨¡å‹è¡¨ç¤ºçš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨è¾“å…¥ç‰¹å¾å‘é‡ä¸­æ·»åŠ ä¸€ä¸ªå€¼ä¸º1çš„é¢å¤–ç»´åº¦ï¼Œå°†åç½®é¡¹å¸æ”¶åˆ°æƒé‡çŸ©é˜µä¸­ã€‚
-   **ä»£æ•°è§†è§’ (Algebraic Viewpoint)**: ä»æ•°å­¦ä»£æ•°ï¼ˆå¦‚çŸ©é˜µä¹˜æ³•ï¼‰çš„è§’åº¦ç†è§£å’Œè¡¨ç¤ºçº¿æ€§åˆ†ç±»å™¨çš„å·¥ä½œåŸç†ã€‚
-   **è§†è§‰è§†è§’ (Visual Viewpoint)**: ä»è§†è§‰æ¨¡å¼æˆ–â€œæ¨¡æ¿â€åŒ¹é…çš„è§’åº¦ç†è§£çº¿æ€§åˆ†ç±»å™¨çš„å·¥ä½œåŸç†ï¼Œæ¯ä¸ªç±»åˆ«å¯¹åº”ä¸€ä¸ª learned templateã€‚
-   **æ¨¡æ¿åŒ¹é… (Template Matching)**: ä¸€ç§å›¾åƒå¤„ç†æŠ€æœ¯ï¼Œé€šè¿‡æ¯”è¾ƒè¾“å…¥å›¾åƒä¸é¢„å®šä¹‰æ¨¡æ¿çš„ç›¸ä¼¼æ€§æ¥è¯†åˆ«å¯¹è±¡æˆ–æ¨¡å¼ã€‚
-   **å‡ ä½•è§†è§’ (Geometric Viewpoint)**: ä»å‡ ä½•ç©ºé—´ï¼ˆå¦‚åƒç´ ç©ºé—´ï¼‰çš„è§’åº¦ç†è§£çº¿æ€§åˆ†ç±»å™¨ï¼Œå…¶ä¸­å†³ç­–è¾¹ç•Œè¢«è¡¨ç¤ºä¸ºè¶…å¹³é¢ã€‚
-   **è¶…å¹³é¢ (Hyperplanes)**: Nç»´ç©ºé—´ä¸­ç»´åº¦ä¸ºN-1çš„å¹³å¦å­ç©ºé—´ï¼Œåœ¨çº¿æ€§åˆ†ç±»å™¨ä¸­ä½œä¸ºä¸åŒç±»åˆ«ä¹‹é—´çš„å†³ç­–è¾¹ç•Œã€‚
-   **æŸå¤±å‡½æ•° (Loss Function)**: ä¸€ä¸ªæ•°å­¦å‡½æ•°ï¼Œç”¨äºé‡åŒ–æ¨¡å‹é¢„æµ‹ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„ä¸åŒ¹é…ç¨‹åº¦ï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–æ­¤å‡½æ•°ã€‚
-   **ç›®æ ‡å‡½æ•° (Objective Function)**: é€šå¸¸ä¸æŸå¤±å‡½æ•°åŒä¹‰ï¼Œè¡¨ç¤ºæ¨¡å‹ä¼˜åŒ–æ‰€è¿½æ±‚çš„ç›®æ ‡ã€‚
-   **æˆæœ¬å‡½æ•° (Cost Function)**: æŸå¤±å‡½æ•°æˆ–ç›®æ ‡å‡½æ•°çš„å¦ä¸€ä¸ªåŒä¹‰è¯ï¼Œé€šå¸¸æŒ‡åœ¨æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ä¸Šçš„å¹³å‡æŸå¤±ã€‚
-   **å¤šåˆ†ç±»æ”¯æŒå‘é‡æœºæŸå¤± (Multiclass SVM Loss)**: ä¸€ç§å¸¸ç”¨çš„æŸå¤±å‡½æ•°ï¼Œå®ƒé¼“åŠ±æ­£ç¡®ç±»åˆ«çš„å¾—åˆ†æ¯”æ‰€æœ‰ä¸æ­£ç¡®ç±»åˆ«çš„å¾—åˆ†è‡³å°‘é«˜å‡ºä¸€ä¸ªé¢„è®¾çš„é—´éš”ã€‚
-   **é“°é“¾æŸå¤± (Hinge Loss)**: å¤šåˆ†ç±»SVMæŸå¤±çš„å…·ä½“å½¢å¼ï¼Œå…¶å›¾å½¢è¡¨ç¤ºä¸ºä¸€ä¸ªâ€œé“°é“¾â€å½¢çŠ¶ï¼Œå³åœ¨è¾¾åˆ°ä¸€å®šé—´éš”åæŸå¤±å˜ä¸ºé›¶ã€‚
-   **è¾¹é™…/é—´éš” (Margin)**: åœ¨SVMæŸå¤±ä¸­ï¼Œæ­£ç¡®åˆ†ç±»çš„å¾—åˆ†éœ€è¦è¶…è¿‡ä¸æ­£ç¡®åˆ†ç±»çš„å¾—åˆ†çš„æœ€å°å·®å€¼ã€‚
-   **æ•°æ®æŸå¤± (Data Loss)**: æŸå¤±å‡½æ•°çš„ä¸€éƒ¨åˆ†ï¼Œé‡åŒ–æ¨¡å‹é¢„æµ‹ä¸è®­ç»ƒæ•°æ®ä¹‹é—´çš„åŒ¹é…ç¨‹åº¦ã€‚
-   **æ­£åˆ™åŒ– (Regularization)**: åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œé€šè¿‡å‘æŸå¤±å‡½æ•°æ·»åŠ ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹æ¥é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ æ›´ç®€å•ã€æ³›åŒ–èƒ½åŠ›æ›´å¥½çš„å‚æ•°ã€‚
-   **L2æ­£åˆ™åŒ– (L2 Regularization)**: ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œå°†æ¨¡å‹æƒé‡çš„å¹³æ–¹å’Œæ·»åŠ åˆ°æŸå¤±å‡½æ•°ä¸­ï¼Œæƒ©ç½šå¤§çš„æƒé‡å€¼ï¼Œä¿ƒä½¿æƒé‡åˆ†æ•£ä¸”è¾ƒå°ã€‚
-   **L1æ­£åˆ™åŒ– (L1 Regularization)**: ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œå°†æ¨¡å‹æƒé‡çš„ç»å¯¹å€¼å’Œæ·»åŠ åˆ°æŸå¤±å‡½æ•°ä¸­ï¼Œå€¾å‘äºäº§ç”Ÿç¨€ç–æ¨¡å‹ï¼ˆå³è®¸å¤šæƒé‡ä¸ºé›¶ï¼‰ï¼Œç”¨äºç‰¹å¾é€‰æ‹©ã€‚
-   **å¼¹æ€§ç½‘ç»œ (Elastic Net)**: ç»“åˆäº†L1å’ŒL2æ­£åˆ™åŒ–çš„ä¸€ç§æ–¹æ³•ã€‚
-   **Dropout (éšæœºå¤±æ´»)**: ä¸€ç§ç¥ç»ç½‘ç»œæ­£åˆ™åŒ–æŠ€æœ¯ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºåœ°ä¸¢å¼ƒï¼ˆç½®é›¶ï¼‰ä¸€éƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
-   **æ‰¹æ ‡å‡†åŒ– (Batch Normalization)**: ä¸€ç§ç¥ç»ç½‘ç»œæŠ€æœ¯ï¼Œç”¨äºæ ‡å‡†åŒ–ç½‘ç»œå±‚è¾“å…¥ï¼Œä»è€Œç¨³å®šå’ŒåŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶å…·æœ‰æ­£åˆ™åŒ–æ•ˆæœã€‚
-   **è¿‡æ‹Ÿåˆ (Overfitting)**: æŒ‡æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æœªè§è¿‡çš„æ–°æ•°æ®ä¸Šè¡¨ç°ä¸ä½³çš„ç°è±¡ã€‚
-   **äº¤å‰ç†µæŸå¤± (Cross-Entropy Loss)**: ä¸€ç§å¸¸ç”¨çš„æŸå¤±å‡½æ•°ï¼Œç‰¹åˆ«é€‚ç”¨äºåˆ†ç±»é—®é¢˜ï¼Œå®ƒè¡¡é‡äº†ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚
-   **æœªå½’ä¸€åŒ–å¯¹æ•°æ¦‚ç‡/Logits (Unnormalized Log-probabilities / Logits)**: åœ¨Softmaxå‡½æ•°åº”ç”¨ä¹‹å‰ï¼Œçº¿æ€§åˆ†ç±»å™¨è¾“å‡ºçš„åŸå§‹å¾—åˆ†ã€‚
-   **Softmaxå‡½æ•° (Softmax Function)**: ä¸€ç§å°†ä»»æ„å®å€¼å‘é‡è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒçš„å‡½æ•°ï¼Œå…¶è¾“å‡ºå€¼åœ¨0åˆ°1ä¹‹é—´ä¸”æ€»å’Œä¸º1ã€‚
-   **æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (Maximum Likelihood Estimation, MLE)**: ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œç”¨äºä¼°è®¡æ¨¡å‹å‚æ•°ï¼Œä½¿å¾—è§‚æµ‹æ•°æ®çš„æ¦‚ç‡æœ€å¤§åŒ–ã€‚
-   **Kullback-Leibler (KL) æ•£åº¦ (Kullback-Leibler (KL) Divergence)**: è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´å·®å¼‚çš„éå¯¹ç§°åº¦é‡ï¼Œåœ¨ä¿¡æ¯è®ºå’Œæœºå™¨å­¦ä¹ ä¸­å¸¸ç”¨äºæ¯”è¾ƒæ¨¡å‹é¢„æµ‹åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒã€‚
-   **äº¤å‰ç†µ (Cross Entropy)**: åœ¨ä¿¡æ¯è®ºä¸­ï¼Œäº¤å‰ç†µè¡¡é‡äº†ä½¿ç”¨ä¸€ä¸ªç¼–ç æ–¹æ¡ˆæ¥è¡¨ç¤ºå¦ä¸€ä¸ªç¼–ç æ–¹æ¡ˆæ‰€éœ€è¦çš„å¹³å‡æ¯”ç‰¹æ•°ï¼Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå®ƒè¢«ç”¨ä½œè¡¡é‡é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒä¹‹é—´å·®å¼‚çš„æŸå¤±å‡½æ•°ã€‚

### ä¸‰ã€æ ¸å¿ƒç®—æ³•ä¸ä»£ç ç‰‡æ®µ (Core Algorithms & Code Snippets)

-   **çº¿æ€§åˆ†ç±»å™¨å¾—åˆ†å‡½æ•° (Linear Classifier Score Function)**:
    -   $f(x,W) = Wx$ (when bias trick is applied where x includes a constant 1 at the end).

-   **å¤šåˆ†ç±»æ”¯æŒå‘é‡æœºæŸå¤± (Multiclass SVM Loss)**:
    -   **å…¬å¼ (Formula)**: $L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + 1)$
    -   **å•ä¸ªç¤ºä¾‹çš„SVMæŸå¤±è®¡ç®—ç¤ºä¾‹ (SVM Loss Calculation Example for a Single Example)**:
        -   **å›¾åƒ** (Image): çŒ« (Cat), **çœŸå®æ ‡ç­¾** (True Label): cat (å¯¹åº”ç´¢å¼• 0)
        -   **æ¨¡å‹é¢„æµ‹å¾—åˆ†** (Model Predicted Scores): $s = [3.2, 5.1, -1.7]$ (cat: 3.2, car: 5.1, frog: -1.7)
        -   **æŸå¤±è®¡ç®—** (Loss Calculation):
            $L_{cat} = \max(0, s_{car} - s_{cat} + 1) + \max(0, s_{frog} - s_{cat} + 1)$
            $L_{cat} = \max(0, 5.1 - 3.2 + 1) + \max(0, -1.7 - 3.2 + 1)$
            $L_{cat} = \max(0, 1.9 + 1) + \max(0, -4.9 + 1)$
            $L_{cat} = \max(0, 2.9) + \max(0, -3.9)$
            $L_{cat} = 2.9 + 0$
            $L_{cat} = 2.9$
        -   **å¹³å‡æŸå¤± (Average Loss)** (åŸºäºè§†é¢‘ä¸­ç»™å‡ºçš„æ‰€æœ‰ä¸‰ä¸ªå›¾åƒçš„æŸå¤±): $L = (2.9 + 0 + 12.9) / 3 = 5.27$

-   **äº¤å‰ç†µæŸå¤± (Cross-Entropy Loss)**:
    -   **Softmaxå‡½æ•° (Softmax Function)**: $P(Y=k|X=x_i) = \frac{e^{s_k}}{\sum_j e^{s_j}}$
    -   **äº¤å‰ç†µæŸå¤±å…¬å¼ (Cross-Entropy Loss Formula)**: $L_i = -\log P(Y=y_i|X=x_i)$ (Put it all together: $L_i = -\log \left( \frac{e^{s_{y_i}}}{\sum_j e^{s_j}} \right)$)
    -   **å•ä¸ªç¤ºä¾‹çš„äº¤å‰ç†µæŸå¤±è®¡ç®—ç¤ºä¾‹ (Cross-Entropy Loss Calculation Example for a Single Example)**:
        -   **å›¾åƒ** (Image): çŒ« (Cat), **çœŸå®æ ‡ç­¾** (True Label): cat (å¯¹åº”ç´¢å¼• 0)
        -   **æ¨¡å‹é¢„æµ‹å¾—åˆ†** (Model Predicted Scores): $s = [3.2, 5.1, -1.7]$
        -   **æ­¥éª¤1: æŒ‡æ•°åŒ– (Exponentiate)**:
            $e^{3.2} \approx 24.5$
            $e^{5.1} \approx 164.0$
            $e^{-1.7} \approx 0.18$
            (è¿™äº›æ˜¯æœªå½’ä¸€åŒ–æ¦‚ç‡ (unnormalized probabilities)ï¼Œå¿…é¡»å¤§äºç­‰äº0 (must be $\geq 0$))
        -   **æ­¥éª¤2: å½’ä¸€åŒ– (Normalize)** (é€šè¿‡Softmaxå‡½æ•°):
            æ€»å’Œ $= 24.5 + 164.0 + 0.18 = 188.68$
            æ¦‚ç‡ (probabilities):
            çŒ« (Cat): $24.5 / 188.68 \approx 0.13$
            è½¦ (Car): $164.0 / 188.68 \approx 0.87$
            é’è›™ (Frog): $0.18 / 188.68 \approx 0.00$
            (è¿™äº›æ¦‚ç‡æ€»å’Œå¿…é¡»ä¸º1 (must sum to 1))
        -   **æ­¥éª¤3: è®¡ç®—æŸå¤± (Calculate Loss)**:
            $L_{cat} = -\log P(Y=cat|X=x_{cat})$
            $L_{cat} = -\log(0.13)$
            $L_{cat} \approx 2.04$

### å››ã€è®²å¸ˆæå‡ºçš„æ€è€ƒé¢˜ (Questions Posed by the Instructor)
-   **Q1: "What happens to the loss if the scores for the car image change a bit?"** (å¦‚æœæ±½è½¦å›¾åƒçš„å¾—åˆ†å‘ç”Ÿä¸€ç‚¹å˜åŒ–ï¼ŒæŸå¤±ä¼šæ€æ ·ï¼Ÿ)
    -   **å¯¹äºSVMæŸå¤± (For SVM Loss)**: æŸå¤±å°†ä¿æŒä¸å˜ã€‚å› ä¸ºæ±½è½¦å›¾åƒå·²ç»æ­£ç¡®åˆ†ç±»ä¸”æ»¡è¶³äº†1çš„è¾¹é™…ï¼ŒæŸå¤±å·²ç»ä¸º0ã€‚å¾®å°çš„åˆ†æ•°å˜åŒ–ä¸ä¼šæ”¹å˜è¿™ä¸ªçŠ¶æ€ã€‚
-   **Q2: "What are the min and max possible loss?"** (æœ€å°å’Œæœ€å¤§å¯èƒ½æŸå¤±æ˜¯å¤šå°‘ï¼Ÿ)
    -   **å¯¹äºSVMæŸå¤± (For SVM Loss)**: æœ€å°æŸå¤±ä¸º0ã€‚æœ€å¤§æŸå¤±æ˜¯æ— é™å¤§ã€‚
    -   **å¯¹äºäº¤å‰ç†µæŸå¤± (For Cross-Entropy Loss)**: æœ€å°æŸå¤±ä¸º0ï¼ˆå½“æ¨¡å‹å¯¹æ­£ç¡®ç±»åˆ«åˆ†é…100%çš„æ¦‚ç‡æ—¶ï¼‰ã€‚æœ€å¤§æŸå¤±æ˜¯æ— é™å¤§ï¼ˆå½“æ¨¡å‹å¯¹æ­£ç¡®ç±»åˆ«åˆ†é…0%çš„æ¦‚ç‡æ—¶ï¼‰ã€‚
-   **Q3: "If all the scores were random, what loss would we expect?"** (å¦‚æœæ‰€æœ‰å¾—åˆ†éƒ½æ˜¯éšæœºçš„å°å€¼ï¼ŒæœŸæœ›çš„æŸå¤±æ˜¯å¤šå°‘ï¼Ÿ)
    -   **å¯¹äºSVMæŸå¤± (For SVM Loss)**: æœŸæœ›çš„æŸå¤±çº¦ä¸º $C-1$ï¼Œå…¶ä¸­ $C$ æ˜¯ç±»åˆ«æ•°ã€‚ä¾‹å¦‚ï¼Œå¯¹äº3ä¸ªç±»åˆ«ï¼ŒæœŸæœ›æŸå¤±ä¸º2ã€‚
    -   **å¯¹äºäº¤å‰ç†µæŸå¤± (For Cross-Entropy Loss)**: æœŸæœ›çš„æŸå¤±çº¦ä¸º $-\log(1/C)$ æˆ– $\log(C)$ã€‚ä¾‹å¦‚ï¼Œå¯¹äº10ä¸ªç±»åˆ«ï¼ŒæœŸæœ›æŸå¤±çº¦ä¸º $\log(10) \approx 2.3$ã€‚
    -   è¿™æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„è°ƒè¯•æŠ€å·§ï¼šå¦‚æœä½ çš„æ¨¡å‹åœ¨éšæœºåˆå§‹åŒ–åæ²¡æœ‰å¾—åˆ°è¿™ä¸ªæŸå¤±å€¼ï¼Œé‚£ä¹ˆä½ çš„å®ç°å¯èƒ½å­˜åœ¨é”™è¯¯ã€‚
-   **Q4: "What would happen if the sum were over all classes? (including i = yi)"** (å¦‚æœæ±‚å’Œæ˜¯é’ˆå¯¹æ‰€æœ‰ç±»åˆ«ï¼ˆåŒ…æ‹¬æ­£ç¡®ç±»åˆ« $i=y_i$ï¼‰ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ)
    -   **å¯¹äºSVMæŸå¤± (For SVM Loss)**: æŸå¤±ä¼šå¢åŠ ä¸€ä¸ªå¸¸æ•°1ã€‚å› ä¸º $max(0, s_{y_i} - s_{y_i} + 1) = max(0, 1) = 1$ã€‚
    -   **å¯¹äºäº¤å‰ç†µæŸå¤± (For Cross-Entropy Loss)**: æŸå¤±çš„ç›¸å¯¹åå¥½ä¸ä¼šæ”¹å˜ï¼Œå› ä¸ºåªæ˜¯æ•´ä½“å¢åŠ äº†ä¸€ä¸ªå¸¸æ•°ï¼Œå®ƒä»ç„¶ä¼šäº§ç”Ÿç›¸åŒçš„åˆ†ç±»å™¨åå¥½ã€‚
-   **Q5: "What if the loss used a mean instead of a sum?"** (å¦‚æœæŸå¤±å‡½æ•°ä½¿ç”¨å¹³å‡å€¼è€Œä¸æ˜¯æ±‚å’Œï¼Œä¼šæ€æ ·ï¼Ÿ)
    -   **å¯¹äºSVMæŸå¤±å’Œäº¤å‰ç†µæŸå¤± (For both SVM and Cross-Entropy Loss)**: æŸå¤±çš„æ•°å€¼ä¼šæŒ‰æ¯”ä¾‹ç¼©å°ï¼Œä½†å¯¹æƒé‡çŸ©é˜µçš„åå¥½ï¼ˆå³å“ªä¸ªæƒé‡çŸ©é˜µæ›´å¥½ï¼‰ä¸ä¼šæ”¹å˜ï¼Œå› ä¸ºè¿™åªæ˜¯ä¸€ä¸ªå•è°ƒå˜æ¢ã€‚
-   **Q6: "What if we used this loss instead?" ($L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + 1)^2$)** (å¦‚æœä½¿ç”¨è¿™ä¸ªæŸå¤±å‡½æ•°ä¼šæ€æ ·ï¼Ÿ)
    -   **å¯¹äºSVMæŸå¤± (For SVM Loss)**: è¿™å°†è¡¨è¾¾ä¸åŒçš„æƒé‡çŸ©é˜µåå¥½ã€‚å› ä¸ºå®ƒå¼•å…¥äº†å¹³æ–¹é¡¹ï¼Œä¼šéçº¿æ€§åœ°æ”¹å˜å¾—åˆ†å¯¹æŸå¤±çš„å½±å“ã€‚å®ƒå°†ä¸å†è¢«ç§°ä¸ºå¤šåˆ†ç±»SVMæŸå¤±ã€‚
-   **Q7: (æ¯”è¾ƒCross-Entropy Losså’ŒSVM Lossåœ¨ç‰¹å®šå¾—åˆ†ä¸‹çš„è¡Œä¸º) What happens to each loss if I slightly change the scores of the last datapoint?** (å¦‚æœæˆ‘ç¨å¾®æ”¹å˜æœ€åä¸€ä¸ªæ•°æ®ç‚¹çš„å¾—åˆ†ï¼Œæ¯ä¸ªæŸå¤±ä¼šæ€æ ·ï¼Ÿ)
    -   **å‡è®¾å¾—åˆ†** (Assume scores): `[10, -100, -100]`ï¼Œ**æ­£ç¡®ç±»åˆ«** (Correct class) $y_i=0$ (å¾—åˆ†ä¸º10)ã€‚
    -   **SVMæŸå¤± (SVM Loss)**: $max(0, -100-10+1) + max(0, -100-10+1) = 0+0 = 0$ã€‚å¦‚æœç¨å¾®æ”¹å˜-100ï¼ŒæŸå¤±ä»ç„¶æ˜¯0ï¼Œå› ä¸ºå®ƒå·²ç»æ»¡è¶³äº†è¾¹è·ã€‚SVMæŸå¤±å¯¹æ­¤ä¸æ•æ„Ÿã€‚
    -   **äº¤å‰ç†µæŸå¤± (Cross-Entropy Loss)**: æŸå¤±ä¼šå‘ç”Ÿå˜åŒ–ã€‚äº¤å‰ç†µæŸå¤±æ€»æ˜¯å¸Œæœ›æ­£ç¡®ç±»åˆ«çš„æ¦‚ç‡å°½å¯èƒ½é«˜ï¼Œå³ä½¿å®ƒå·²ç»æ­£ç¡®åˆ†ç±»ï¼Œä¹Ÿä¼šç»§ç»­æ¨åŠ¨å¾—åˆ†åˆ†å¼€ã€‚
-   **Q8: (æ¯”è¾ƒCross-Entropy Losså’ŒSVM Lossåœ¨ç‰¹å®šå¾—åˆ†ä¸‹çš„è¡Œä¸º) What happens to each loss if I double the score of the correct class from 10 to 20?** (å¦‚æœæˆ‘å°†æ­£ç¡®ç±»åˆ«çš„å¾—åˆ†ä»10åŠ å€åˆ°20ï¼Œæ¯ä¸ªæŸå¤±ä¼šæ€æ ·ï¼Ÿ)
    -   **SVMæŸå¤± (SVM Loss)**: æŸå¤±ä»ç„¶æ˜¯0ã€‚å› ä¸ºå·²ç»æ»¡è¶³äº†è¾¹è·ã€‚
    -   **äº¤å‰ç†µæŸå¤± (Cross-Entropy Loss)**: æŸå¤±ä¼š**å‡å°**ã€‚å› ä¸ºæ­£ç¡®ç±»åˆ«çš„æ¦‚ç‡ä¼šå¢åŠ ï¼Œ$-\log(P)$ å°±ä¼šå‡å°ã€‚äº¤å‰ç†µæŸå¤±æ€»æ˜¯ä¼šç»§ç»­æ¨åŠ¨å¾—åˆ†åˆ†å¼€ã€‚
-   **Q9: (è°ƒè¯•æŠ€å·§) If all scores are small random values, what is the loss?** (å¦‚æœæ‰€æœ‰å¾—åˆ†éƒ½æ˜¯å°çš„éšæœºå€¼ï¼ŒæŸå¤±æ˜¯å¤šå°‘ï¼Ÿ)
    -   **å¯¹äºäº¤å‰ç†µæŸå¤± (For Cross-Entropy Loss)**: ç­”æ¡ˆæ˜¯ $-\log(C)$ï¼Œå…¶ä¸­ $C$ æ˜¯ç±»åˆ«æ•°ã€‚å¯¹äº CIFAR-10 ($C=10$)ï¼Œè¿™ä¸ªå€¼å¤§çº¦æ˜¯ $\log(10) \approx 2.3$ã€‚
    -   è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„è°ƒè¯•å·¥å…·ï¼šå¦‚æœä½ åœ¨è®­ç»ƒå¼€å§‹æ—¶ï¼ˆæƒé‡éšæœºåˆå§‹åŒ–ï¼‰æ²¡æœ‰çœ‹åˆ°æ¥è¿‘è¿™ä¸ªå€¼çš„æŸå¤±ï¼Œé‚£ä¹ˆä½ çš„ä»£ç å¯èƒ½æœ‰bugã€‚

---
