### [ğŸ“š] è§†é¢‘å­¦ä¹ è„šæ‰‹æ¶: Lecture 10: Training Neural Networks (Part 1)

### ä¸€ã€æ ¸å¿ƒå†…å®¹å¤§çº² (Core Content Outline)
-   **å‰æƒ…å›é¡¾: ç¡¬ä»¶ä¸è½¯ä»¶ (Last Time: Hardware and Software)**
    -   å›é¡¾ CPU, GPU, TPU ç­‰ä¸åŒç¡¬ä»¶ç±»å‹åŠå…¶åº”ç”¨ (Reviewed different hardware types like CPU, GPU, and TPU and their applications).
    -   è®¨è®ºé™æ€å›¾ä¸åŠ¨æ€å›¾çš„å·®å¼‚ (Discussed the differences between static and dynamic graphs).
    -   æ¯”è¾ƒ PyTorch å’Œ TensorFlow ç­‰è½¯ä»¶ç³»ç»ŸåŠå…¶ä¼˜åŠ£ (Compared software systems like PyTorch and TensorFlow and their trade-offs).
-   **è®­ç»ƒç¥ç»ç½‘ç»œæ€»è§ˆ (Overview of Training Neural Networks)**
    -   æœ¬æ¬¡è¯¾ç¨‹åŠåç»­è¯¾ç¨‹å°†æ¶µç›–çš„ä¸‰ä¸ªä¸»è¦é˜¶æ®µ (Three main phases covered in this lecture and the next):
        1.  **ä¸€æ¬¡æ€§è®¾ç½® (One-time Setup)**: è®­ç»ƒè¿‡ç¨‹å¼€å§‹å‰çš„å‡†å¤‡å·¥ä½œ (Preparation before the training process starts).
            -   æ¿€æ´»å‡½æ•° (Activation Functions)
            -   æ•°æ®é¢„å¤„ç† (Data Preprocessing)
            -   æƒé‡åˆå§‹åŒ– (Weight Initialization)
            -   æ­£åˆ™åŒ– (Regularization)
        2.  **è®­ç»ƒåŠ¨æ€ (Training Dynamics)**: ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„è°ƒæ•´ (Adjustments during the optimization process).
            -   å­¦ä¹ ç‡è°ƒåº¦ (Learning Rate Schedules)
            -   å¤§æ‰¹é‡è®­ç»ƒ (Large-batch Training)
            -   è¶…å‚æ•°ä¼˜åŒ– (Hyperparameter Optimization)
        3.  **è®­ç»ƒåå¤„ç† (After Training)**: æ¨¡å‹è®­ç»ƒå®Œæˆåçš„é¢å¤–æ­¥éª¤ (Additional steps after model training).
            -   æ¨¡å‹é›†æˆ (Model Ensembles)
            -   è¿ç§»å­¦ä¹  (Transfer Learning)
-   **æ¿€æ´»å‡½æ•° (Activation Functions)**
    -   **å¿…è¦æ€§**: éçº¿æ€§æ¿€æ´»å‡½æ•°å¯¹ç¥ç»ç½‘ç»œçš„å¤„ç†èƒ½åŠ›è‡³å…³é‡è¦ï¼Œé˜²æ­¢å¤šå±‚çº¿æ€§æ“ä½œé€€åŒ–ä¸ºå•å±‚çº¿æ€§æ“ä½œ (Non-linear activation functions are critical for neural network processing power, preventing multi-layer linear operations from collapsing into a single linear layer).
    -   **Sigmoid æ¿€æ´»å‡½æ•° (Sigmoid Activation Function)**: $\sigma(x) = 1 / (1 + e^{-x})$
        -   å°†æ•°å€¼å‹ç¼©åˆ° (Squashes numbers to range).
        -   å†å²ä¸Šæµè¡Œï¼Œå¯ä»¥è§£é‡Šä¸ºç¥ç»å…ƒçš„é¥±å’Œâ€œå‘æ”¾ç‡â€æˆ–æ¦‚ç‡ (Historically popular as it can be interpreted as a saturating "firing rate" of a neuron or a probability).
        -   **3 ä¸ªä¸»è¦é—®é¢˜ (3 Main Problems)**:
            1.  **é¥±å’Œç¥ç»å…ƒâ€œæ€æ­»â€æ¢¯åº¦ (Saturated neurons "kill" the gradients)**:
                -   å½“è¾“å…¥ `x` è¿‡å°æˆ–è¿‡å¤§æ—¶ï¼Œå±€éƒ¨æ¢¯åº¦æ¥è¿‘äºé›¶ï¼Œå¯¼è‡´æƒé‡æ›´æ–°éå¸¸ç¼“æ…¢æˆ–åœæ­¢ (When input `x` is very small or very large, the local gradient is very close to zero, leading to very slow or no weight updates).
                -   è¿™åœ¨æ·±åº¦ç½‘ç»œä¸­ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ (This leads to the vanishing gradient problem in deep networks).
            2.  **è¾“å‡ºéé›¶å‡å€¼ (Outputs are not zero-centered)**:
                -   æ‰€æœ‰ Sigmoid è¾“å‡ºéƒ½æ˜¯æ­£æ•° (All Sigmoid outputs are positive).
                -   å¯¼è‡´æƒé‡æ¢¯åº¦å§‹ç»ˆåŒå·ï¼Œå¼•èµ·è®­ç»ƒè¿‡ç¨‹ä¸­çš„é”¯é½¿å½¢ä¼˜åŒ–è·¯å¾„ï¼Œé™ä½æ”¶æ•›é€Ÿåº¦ (Causes weight gradients to always have the same sign, leading to zig-zagging optimization paths and slower convergence).
            3.  `exp()` **è®¡ç®—æˆæœ¬é«˜ (exp() is computationally expensive)**:
                -   æŒ‡æ•°å‡½æ•°è®¡ç®—å¤æ‚ï¼Œåœ¨ CPU å’Œç§»åŠ¨è®¾å¤‡ä¸Šæ•ˆç‡è¾ƒä½ (Exponential function is computationally complex, less efficient on CPUs and mobile devices).
    -   **Tanh æ¿€æ´»å‡½æ•° (Tanh Activation Function)**: `tanh(x)`
        -   å°†æ•°å€¼å‹ç¼©åˆ° [-1, 1] èŒƒå›´ (Squashes numbers to range [-1, 1]).
        -   **é›¶å‡å€¼ (Zero-centered)**: è§£å†³äº† Sigmoid çš„è¾“å‡ºéé›¶å‡å€¼é—®é¢˜ (Solves Sigmoid's non-zero-centered output problem).
        -   **ä»å­˜åœ¨æ¢¯åº¦é¥±å’Œé—®é¢˜ (Still kills gradients when saturated)**: æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ä¾æ—§å­˜åœ¨ (Vanishing gradient problem persists).
    -   **ReLU (ä¿®æ­£çº¿æ€§å•å…ƒ) æ¿€æ´»å‡½æ•° (ReLU (Rectified Linear Unit) Activation Function)**: `f(x) = max(0, x)`
        -   åœ¨æ­£å€¼åŒºåŸŸä¸é¥±å’Œ (Does not saturate in the positive region).
        -   è®¡ç®—æ•ˆç‡é«˜ï¼Œåªéœ€ç®€å•é˜ˆå€¼åˆ¤æ–­ (Very computationally efficient, only a simple threshold check).
        -   å®è·µä¸­æ”¶æ•›é€Ÿåº¦æ¯” Sigmoid/Tanh å¿«å¾—å¤š (e.g., 6å€) (Converges much faster than Sigmoid/Tanh in practice, e.g., 6x).
        -   **è¾“å‡ºéé›¶å‡å€¼ (Not zero-centered output)**:
        -   **â€œæ­»äº¡ ReLUâ€é—®é¢˜ ("Dying ReLU" Problem)**:
            -   å½“è¾“å…¥ `x < 0` æ—¶ï¼Œæ¢¯åº¦æ­£å¥½ä¸ºé›¶ï¼Œå¯¼è‡´ç¥ç»å…ƒåœæ­¢å­¦ä¹  (When input `x < 0`, the gradient is exactly zero, causing the neuron to stop learning).
            -   ä¸€æ—¦ ReLU ç¥ç»å…ƒè¿›å…¥æ­¤çŠ¶æ€ï¼Œå…¶æƒé‡å°†ä¸å†æ›´æ–° (Once a ReLU neuron enters this state, its weights will never update).
            -   è§£å†³æ–¹æ¡ˆ: æœ‰æ—¶å°† ReLU ç¥ç»å…ƒçš„åå·®åˆå§‹åŒ–ä¸ºç•¥æ­£çš„å€¼ (e.g., 0.01) (Sometimes initialize ReLU neurons with slightly positive biases (e.g., 0.01)).
    -   **Leaky ReLU æ¿€æ´»å‡½æ•° (Leaky ReLU Activation Function)**: `f(x) = max(0.01x, x)`
        -   åœ¨è´Ÿå€¼åŒºåŸŸæœ‰å°çš„æ­£æ–œç‡ï¼Œå› æ­¤ä¸ä¼šé¥±å’Œä¸”ä¸ä¼šâ€œæ­»äº¡â€ (Has a small positive slope in the negative region, so it doesn't saturate or "die").
        -   è®¡ç®—æ•ˆç‡é«˜ (Computationally efficient).
        -   åœ¨è´Ÿå€¼åŒºåŸŸçš„æ–œç‡ (e.g., 0.01) æ˜¯éœ€è¦è°ƒä¼˜çš„è¶…å‚æ•° (The slope (e.g., 0.01) in the negative region is a hyperparameter to tune).
    -   **Parametric Rectifier (PReLU) æ¿€æ´»å‡½æ•° (Parametric Rectifier (PReLU) Activation Function)**: `f(x) = max(\alpha x, x)`
        -   ç±»ä¼¼ Leaky ReLUï¼Œä½†è´Ÿå€¼åŒºåŸŸçš„æ–œç‡ `\alpha` æ˜¯ç½‘ç»œå¯å­¦ä¹ çš„å‚æ•° (Similar to Leaky ReLU, but the slope `\alpha` in the negative region is a learnable parameter of the network).
    -   **Exponential Linear Unit (ELU) æ¿€æ´»å‡½æ•° (Exponential Linear Unit (ELU) Activation Function)**: `f(x) = x` (if `x > 0`), `f(x) = \alpha (exp(x) - 1)` (if `x <= 0`)
        -   å…·æœ‰ ReLU çš„æ‰€æœ‰ä¼˜ç‚¹ (All benefits of ReLU).
        -   è¾“å‡ºæ›´æ¥è¿‘é›¶å‡å€¼ (Closer to zero mean outputs).
        -   è´Ÿé¥±å’ŒåŒºåŸŸå¢åŠ äº†å¯¹å™ªå£°çš„é²æ£’æ€§ (Negative saturation regime adds robustness to noise).
        -   è®¡ç®—éœ€è¦ `exp()` å‡½æ•°ï¼Œå› æ­¤è®¡ç®—æˆæœ¬è¾ƒé«˜ (Computation requires `exp()` function, thus computationally expensive).
    -   **Scaled Exponential Linear Unit (SELU) æ¿€æ´»å‡½æ•° (Scaled Exponential Linear Unit (SELU) Activation Function)**:
        -   ELU çš„ç¼©æ”¾ç‰ˆæœ¬ï¼Œåœ¨æ·±åº¦ç½‘ç»œä¸­è¡¨ç°æ›´å¥½ (Scaled version of ELU that works better for deep networks).
        -   å…·æœ‰â€œè‡ªå½’ä¸€åŒ–â€ç‰¹æ€§ï¼Œå¯ä»¥åœ¨ä¸ä½¿ç”¨ Batch Normalization çš„æƒ…å†µä¸‹è®­ç»ƒæ·±åº¦ç½‘ç»œ (Has a "Self-Normalizing" property; can train deep SELU networks without Batch Normalization).
        -   æ•°å­¦æ¨å¯¼å¤æ‚ (Derivation is complex).
    -   **æ¿€æ´»å‡½æ•°æ€§èƒ½æ€»ç»“ (Activation Function Performance Summary)**
        -   CIFAR10 ä¸Šçš„å‡†ç¡®ç‡æ¯”è¾ƒæ˜¾ç¤ºï¼Œå¤§å¤šæ•°ç°ä»£æ¿€æ´»å‡½æ•°åœ¨æ€§èƒ½ä¸Šå·®å¼‚ä¸å¤§ (Accuracy comparison on CIFAR10 shows most modern activation functions have similar performance).
        -   åœ¨å®è·µä¸­ï¼Œä¸åŒæ¿€æ´»å‡½æ•°ä¹‹é—´çš„æ€§èƒ½å·®å¼‚é€šå¸¸åœ¨ 1% ä»¥å†…ï¼Œå¹¶ä¸”è¶‹åŠ¿ä¸ä¸€è‡´ (In practice, performance differences are usually within 1% and trends are not consistent).
        -   **å»ºè®® (Advice)**:
            -   ä¸è¦è¿‡åº¦æ€è€ƒï¼Œç›´æ¥ä½¿ç”¨ **ReLU** (Don't think too hard. Just use ReLU). [27:39]
            -   å¦‚æœéœ€è¦â€œå‹æ¦¨â€æœ€å 0.1% çš„æ€§èƒ½ï¼Œå¯ä»¥å°è¯• Leaky ReLU / ELU / SELU / GELU (Try out Leaky ReLU / ELU / SELU / GELU if you need to squeeze that last 0.1%). [27:48]
            -   **ä¸è¦ä½¿ç”¨ Sigmoid æˆ– Tanh**ï¼Œå®ƒä»¬é€šå¸¸ä¼šå¯¼è‡´ç½‘ç»œéš¾ä»¥æ”¶æ•› (Don't use sigmoid or tanh, they often prevent networks from converging). [28:18]
-   **æ•°æ®é¢„å¤„ç† (Data Preprocessing)**
    -   **ç›®çš„**: ä½¿æ•°æ®æ›´é€‚åˆé«˜æ•ˆçš„ç¥ç»ç½‘ç»œè®­ç»ƒ (Purpose: Make data more amenable to efficient neural network training). [30:57]
    -   **å¸¸è§æŠ€æœ¯ (Common Techniques)**:
        1.  **é›¶å‡å€¼åŒ– (Zero-centering)**:
            -   ä»æ¯ä¸ªç‰¹å¾ä¸­å‡å»è®­ç»ƒé›†çš„å‡å€¼ï¼Œä½¿æ•°æ®ä¸­å¿ƒä½äºåŸç‚¹ (Subtract the mean of the training data from each feature, centering the data at the origin). [31:56]
            -   ä»£ç ç¤ºä¾‹ (Code Example): `X -= np.mean(X, axis = 0)` [31:56]
            -   **é‡è¦æ€§**: é¿å…æ‰€æœ‰æƒé‡æ¢¯åº¦å§‹ç»ˆåŒå·çš„é—®é¢˜ (Importance: Avoids the problem of weight gradients always having the same sign). [32:19]
        2.  **å½’ä¸€åŒ– (Normalization)**:
            -   å°†æ¯ä¸ªç‰¹å¾é™¤ä»¥å…¶åœ¨è®­ç»ƒé›†ä¸Šçš„æ ‡å‡†å·®ï¼Œä½¿æ¯ä¸ªç‰¹å¾å…·æœ‰å•ä½æ–¹å·® (Divide each feature by its standard deviation on the training set, giving each feature unit variance). [32:00]
            -   ä»£ç ç¤ºä¾‹ (Code Example): `X /= np.std(X, axis = 0)` [32:00]
        -   **æ•ˆæœ**: é¢„å¤„ç†åçš„æ•°æ®æ›´é›†ä¸­ï¼ŒæŸå¤±å‡½æ•°å¯¹æƒé‡å˜åŒ–ä¸å†æ•æ„Ÿï¼Œæ›´å®¹æ˜“ä¼˜åŒ– (Effect: Preprocessed data is more concentrated, making the loss function less sensitive to weight changes and easier to optimize). [34:17]
    -   **é«˜çº§æŠ€æœ¯ (Advanced Techniques)** (éå›¾åƒæ•°æ®ä¸­æ›´å¸¸è§) (More common in non-image data):
        -   **PCA (ä¸»æˆåˆ†åˆ†æ) (Principal Component Analysis)**: å¯¹æ•°æ®è¿›è¡Œå»ç›¸å…³ï¼Œæ—‹è½¬æ•°æ®äº‘ä½¿å…¶ç‰¹å¾ä¸åæ ‡è½´å¯¹é½ (Decorrelates data, rotating the data cloud so features align with coordinate axes). [33:09]
        -   **ç™½åŒ– (Whitening)**: åœ¨å»ç›¸å…³æ•°æ®åè¿›ä¸€æ­¥ç¼©æ”¾ï¼Œä½¿æ¯ä¸ªç‰¹å¾å…·æœ‰å•ä½æ–¹å·®ï¼Œåæ–¹å·®çŸ©é˜µå˜ä¸ºå•ä½çŸ©é˜µ (Further scales decorrelated data so each feature has unit variance, making the covariance matrix an identity matrix). [33:28]
        -   **å›¾åƒæ•°æ®é¢„å¤„ç† (Data Preprocessing for Images)**:
            -   **å‡å»å‡å€¼å›¾åƒ (Subtract the mean image)** (ä¾‹å¦‚ AlexNet): è®¡ç®—è®­ç»ƒé›†æ‰€æœ‰å›¾åƒçš„å‡å€¼å›¾åƒ (32x32x3 æ•°ç»„)ï¼Œç„¶åä»æ¯ä¸ªå›¾åƒä¸­å‡å» (Compute the mean image of all training images (32x32x3 array), then subtract it from each image). [36:51]
            -   **å‡å»æ¯é€šé“å‡å€¼ (Subtract per-channel mean)** (ä¾‹å¦‚ VGGNet): è®¡ç®—æ¯ä¸ªé¢œè‰²é€šé“ï¼ˆR, G, Bï¼‰çš„å‡å€¼ï¼Œç„¶åä»å¯¹åº”é€šé“çš„åƒç´ å€¼ä¸­å‡å» (Compute the mean for each color channel (R, G, B), then subtract it from the pixel values of the corresponding channel). [37:13]
            -   **å‡å»æ¯é€šé“å‡å€¼å¹¶é™¤ä»¥æ¯é€šé“æ ‡å‡†å·® (Subtract per-channel mean and divide by per-channel std)** (ä¾‹å¦‚ ResNet): è¿™æ˜¯æœ€å¸¸è§çš„å›¾åƒé¢„å¤„ç†æ–¹æ³• (This is the most common image preprocessing method). [37:29]
            -   **æ³¨æ„äº‹é¡¹**: å§‹ç»ˆåœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—ç»Ÿè®¡æ•°æ®ï¼Œå¹¶åº”ç”¨äºè®­ç»ƒå’Œæµ‹è¯•é›†ï¼Œä»¥æ¨¡æ‹Ÿå®é™…éƒ¨ç½²åœºæ™¯ (Always compute statistics on the training set and apply them to both training and test sets to simulate real-world deployment). [37:57]
            -   **ä¸å¸¸è§**: å¯¹å›¾åƒæ•°æ®è¿›è¡Œ PCA æˆ–ç™½åŒ–é€šå¸¸ä¸å¸¸è§ (PCA or whitening are not common for image data). [36:40]
-   **æƒé‡åˆå§‹åŒ– (Weight Initialization)**
    -   **é—®é¢˜**: å¦‚æœæ‰€æœ‰æƒé‡éƒ½åˆå§‹åŒ–ä¸ºé›¶æˆ–å¸¸æ•°ï¼Œæ‰€æœ‰ç¥ç»å…ƒå°†å­¦ä¹ ç›¸åŒçš„ä¸œè¥¿ï¼Œå¯¼è‡´å¯¹ç§°æ€§é—®é¢˜å’Œæ¢¯åº¦ä¸ºé›¶ï¼Œæ— æ³•å­¦ä¹  (Problem: If all weights are initialized to zero or constants, all neurons learn the same thing, leading to symmetry issues and zero gradients, preventing learning). [39:17, 40:01]
    -   **è§£å†³æ–¹æ¡ˆ: å°éšæœºæ•° (Small Random Numbers)**:
        -   ä»é›¶å‡å€¼çš„é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·å°éšæœºæ•° (e.g., std=0.01) è¿›è¡Œåˆå§‹åŒ– (Initialize with small random numbers sampled from a Gaussian distribution with zero mean (e.g., std=0.01)). [40:19]
        -   å¯¹å°å‹ç½‘ç»œæœ‰æ•ˆï¼Œä½†å¯¹æ·±åº¦ç½‘ç»œæœ‰ç¼ºé™· (Works okay for small networks but has problems with deeper networks). [40:40]
        -   **æ¿€æ´»ç»Ÿè®¡ (Activation Statistics)**:
            -   å¯¹äºæ·±å±‚ç½‘ç»œ (ä¾‹å¦‚ 6 å±‚ tanh ç½‘ç»œï¼Œéšè—å±‚å¤§å° 4096)ï¼Œå¦‚æœæƒé‡è¿‡å°ï¼Œæ¿€æ´»å€¼ä¼šè¶‹å‘äºé›¶ï¼Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤± (For deep networks (e.g., 6-layer tanh network, hidden size 4096), if weights are too small, activations tend to zero, leading to vanishing gradients). [41:20]
            -   å¦‚æœæƒé‡è¿‡å¤§ (ä¾‹å¦‚ std=0.05)ï¼Œæ¿€æ´»å€¼ä¼šé¥±å’Œï¼Œæ¢¯åº¦ä¹Ÿä¼šè¶‹å‘äºé›¶ (If weights are too large (e.g., std=0.05), activations saturate, also leading to zero gradients). [42:51]
    -   **Xavier åˆå§‹åŒ– (Xavier Initialization)**:
        -   é€šè¿‡è®¾ç½®æ ‡å‡†å·®ä¸º `1 / sqrt(Din)` æ¥è§£å†³æ¿€æ´»å€¼è¿‡å°æˆ–è¿‡å¤§çš„é—®é¢˜ï¼Œå…¶ä¸­ `Din` æ˜¯è¾“å…¥ç»´åº¦ (Solves the issue of too small/large activations by setting std to `1 / sqrt(Din)`, where `Din` is the input dimension). [43:51]
        -   **æ¨å¯¼**: ç›®æ ‡æ˜¯ä½¿è¾“å‡ºçš„æ–¹å·®ç­‰äºè¾“å…¥çš„æ–¹å·®ï¼Œä»¥ä¿æŒæ¿€æ´»å€¼åˆ†å¸ƒçš„ç¨³å®šæ€§ (Derivation: Goal is for variance of output to equal variance of input, maintaining stable activation distribution). [44:45]
        -   å¯¹äºå…¨è¿æ¥å±‚ï¼Œ`Din` æ˜¯è¾“å…¥ç¥ç»å…ƒçš„æ•°é‡ (For fully-connected layers, `Din` is the number of input neurons).
        -   å¯¹äºå·ç§¯å±‚ï¼Œ`Din` æ˜¯ `kernel_size * kernel_size * input_channels` (For conv layers, `Din` is `kernel_size * kernel_size * input_channels`). [44:21]
        -   **é—®é¢˜**: Xavier å‡å®šæ¿€æ´»å‡½æ•°æ˜¯é›¶å‡å€¼çš„ï¼ˆä¾‹å¦‚ Tanhï¼‰ï¼Œå¯¹ ReLU ç­‰éé›¶å‡å€¼æ¿€æ´»å‡½æ•°æ•ˆæœä¸å¥½ï¼Œä¼šå¯¼è‡´æ¿€æ´»å€¼ä»è¶‹äºé›¶ (Problem: Xavier assumes zero-centered activation functions (like Tanh), but performs poorly with non-zero-centered ones like ReLU, causing activations to collapse to zero). [47:33]
    -   **Kaiming / MSRA åˆå§‹åŒ– (Kaiming / MSRA Initialization)**:
        -   ä¸º ReLU æ¿€æ´»å‡½æ•°è®¾è®¡çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œæ ‡å‡†å·®ä¸º `sqrt(2 / Din)` (Initialization method designed for ReLU activation functions, std is `sqrt(2 / Din)`). [48:04]
        -   è§£å†³äº† ReLU çš„â€œæ­»äº¡â€é—®é¢˜ï¼Œä½¿æ¿€æ´»å€¼åœ¨æ·±å±‚ç½‘ç»œä¸­ä»èƒ½ä¿æŒè‰¯å¥½å°ºåº¦ (Solves the "dying ReLU" problem, keeping activations well-scaled in deep networks). [48:17]
    -   **æ®‹å·®ç½‘ç»œåˆå§‹åŒ– (Weight Initialization for Residual Networks)**:
        -   å¯¹äºæ®‹å·®å—å†…éƒ¨çš„ç¬¬ä¸€ä¸ªå·ç§¯å±‚ï¼Œä½¿ç”¨ MSRA åˆå§‹åŒ– (For the first convolution layer within a residual block, use MSRA initialization). [50:25]
        -   å¯¹äºæ®‹å·®å—å†…éƒ¨çš„ç¬¬äºŒä¸ªå·ç§¯å±‚ï¼Œå°†å…¶æƒé‡åˆå§‹åŒ–ä¸ºé›¶ (For the second convolution layer within a residual block, initialize its weights to zero). [50:29]
        -   è¿™æ ·ï¼Œåœ¨åˆå§‹åŒ–æ—¶ï¼Œæ®‹å·®å—è¿‘ä¼¼äºä¸€ä¸ªæ’ç­‰æ˜ å°„ï¼Œé¿å…äº†æ–¹å·®çˆ†ç‚¸é—®é¢˜ (This way, at initialization, the residual block approximates an identity function, preventing variance explosion). [50:39]
    -   **æ€»ç»“**: é€‚å½“çš„åˆå§‹åŒ–æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸ (Proper initialization is an active area of research). [50:46]
-   **æ­£åˆ™åŒ– (Regularization)**
    -   **ç›®çš„**: é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ® (Purpose: Prevent the model from overfitting the training data). [52:00]
    -   **å¸¸è§æ¨¡å¼ (Common Pattern)**:
        -   **è®­ç»ƒæ—¶**: æ·»åŠ æŸç§éšæœºæ€§ (Training: Add some kind of randomness).
        -   **æµ‹è¯•æ—¶**: å¯¹éšæœºæ€§è¿›è¡Œå¹³å‡ï¼ˆæœ‰æ—¶è¿‘ä¼¼ï¼‰ (Testing: Average out the randomness (sometimes approximate)).
    -   **L2 æ­£åˆ™åŒ– (L2 Regularization)** (ä¹Ÿç§°ä¸ºæƒé‡è¡°å‡ Weight Decay):
        -   åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒé‡å¹³æ–¹å’Œçš„æƒ©ç½šé¡¹ (Adds a penalty term of the sum of squared weights to the loss function). [52:33]
        -   å¼ºåˆ¶æƒé‡å˜å°ï¼Œä»è€Œä½¿æ¨¡å‹æ›´ç®€å•ï¼Œå‡å°‘è¿‡æ‹Ÿåˆ (Forces weights to be smaller, making the model simpler and reducing overfitting).
    -   **L1 æ­£åˆ™åŒ– (L1 Regularization)**:
        -   åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒé‡ç»å¯¹å€¼å’Œçš„æƒ©ç½šé¡¹ (Adds a penalty term of the sum of absolute values of weights to the loss function). [52:45]
        -   å€¾å‘äºä½¿ä¸é‡è¦çš„ç‰¹å¾çš„æƒé‡å˜ä¸ºé›¶ï¼Œå®ç°ç‰¹å¾é€‰æ‹© (Tends to drive weights of unimportant features to zero, enabling feature selection).
    -   **Elastic Net (L1 + L2)**:
        -   ç»“åˆ L1 å’Œ L2 æ­£åˆ™åŒ– (Combines L1 and L2 regularization). [52:49]
    -   **Dropout (éšæœºå¤±æ´»)**:
        -   **è®­ç»ƒæ—¶**: åœ¨æ¯ä¸ªå‰å‘ä¼ æ’­ä¸­ï¼Œéšæœºå°†éƒ¨åˆ†ç¥ç»å…ƒï¼ˆåŠå…¶è¿æ¥ï¼‰çš„è¾“å‡ºè®¾ç½®ä¸ºé›¶ã€‚ä¸¢å¼ƒçš„æ¦‚ç‡ `p` æ˜¯è¶…å‚æ•°ï¼Œ0.5 å¸¸è§ (Training: Randomly set outputs of some neurons (and their connections) to zero in each forward pass. Dropping probability `p` is a hyperparameter, 0.5 is common). [53:05]
        -   **æ•ˆæœ**: å¼ºåˆ¶ç½‘ç»œå­¦ä¹ å†—ä½™è¡¨ç¤ºï¼Œé˜²æ­¢ç‰¹å¾ä¹‹é—´çš„â€œååŒé€‚åº”â€ï¼ˆå³ç¥ç»å…ƒè¿‡åº¦ä¾èµ–ç‰¹å®šè¾“å…¥ï¼‰ (Effect: Forces the network to have a redundant representation; prevents co-adaptation of features). [53:52]
        -   **æµ‹è¯•æ—¶**: æ‰€æœ‰ç¥ç»å…ƒéƒ½ä¿æŒæ¿€æ´»ï¼Œä½†å…¶è¾“å‡ºæŒ‰ `p` ç¼©æ”¾ï¼Œä»¥è¿‘ä¼¼å¹³å‡è®­ç»ƒæ—¶çš„éšæœºæ€§ (Testing: All neurons are active, but their outputs are scaled by `p` to approximate averaging out randomness from training). [58:41]
        -   **â€œå€’ç½® Dropout (Inverted Dropout)â€**: æ›´å¸¸è§çš„å®ç°ï¼Œè®­ç»ƒæ—¶ç›´æ¥å°†ä¿ç•™çš„ç¥ç»å…ƒè¾“å‡ºä¹˜ä»¥ `1/p`ï¼Œæµ‹è¯•æ—¶æ— éœ€ç¼©æ”¾ (More common implementation: Scale retained neuron outputs by `1/p` during training; no scaling needed during testing). [59:52]
        -   **åº”ç”¨**: é€šå¸¸åº”ç”¨äºç½‘ç»œæœ«ç«¯çš„å…¨è¿æ¥å±‚ï¼Œè¿™äº›å±‚å‚æ•°é‡å¤§ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ (Typically applied to large fully-connected layers at the end of the network, which are prone to overfitting). [60:58]
    -   **Batch Normalization (æ‰¹é‡å½’ä¸€åŒ–)**:
        -   **è®­ç»ƒæ—¶**: å¯¹æ¯ä¸ª Batch çš„ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿ç”¨å½“å‰ Batch çš„å‡å€¼å’Œæ–¹å·® (Training: Normalize features within each batch using the mean and variance of that batch). [62:38]
        -   **æµ‹è¯•æ—¶**: ä½¿ç”¨è®­ç»ƒé›†æ‰€æœ‰ Batch çš„å›ºå®šè¿è¡Œå‡å€¼å’Œæ–¹å·®è¿›è¡Œå½’ä¸€åŒ– (Testing: Use fixed running mean and variance accumulated from all training batches for normalization). [63:08]
        -   **ä½œä¸ºæ­£åˆ™åŒ–å™¨**: Batch Normalization æœ¬èº«å…·æœ‰æ­£åˆ™åŒ–æ•ˆæœï¼Œå› ä¸ºæ¯ä¸ªæ ·æœ¬çš„å½’ä¸€åŒ–éƒ½ä¾èµ–äºå½“å‰ Batch çš„å…¶ä»–æ ·æœ¬ï¼Œå¼•å…¥äº†éšæœºæ€§ (Batch Normalization itself acts as a regularizer because normalization of each sample depends on other samples in the batch, introducing randomness). [62:45]
        -   **å½±å“**: ResNet ç­‰è¾ƒæ–°çš„æ¶æ„é€šå¸¸åªä½¿ç”¨ L2 æ­£åˆ™åŒ–å’Œ Batch Normalizationï¼Œè€Œä¸å†ä½¿ç”¨ Dropout (Newer architectures like ResNet often only use L2 regularization and Batch Normalization, and no longer use Dropout). [63:18]
    -   **æ•°æ®å¢å¼º (Data Augmentation)**:
        -   **åŸç†**: å¯¹åŸå§‹è®­ç»ƒæ•°æ®è¿›è¡Œä¿ç•™æ ‡ç­¾çš„éšæœºå˜æ¢ï¼Œäººä¸ºåœ°å¢åŠ è®­ç»ƒé›†çš„å¤§å°å’Œå¤šæ ·æ€§ (Principle: Apply label-preserving random transformations to the original training data, artificially increasing the size and diversity of the training set). [64:25]
        -   **æ•ˆæœ**: æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ (Effect: Improves model generalization and prevents overfitting).
        -   **å¸¸è§å›¾åƒå¢å¼ºæ–¹æ³• (Common Image Augmentation Methods)**:
            -   æ°´å¹³ç¿»è½¬ (Horizontal flips) [64:41]
            -   éšæœºè£å‰ªå’Œç¼©æ”¾ (Random crops and scales) [65:28]
            -   è‰²å½©æŠ–åŠ¨ (Color jittering): éšæœºè°ƒæ•´å›¾åƒçš„å¯¹æ¯”åº¦å’Œäº®åº¦ï¼Œæˆ–æ›´å¤æ‚åœ°æ²¿ä¸»æˆåˆ†æ–¹å‘æ·»åŠ é¢œè‰²åç§» (Randomly adjust contrast and brightness, or more complex color offsets along principal component directions). [66:44]
            -   å…¶ä»– (Others): å¹³ç§» (translation), æ—‹è½¬ (rotation), æ‹‰ä¼¸ (stretching), å‰ªåˆ‡ (shearing), é•œå¤´ç•¸å˜ (lens distortions) ç­‰ã€‚å¯ä»¥æ ¹æ®å…·ä½“é—®é¢˜å‘æŒ¥åˆ›æ„ (translation, rotation, stretching, shearing, lens distortions, etc. Be creative for your problem). [66:51]
        -   **æµ‹è¯•æ—¶**: é€šå¸¸ä½¿ç”¨åŸå§‹å›¾åƒè¿›è¡Œè¯„ä¼°ï¼Œæˆ–å¯¹å›ºå®šé›†åˆçš„å¢å¼ºå›¾åƒè¿›è¡Œå¹³å‡é¢„æµ‹ (Testing: Usually evaluate on original images, or average predictions over a fixed set of augmented images). [65:58]
    -   **DropConnect**:
        -   **è®­ç»ƒæ—¶**: éšæœºå°†ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥ï¼ˆæƒé‡ï¼‰è®¾ç½®ä¸ºé›¶ (Training: Randomly set connections (weights) between neurons to zero). [68:08]
        -   **æµ‹è¯•æ—¶**: ä½¿ç”¨æ‰€æœ‰è¿æ¥ (Testing: Use all connections).
    -   **Fractional Max Pooling (åˆ†å½¢æœ€å¤§æ± åŒ–)**:
        -   **è®­ç»ƒæ—¶**: ä½¿ç”¨éšæœºåŒ–çš„æ± åŒ–åŒºåŸŸ (Training: Use randomized pooling regions). [68:24]
        -   **æµ‹è¯•æ—¶**: å¹³å‡ä¸åŒæ ·æœ¬çš„é¢„æµ‹ (Testing: Average predictions over different samples).
    -   **Stochastic Depth (éšæœºæ·±åº¦)**:
        -   **è®­ç»ƒæ—¶**: éšæœºè·³è¿‡ ResNet ä¸­çš„ä¸€äº›æ®‹å·®å— (Training: Randomly skip some residual blocks in ResNet). [69:01]
        -   **æµ‹è¯•æ—¶**: ä½¿ç”¨æ•´ä¸ªç½‘ç»œ (Testing: Use the whole network).
    -   **Cutout (å‰ªè£)**:
        -   **è®­ç»ƒæ—¶**: å°†å›¾åƒä¸­çš„éšæœºåŒºåŸŸè®¾ç½®ä¸ºé›¶ (Training: Set random image regions to zero). [69:31]
        -   **æµ‹è¯•æ—¶**: ä½¿ç”¨æ•´ä¸ªå›¾åƒ (Testing: Use the whole image).
        -   **é€‚ç”¨æ€§**: å¯¹ CIFAR ç­‰å°å‹æ•°æ®é›†æ•ˆæœå¾ˆå¥½ï¼Œå¯¹ ImageNet ç­‰å¤§å‹æ•°æ®é›†ä¸å¤ªå¸¸è§ (Works very well for small datasets like CIFAR, less common for large datasets like ImageNet).
    -   **Mixup (æ··åˆ)**:
        -   **è®­ç»ƒæ—¶**: éšæœºæ··åˆä¸€å¯¹å›¾åƒçš„åƒç´ å€¼ï¼Œå¹¶æŒ‰æ¯”ä¾‹æ··åˆå…¶æ ‡ç­¾ (Training: Randomly blend pixel values of pairs of images and proportionally blend their labels). [69:58]
        -   **æµ‹è¯•æ—¶**: ä½¿ç”¨åŸå§‹å›¾åƒ (Testing: Use original images).
        -   **æ•ˆæœ**: å¼ºåˆ¶æ¨¡å‹åœ¨è®­ç»ƒæ—¶æ¢ç´¢æ ·æœ¬ä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡ï¼Œæé«˜æ³›åŒ–èƒ½åŠ› (Effect: Forces the model to explore smooth transitions between samples during training, improving generalization).

### äºŒã€å…³é”®æœ¯è¯­å®šä¹‰ (Key Term Definitions)
-   **é™æ€å›¾ (Static Graph)**: åœ¨æ‰§è¡Œå‰å®Œå…¨å®šä¹‰è®¡ç®—å›¾ï¼Œç¼–è¯‘åæ‰§è¡Œï¼Œé€šå¸¸ç”¨äº TensorFlowã€‚ (A computational graph defined entirely before execution, then compiled and run, typically seen in TensorFlow.)
-   **åŠ¨æ€å›¾ (Dynamic Graph)**: è®¡ç®—å›¾åœ¨è¿è¡Œæ—¶åŠ¨æ€æ„å»ºï¼Œæ›´çµæ´»ï¼Œå¸¸ç”¨äº PyTorchã€‚ (A computational graph built dynamically at runtime, offering more flexibility, commonly used in PyTorch.)
-   **æ¿€æ´»å‡½æ•° (Activation Function)**: ç¥ç»ç½‘ç»œä¸­å¼•å…¥éçº¿æ€§çš„å‡½æ•°ï¼Œå°†ç¥ç»å…ƒçš„åŠ æƒè¾“å…¥è½¬æ¢ä¸ºè¾“å‡ºã€‚ (A function in a neural network that introduces non-linearity, transforming the weighted input of a neuron into its output.)
-   **Sigmoid æ¿€æ´»å‡½æ•° (Sigmoid Activation Function)**: ä¸€ç§ S å½¢çš„æ¿€æ´»å‡½æ•°ï¼Œå°†è¾“å…¥å‹ç¼©åˆ° (Squashes numbers to range).
-   **Tanh æ¿€æ´»å‡½æ•° (Tanh Activation Function)**: åŒæ›²æ­£åˆ‡æ¿€æ´»å‡½æ•°ï¼Œå°†è¾“å…¥å‹ç¼©åˆ° [-1, 1] èŒƒå›´ï¼Œä¸”æ˜¯é›¶å‡å€¼çš„ã€‚ (Hyperbolic tangent activation function that squashes inputs to the range [-1, 1] and is zero-centered.)
-   **ReLU (ä¿®æ­£çº¿æ€§å•å…ƒ) (ReLU (Rectified Linear Unit))**: ä¸€ç§æ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºä¸º `max(0, x)`ï¼Œå³è´Ÿè¾“å…¥ä¸ºé›¶ï¼Œæ­£è¾“å…¥ä¿æŒä¸å˜ã€‚ (An activation function that outputs `max(0, x)`, meaning negative inputs become zero while positive inputs remain unchanged.)
-   **æ­»äº¡ ReLU (Dying ReLU)**: ReLU ç¥ç»å…ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åœæ­¢æ¿€æ´»å¹¶åœæ­¢æ›´æ–°æƒé‡çš„ç°è±¡ã€‚ (A phenomenon where a ReLU neuron stops activating and its weights stop updating during training.)
-   **Leaky ReLU æ¿€æ´»å‡½æ•° (Leaky ReLU Activation Function)**: ReLU çš„å˜ä½“ï¼Œåœ¨è´Ÿå€¼åŒºåŸŸæœ‰ä¸€ä¸ªå°çš„éé›¶æ–œç‡ (e.g., 0.01x)ï¼Œä»¥é¿å…â€œæ­»äº¡ ReLUâ€é—®é¢˜ã€‚ (A variant of ReLU that has a small, non-zero slope (e.g., 0.01x) for negative inputs, to prevent the "dying ReLU" problem.)
-   **Parametric Rectifier (PReLU) æ¿€æ´»å‡½æ•° (Parametric Rectifier (PReLU) Activation Function)**: Leaky ReLU çš„å˜ä½“ï¼Œè´Ÿå€¼åŒºåŸŸçš„æ–œç‡æ˜¯å¯å­¦ä¹ çš„å‚æ•°ã€‚ (A variant of Leaky ReLU where the slope in the negative region is a learnable parameter.)
-   **ELU (æŒ‡æ•°çº¿æ€§å•å…ƒ) (ELU (Exponential Linear Unit))**: ä¸€ç§æ¿€æ´»å‡½æ•°ï¼Œåœ¨è´Ÿå€¼åŒºåŸŸä½¿ç”¨æŒ‡æ•°å‡½æ•°ï¼Œå¹¶å…·æœ‰é›¶å‡å€¼è¾“å‡ºçš„å€¾å‘ã€‚ (An activation function that uses an exponential function for negative inputs and tends to produce zero-mean outputs.)
-   **SELU (ç¼©æ”¾æŒ‡æ•°çº¿æ€§å•å…ƒ) (SELU (Scaled Exponential Linear Unit))**: ELU çš„ç¼©æ”¾ç‰ˆæœ¬ï¼Œæ—¨åœ¨ä½¿ç¥ç»ç½‘ç»œåœ¨æ·±åº¦å¢åŠ æ—¶ä¿æŒæ¿€æ´»å€¼çš„å‡å€¼å’Œæ–¹å·®ä¸å˜ï¼Œä»è€Œå®ç°â€œè‡ªå½’ä¸€åŒ–â€ã€‚ (A scaled version of ELU designed to maintain constant mean and variance of activations as network depth increases, enabling "self-normalization".)
-   **æ•°æ®é¢„å¤„ç† (Data Preprocessing)**: åœ¨å°†æ•°æ®è¾“å…¥ç¥ç»ç½‘ç»œä¹‹å‰å¯¹å…¶è¿›è¡Œè½¬æ¢ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚ (Transforming data before feeding it into a neural network to improve training efficiency and model performance.)
-   **é›¶å‡å€¼åŒ– (Zero-centering)**: æ•°æ®é¢„å¤„ç†çš„ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡ä»æ¯ä¸ªç‰¹å¾ä¸­å‡å»å‡å€¼æ¥ä½¿æ•°æ®çš„å‡å€¼ä¸ºé›¶ã€‚ (A data preprocessing technique that involves subtracting the mean from each feature to make the data have a mean of zero.)
-   **å½’ä¸€åŒ– (Normalization)**: æ•°æ®é¢„å¤„ç†çš„ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡é™¤ä»¥æ ‡å‡†å·®æ¥ç¼©æ”¾æ•°æ®ï¼Œä½¿æ¯ä¸ªç‰¹å¾å…·æœ‰å•ä½æ–¹å·®ã€‚ (A data preprocessing technique that involves scaling data by dividing by the standard deviation, so each feature has unit variance.)
-   **PCA (ä¸»æˆåˆ†åˆ†æ) (Principal Component Analysis)**: ä¸€ç§é™ç»´å’Œå»ç›¸å…³æŠ€æœ¯ï¼Œé€šè¿‡æ—‹è½¬æ•°æ®å°†å…¶ç‰¹å¾å¯¹é½åˆ°ä¸»æˆåˆ†æ–¹å‘ã€‚ (A dimensionality reduction and decorrelation technique that rotates data to align its features with principal components.)
-   **ç™½åŒ– (Whitening)**: æ•°æ®é¢„å¤„ç†çš„ä¸€ç§æ–¹æ³•ï¼Œå®ƒå»é™¤äº†ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶ä½¿æ¯ä¸ªç‰¹å¾å…·æœ‰ç›¸åŒçš„æ–¹å·®ï¼ˆé€šå¸¸ä¸º1ï¼‰ã€‚ (A data preprocessing technique that removes correlations between features and makes each feature have the same variance (usually 1).)
-   **æƒé‡åˆå§‹åŒ– (Weight Initialization)**: åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒå¼€å§‹æ—¶ï¼Œä¸ºç½‘ç»œä¸­çš„æƒé‡å‚æ•°è®¾ç½®åˆå§‹å€¼ã€‚ (Setting initial values for the weight parameters in a neural network before training begins.)
-   **Xavier åˆå§‹åŒ– (Xavier Initialization)**: ä¸€ç§æƒé‡åˆå§‹åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨ä¿æŒå„å±‚æ¿€æ´»å€¼çš„æ–¹å·®ç¨³å®šï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ã€‚é€‚ç”¨äºé›¶å‡å€¼æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ Tanhï¼‰ã€‚ (A weight initialization method aiming to keep the variance of activations stable across layers, preventing vanishing or exploding gradients. Suitable for zero-mean activation functions like Tanh.)
-   **Kaiming / MSRA åˆå§‹åŒ– (Kaiming / MSRA Initialization)**: ä¸€ç§æƒé‡åˆå§‹åŒ–æ–¹æ³•ï¼Œä¸º ReLU æ¿€æ´»å‡½æ•°è®¾è®¡ï¼Œé€šè¿‡è€ƒè™‘ ReLU çš„éçº¿æ€§ç‰¹æ€§æ¥ä¿æŒæ¿€æ´»å€¼æ–¹å·®çš„ç¨³å®šæ€§ã€‚ (A weight initialization method designed for ReLU activation functions, maintaining stable activation variance by accounting for ReLU's non-linear property.)
-   **æ­£åˆ™åŒ– (Regularization)**: æ—¨åœ¨é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›çš„ç­–ç•¥ã€‚ (Strategies aimed at preventing a model from overfitting the training data and improving its generalization ability.)
-   **L2 æ­£åˆ™åŒ– (L2 Regularization)**: ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œé€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒé‡å¹³æ–¹å’Œçš„æƒ©ç½šé¡¹æ¥å‡å°æ¨¡å‹æƒé‡ã€‚ (A regularization technique that penalizes the sum of the squares of the weights in the loss function to reduce model weights.)
-   **L1 æ­£åˆ™åŒ– (L1 Regularization)**: ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œé€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒé‡ç»å¯¹å€¼å’Œçš„æƒ©ç½šé¡¹æ¥é¼“åŠ±ç¨€ç–æ¨¡å‹ï¼ˆå³è®¸å¤šæƒé‡ä¸ºé›¶ï¼‰ã€‚ (A regularization technique that penalizes the sum of the absolute values of the weights in the loss function to encourage sparse models (i.e., many weights are zero).)
-   **Elastic Net**: L1 å’Œ L2 æ­£åˆ™åŒ–çš„ç»“åˆã€‚ (A combination of L1 and L2 regularization.)
-   **Dropout (éšæœºå¤±æ´»)**: ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºåœ°å°†éƒ¨åˆ†ç¥ç»å…ƒï¼ˆåŠå…¶è¿æ¥ï¼‰çš„è¾“å‡ºè®¾ç½®ä¸ºé›¶ã€‚ (A regularization technique where randomly selected neurons (and their connections) are temporarily dropped out of the network during training.)
-   **DropConnect**: ç±»ä¼¼äº Dropoutï¼Œä½†éšæœºæ–­å¼€çš„æ˜¯ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥ï¼ˆæƒé‡ï¼‰ï¼Œè€Œä¸æ˜¯æ•´ä¸ªç¥ç»å…ƒã€‚ (Similar to Dropout, but instead of dropping entire neurons, random connections (weights) between neurons are dropped.)
-   **æ•°æ®å¢å¼º (Data Augmentation)**: é€šè¿‡å¯¹ç°æœ‰è®­ç»ƒæ•°æ®è¿›è¡Œéšæœºå˜æ¢ï¼ˆå¦‚ç¿»è½¬ã€è£å‰ªã€ç¼©æ”¾ã€é¢œè‰²æŠ–åŠ¨ç­‰ï¼‰æ¥æ‰©å……æ•°æ®é›†ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ (Expanding the dataset by applying random transformations (e.g., flips, crops, scaling, color jitter) to existing training data, to improve model generalization.)
-   **åˆ†å½¢æœ€å¤§æ± åŒ– (Fractional Max Pooling)**: ä¸€ç§æ± åŒ–æŠ€æœ¯ï¼Œä½¿ç”¨éšæœºåŒ–çš„ã€éæ•´æ•°æ¯”ä¾‹çš„æ± åŒ–åŒºåŸŸï¼Œå¼•å…¥éšæœºæ€§ã€‚ (A pooling technique that uses randomized, non-integer ratios for pooling regions, introducing randomness.)
-   **éšæœºæ·±åº¦ (Stochastic Depth)**: ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œåœ¨è®­ç»ƒæ—¶éšæœºè·³è¿‡æ·±åº¦ç½‘ç»œï¼ˆå¦‚ ResNetï¼‰ä¸­çš„ä¸€äº›æ®‹å·®å—ã€‚ (A regularization technique that randomly skips some residual blocks in deep networks (like ResNet) during training.)
-   **Cutout (å‰ªè£)**: ä¸€ç§æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œåœ¨è®­ç»ƒå›¾åƒä¸­éšæœºé®æŒ¡ä¸€ä¸ªæ­£æ–¹å½¢åŒºåŸŸï¼Œä»¥å¼ºåˆ¶æ¨¡å‹å…³æ³¨å›¾åƒçš„æ›´å¤šéƒ¨åˆ†ã€‚ (A data augmentation technique that involves randomly masking out a square region in training images, to force the model to focus on more parts of the image.)
-   **Mixup (æ··åˆ)**: ä¸€ç§æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œé€šè¿‡çº¿æ€§æ’å€¼ç»„åˆä¸¤ä¸ªè®­ç»ƒæ ·æœ¬åŠå…¶å¯¹åº”çš„æ ‡ç­¾ï¼Œç”Ÿæˆæ–°çš„è®­ç»ƒæ ·æœ¬ã€‚ (A data augmentation technique that generates new training samples by linearly interpolating two training samples and their corresponding labels.)

### ä¸‰ã€æ ¸å¿ƒç®—æ³•ä¸ä»£ç ç‰‡æ®µ (Core Algorithms & Code Snippets)

-   **æ•°æ®é›¶å‡å€¼åŒ– (Zero-centering Data)**:
    ```python
    X -= np.mean(X, axis = 0)
    ```

-   **æ•°æ®å½’ä¸€åŒ– (Normalizing Data)**:
    ```python
    X /= np.std(X, axis = 0)
    ```

-   **æƒé‡åˆå§‹åŒ–: å°éšæœºæ•° (Weight Initialization: Small Random Numbers)**:
    ```python
    W = 0.01 * np.random.randn(Din, Dout)
    ```

-   **æƒé‡åˆå§‹åŒ–: Xavier åˆå§‹åŒ– (Weight Initialization: Xavier Initialization)**:
    ```python
    W = np.random.randn(Din, Dout) / np.sqrt(Din)
    ```

-   **æƒé‡åˆå§‹åŒ–: Kaiming / MSRA åˆå§‹åŒ– (Weight Initialization: Kaiming / MSRA Initialization)**:
    ```python
    W = np.random.randn(Din, Dout) * np.sqrt(2 / Din) # ReLU correction
    ```

-   **æ­£åˆ™åŒ–: Dropout å‰å‘ä¼ æ’­ (Regularization: Dropout Forward Pass)**:
    ```python
    # p = 0.5 # probability of keeping a unit active. higher = less dropout
    def train_step(X):
        """ X contains the data """
        # forward pass for example 3-layer neural network
        H1 = np.maximum(0, np.dot(W1, X) + b1)
        U1 = (np.random.rand(*H1.shape) < p) / p # first dropout mask. Notice /p!
        H1 *= U1 # drop!

        H2 = np.maximum(0, np.dot(W2, H1) + b2)
        U2 = (np.random.rand(*H2.shape) < p) / p # second dropout mask. Notice /p!
        H2 *= U2 # drop!

        out = np.dot(W3, H2) + b3

        # backward pass: compute gradients... (not shown)
        # perform parameter update... (not shown)
    ```

-   **æ­£åˆ™åŒ–: Dropout æµ‹è¯•æ—¶ç¼©æ”¾ (Regularization: Dropout Scaling at Test Time)**:
    ```python
    def predict(X):
        # ensembled forward pass
        H1 = np.maximum(0, np.dot(W1, X) + b1) * p # NOTE: scale the activations
        H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # NOTE: scale the activations
        out = np.dot(W3, H2) + b3
    ```

-   **æ­£åˆ™åŒ–: Inverted Dropout (å€’ç½® Dropout)** (æ›´å¸¸è§ï¼Œæµ‹è¯•æ—¶æ— éœ€ç¼©æ”¾) (More common, no scaling needed at test time):
    ```python
    # p = 0.5 # probability of keeping a unit active. higher = less dropout
    def train_step(X):
        """ X contains the data """
        # forward pass for example 3-layer neural network
        H1 = np.maximum(0, np.dot(W1, X) + b1)
        U1 = (np.random.rand(*H1.shape) < p) / p # first dropout mask. Notice /p!
        H1 *= U1 # drop!

        H2 = np.maximum(0, np.dot(W2, H1) + b2)
        U2 = (np.random.rand(*H2.shape) < p) / p # second dropout mask. Notice /p!
        H2 *= U2 # drop!

        out = np.dot(W3, H2) + b3

        # backward pass: compute gradients... (not shown)
        # perform parameter update... (not shown)

    def predict(X):
        # ensembled forward pass
        H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary
        H2 = np.maximum(0, np.dot(W2, H1) + b2) # no scaling necessary
        out = np.dot(W3, H2) + b3
    ```

### å››ã€è®²å¸ˆæå‡ºçš„æ€è€ƒé¢˜ (Questions Posed by the Instructor)
-   å¯¹äº Sigmoid æ¿€æ´»å‡½æ•°ï¼Œå½“ `x = -10` æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿå½“ `x = 0` æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿå½“ `x = 10` æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ (For the Sigmoid activation function, what happens when `x = -10`? What happens when `x = 0`? What happens when `x = 10`?)
-   å½“ç¥ç»å…ƒçš„è¾“å…¥å§‹ç»ˆä¸ºæ­£æ—¶ï¼Œå…³äºæƒé‡ `w` çš„æ¢¯åº¦æˆ‘ä»¬èƒ½è¯´ä»€ä¹ˆï¼Ÿ (When the input to a neuron is always positive, what can we say about the gradients on `w`?)
-   å½“ `x < 0` æ—¶ï¼ŒReLU çš„æ¢¯åº¦æ˜¯å¤šå°‘ï¼Ÿ (What is the gradient of ReLU when `x < 0`?)
-   å¦‚æœæˆ‘ä»¬æŠŠæ‰€æœ‰çš„ `W` éƒ½åˆå§‹åŒ–ä¸º 0ï¼Œ`b` ä¹Ÿåˆå§‹åŒ–ä¸º 0ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ (What happens if we initialize all W=0, b=0?)
-   Xavier åˆå§‹åŒ–æ³•ä¸­ï¼Œå¦‚æœæ‰€æœ‰æ¿€æ´»å‡½æ•°éƒ½è¶‹äºé›¶ï¼Œæ¢¯åº¦ä¼šæ˜¯æ€æ ·çš„ï¼Ÿ (For Xavier Initialization, if all activations tend to zero, what do the gradients dL/dW look like?)
-   Xavier åˆå§‹åŒ–æ³•ä¸­ï¼Œå¦‚æœæ‰€æœ‰æ¿€æ´»å‡½æ•°éƒ½é¥±å’Œäº†ï¼Œæ¢¯åº¦ä¼šæ˜¯æ€æ ·çš„ï¼Ÿ (For Xavier Initialization, if all activations saturate, what do the gradients dL/dW look like?)

---
