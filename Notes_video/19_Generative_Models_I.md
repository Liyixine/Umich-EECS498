### [ğŸ“š] è§†é¢‘å­¦ä¹ è„šæ‰‹æ¶: Generative Models Part 1 - Lecture 19

### ä¸€ã€æ ¸å¿ƒå†…å®¹å¤§çº² (Core Content Outline)
-   **è®²åº§ä»‹ç»ä¸å›é¡¾ (Lecture Introduction & Recap)**
    -   æ¬¢è¿æ¥åˆ°ç¬¬ 19 è®²ï¼šç”Ÿæˆæ¨¡å‹ï¼Œç¬¬ä¸€éƒ¨åˆ† (Welcome to Lecture 19: Generative Models, Part 1) [0:00]
    -   ä¸Šæ¬¡å›é¡¾ï¼šè§†é¢‘æ¨¡å‹ (Last Time: Videos) [0:11]
        -   è®¨è®ºäº†å¤šç§è§†é¢‘å¤„ç†æ¨¡å‹ï¼Œå¦‚å•å¸§ CNN (Single-frame CNN)ã€åæœŸèåˆ (Late Fusion)ã€æ—©æœŸèåˆ (Early Fusion)ã€3D CNN / C3D [0:37]
        -   ä»¥åŠåŒæµç½‘ç»œ (Two-stream Networks)ã€CNN + RNNã€å·ç§¯ RNN (Convolutional RNN)ã€æ—¶ç©ºè‡ªæ³¨æ„åŠ› (Spatio-temporal Self-attention) å’Œ SlowFast ç½‘ç»œ (SlowFast Networks) [0:40]
-   **ç”Ÿæˆæ¨¡å‹ç®€ä»‹ (Introduction to Generative Models)** [0:58]
    -   æœ¬è®²å°†é‡ç‚¹æ¢è®¨ç”Ÿæˆæ¨¡å‹ (Generative Models) [1:06]
    -   é¢„å‘Šï¼šæœ¬è®²å†…å®¹å°†æ¶‰åŠæ›´å¤šæ•°å­¦å’Œæ›´å°‘å›¾ç¤º [1:15]
    -   æœ¬è®²å°†æ¶µç›–å˜åˆ†è‡ªç¼–ç å™¨ (Variational Autoencoders) å’Œè‡ªå›å½’æ¨¡å‹ (Autoregressive Models)ï¼Œä¸‹ä¸€è®²å°†è®¨è®ºç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (Generative Adversarial Networks) [1:45]
-   **ç›‘ç£å­¦ä¹  vs. æ— ç›‘ç£å­¦ä¹  (Supervised vs. Unsupervised Learning)** [2:06]
    -   **ç›‘ç£å­¦ä¹  (Supervised Learning)**
        -   æ•°æ®å½¢å¼: $(x, y)$ï¼Œå…¶ä¸­ $x$ ä¸ºæ•°æ®ï¼Œ $y$ ä¸ºæ ‡ç­¾ (Data: x, y; x is data, y is label) [2:18]
        -   ç›®æ ‡: å­¦ä¹ ä¸€ä¸ªå°† $x$ æ˜ å°„åˆ° $y$ çš„å‡½æ•° (Goal: Learn a function to map x -> y) [2:40]
        -   ä¾‹å­: å›¾åƒåˆ†ç±» (Image Classification)ã€å›å½’ (Regression)ã€ç›®æ ‡æ£€æµ‹ (Object Detection)ã€è¯­ä¹‰åˆ†å‰² (Semantic Segmentation)ã€å›¾åƒæ ‡æ³¨ (Image Captioning) ç­‰ [3:30]
        -   æ ¸å¿ƒç‰¹ç‚¹: éœ€è¦äººå·¥æ ‡æ³¨æ•°æ® (Requires human annotation) [2:54]
    -   **æ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)**
        -   æ•°æ®å½¢å¼: ä»…æœ‰æ•°æ® $x$ï¼Œæ²¡æœ‰æ ‡ç­¾ (Data: x, Just data, no labels!) [4:30]
        -   ç›®æ ‡: å­¦ä¹ æ•°æ®ä¸­æ½œåœ¨çš„éšè—ç»“æ„ (Goal: Learn some underlying hidden structure of the data) [5:00]
        -   ä¾‹å­: èšç±» (Clustering) (å¦‚ K-Means)ã€é™ç»´ (Dimensionality Reduction) (å¦‚ä¸»æˆåˆ†åˆ†æ PCA)ã€ç‰¹å¾å­¦ä¹  (Feature Learning) (å¦‚è‡ªç¼–ç å™¨ Autoencoders)ã€å¯†åº¦ä¼°è®¡ (Density Estimation) [6:00]
        -   æ ¸å¿ƒç‰¹ç‚¹: ä¸éœ€è¦äººå·¥æ ‡æ³¨ (Doesn't require human annotation)ï¼Œè¢«è§†ä¸ºæœºå™¨å­¦ä¹ çš„â€œåœ£æ¯â€ (Holy Grail) [5:17]
-   **åˆ¤åˆ«æ¨¡å‹ vs. ç”Ÿæˆæ¨¡å‹ (Discriminative vs. Generative Models)** [9:09]
    -   **åˆ¤åˆ«æ¨¡å‹ (Discriminative Model)**
        -   å­¦ä¹ æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ $p(y|x)$ (Learn a probability distribution $p(y|x)$) [9:57]
        -   ç‰¹ç‚¹: å¯¹äºæ¯ä¸ªè¾“å…¥ï¼Œå¯èƒ½çš„æ ‡ç­¾ä¹‹é—´ç«äº‰æ¦‚ç‡è´¨é‡ (Possible labels for each input "compete" for probability mass) [14:45]
        -   å±€é™æ€§: æ— æ³•å¤„ç†ä¸åˆç†çš„è¾“å…¥ï¼Œå› ä¸ºå®ƒå¿…é¡»ä¸ºæ‰€æœ‰å›¾åƒç»™å‡ºæ ‡ç­¾åˆ†å¸ƒ (No way for the model to handle unreasonable inputs; it must give label distributions for all images) [14:57]
        -   è¿™å¯èƒ½æ˜¯å¯¹æŠ—æ€§æ”»å‡» (Adversarial Attacks) å¯èƒ½çš„åŸå› ä¹‹ä¸€ [15:51]
    -   **ç”Ÿæˆæ¨¡å‹ (Generative Model)**
        -   å­¦ä¹ æ•°æ® $x$ çš„æ¦‚ç‡åˆ†å¸ƒ $p(x)$ (Learn a probability distribution $p(x)$) [10:26]
        -   ç‰¹ç‚¹: æ‰€æœ‰å¯èƒ½çš„å›¾åƒä¹‹é—´ç«äº‰æ¦‚ç‡è´¨é‡ (All possible images compete with each other for probability mass) [16:40]
        -   ä¼˜åŠ¿: å¯ä»¥â€œæ‹’ç»â€ä¸åˆç†çš„è¾“å…¥ï¼Œé€šè¿‡ä¸ºå…¶åˆ†é…éå¸¸å°çš„å€¼ (Can "reject" unreasonable inputs by assigning them small values) [20:00]
        -   éœ€è¦å¯¹å›¾åƒæœ‰æ·±åˆ»çš„ç†è§£ (Requires deep image understanding) [17:09]
    -   **æ¡ä»¶ç”Ÿæˆæ¨¡å‹ (Conditional Generative Model)**
        -   å­¦ä¹ ç»™å®šæ ‡ç­¾ $y$ ä¸‹æ•°æ® $x$ çš„æ¦‚ç‡åˆ†å¸ƒ $p(x|y)$ (Learn $p(x|y)$) [10:32]
        -   ç‰¹ç‚¹: æ¯ä¸ªå¯èƒ½çš„æ ‡ç­¾éƒ½ä¼šå¼•èµ·æ‰€æœ‰å›¾åƒä¹‹é—´çš„ç«äº‰ (Each possible label induces a competition among all images) [20:41]
        -   å¯ä»¥åˆ†é…æ ‡ç­¾ï¼ŒåŒæ—¶æ‹’ç»å¼‚å¸¸å€¼ (Assign labels, while rejecting outliers!) [21:45]
        -   å¯ä»¥ç”Ÿæˆä»¥è¾“å…¥æ ‡ç­¾ä¸ºæ¡ä»¶çš„æ–°æ•°æ® (Generate new data conditioned on input labels) [25:46]
    -   **æ¦‚ç‡å›é¡¾ (Probability Recap)**
        -   å¯†åº¦å‡½æ•° $p(x)$ (Density Function $p(x)$): ä¸ºæ¯ä¸ªå¯èƒ½çš„ $x$ åˆ†é…ä¸€ä¸ªæ­£æ•°ï¼Œæ•°å€¼è¶Šé«˜è¡¨ç¤º $x$ è¶Šå¯èƒ½ (assigns a positive number to each possible $x$; higher numbers mean $x$ is more likely) [11:16]
        -   å¯†åº¦å‡½æ•°æ˜¯å½’ä¸€åŒ–çš„: $\int_{X} p(x) dx = 1$ (Density functions are normalized) [12:00]
        -   ä¸åŒçš„ $x$ å€¼ç«äº‰å¯†åº¦ (Different values of x compete for density) [12:44]
    -   **è´å¶æ–¯å®šç† (Bayes' Rule)**
        -   $P(x|y) = \frac{P(y|x)}{P(y)} P(x)$ [22:33]
        -   è¿™è¡¨æ˜å¯ä»¥ä»åˆ¤åˆ«æ¨¡å‹ (Discriminative Model, $P(y|x)$)ã€æ ‡ç­¾çš„å…ˆéªŒåˆ†å¸ƒ (Prior over labels, $P(y)$) å’Œæ— æ¡ä»¶ç”Ÿæˆæ¨¡å‹ (Unconditional Generative Model, $P(x)$) æ„å»ºæ¡ä»¶ç”Ÿæˆæ¨¡å‹ (Conditional Generative Model, $P(x|y)$) [22:50]
-   **ç”Ÿæˆæ¨¡å‹åˆ†ç±» (Taxonomy of Generative Models)** [26:19]
    -   **æ˜¾å¼å¯†åº¦æ¨¡å‹ (Explicit density)**: æ¨¡å‹å¯ä»¥è®¡ç®— $p(x)$ (Model can compute $p(x)$) [26:43]
        -   **å¯å¤„ç†å¯†åº¦ (Tractable density)**: å¯ä»¥ç›´æ¥è®¡ç®— $p(x)$ (Can compute $p(x)$) [27:30]
            -   è‡ªå›å½’æ¨¡å‹ (Autoregressive models) [28:45, 30:05]
            -   NADE (Neural Autoregressive Distribution Estimator) / MADE (Masked Autoencoder for Density Estimation)
            -   NICE (Non-linear Independent Components Estimation) / RealNVP (Real NVP)
            -   Glow (Generative Flow with Invertible 1x1 Convolutions)
            -   FFjord (Free-form Jacobian of Flows with Ordinary Differential Equations)
        -   **è¿‘ä¼¼å¯†åº¦ (Approximate density)**: å¯ä»¥è®¡ç®— $p(x)$ çš„è¿‘ä¼¼å€¼ (Can compute approximation to $p(x)$) [27:43]
            -   å˜åˆ†æ–¹æ³• (Variational): å˜åˆ†è‡ªç¼–ç å™¨ (Variational Autoencoder) [28:03]
            -   é©¬å°”å¯å¤«é“¾ (Markov Chain): GSN (Generative Stochastic Networks), Boltzmann Machine
    -   **éšå¼å¯†åº¦æ¨¡å‹ (Implicit density)**: æ¨¡å‹ä¸æ˜¾å¼è®¡ç®— $p(x)$ï¼Œä½†å¯ä»¥ä»ä¸­é‡‡æ · (Model does not explicitly compute $p(x)$, but can sample from $p(x)$) [27:14]
        -   é©¬å°”å¯å¤«é“¾ (Markov Chain): GSN
        -   ç›´æ¥ (Direct): ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (Generative Adversarial Networks (GANs)) [28:11]
-   **æ˜¾å¼å¯†åº¦ä¼°è®¡ï¼šè‡ªå›å½’æ¨¡å‹ (Explicit Density: Autoregressive Models)** [30:12]
    -   ç›®æ ‡ (Goal): å†™å‡º $p(x) = f(x, W)$ çš„æ˜¾å¼å‡½æ•° [30:12]
    -   å‡è®¾ $x$ åŒ…å«å¤šä¸ªå­éƒ¨åˆ† (Assume x consists of multiple subparts): $x = (x_1, x_2, x_3, ..., x_T)$ [32:37]
    -   ä½¿ç”¨é“¾å¼æ³•åˆ™åˆ†è§£æ¦‚ç‡ (Break down probability using the chain rule): $p(x) = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)... = \prod_{t=1}^{T} p(x_t | x_1, ..., x_{t-1})$ [32:58]
        -   è¿™è¡¨ç¤ºä¸‹ä¸€ä¸ªå­éƒ¨åˆ†ç»™å®šæ‰€æœ‰å‰ä¸€ä¸ªå­éƒ¨åˆ†çš„æ¦‚ç‡ (Probability of the next subpart given all the previous subparts) [33:40]
        -   è¿™ç§æ¨¡å¼ä¸å¾ªç¯ç¥ç»ç½‘ç»œ (RNN) ä¸­çš„è¯­è¨€å»ºæ¨¡ (Language modeling) ç±»ä¼¼ [34:07]
    -   **PixelRNN** [35:30]
        -   ä»å›¾åƒå·¦ä¸Šè§’å¼€å§‹ï¼Œä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªå›¾åƒåƒç´  (Generate image pixels one at a time, starting at the upper left corner) [35:42]
        -   ä¸ºæ¯ä¸ªåƒç´ è®¡ç®—ä¸€ä¸ªéšè—çŠ¶æ€ (Compute a hidden state for each pixel)
        -   éšè—çŠ¶æ€å–å†³äºå·¦ä¾§å’Œä¸Šæ–¹åƒç´ çš„éšè—çŠ¶æ€å’Œ RGB å€¼ (depends on hidden states and RGB values from the left and from above (LSTM recurrence)) [35:53]
            -   $h_{x,y} = f(h_{x-1,y}, h_{x,y-1}, W)$ [35:53]
        -   åœ¨æ¯ä¸ªåƒç´ å¤„ï¼Œé¢„æµ‹çº¢è‰²ï¼Œç„¶åè“è‰²ï¼Œç„¶åç»¿è‰² (At each pixel, predict red, then blue, then green): å¯¹ [0, 1, ..., 255] è¿›è¡Œ Softmax [36:07]
        -   æ¯ä¸ªåƒç´ éšå¼ä¾èµ–äºå…¶ä¸Šæ–¹å’Œå·¦ä¾§çš„æ‰€æœ‰åƒç´  (Each pixel depends implicitly on all pixels above and to the left) [37:32]
        -   **é—®é¢˜ (Problem)**: åœ¨è®­ç»ƒå’Œæµ‹è¯•æœŸé—´éƒ½éå¸¸æ…¢ (Very slow during both training and testing) [38:20]
            -   N x N å›¾åƒéœ€è¦ 2N-1 ä¸ªé¡ºåºæ­¥éª¤ (N x N image requires 2N-1 sequential steps) [38:43]
    -   **PixelCNN** [38:58]
        -   ä»ç„¶ä»è§’è½å¼€å§‹ç”Ÿæˆå›¾åƒåƒç´  (Still generate image pixels starting from corner) [39:07]
        -   å¯¹å…ˆå‰åƒç´ çš„ä¾èµ–ç°åœ¨ä½¿ç”¨ä¸Šä¸‹æ–‡åŒºåŸŸä¸Šçš„ CNN è¿›è¡Œå»ºæ¨¡ (Dependency on previous pixels now modeled using a CNN over context region) [39:19]
        -   è®­ç»ƒ: æœ€å¤§åŒ–è®­ç»ƒå›¾åƒçš„ä¼¼ç„¶ (Training: maximize likelihood of training images) [39:41]
        -   è®­ç»ƒé€Ÿåº¦æ¯” PixelRNN å¿« (Training is faster than PixelRNN)
            -   å¯ä»¥å¹¶è¡ŒåŒ–å·ç§¯ï¼Œå› ä¸ºè®­ç»ƒå›¾åƒä¸­çš„ä¸Šä¸‹æ–‡åŒºåŸŸå€¼æ˜¯å·²çŸ¥çš„ (can parallelize convolutions since context region values known from training images) [39:50]
        -   ç”Ÿæˆä»ç„¶éœ€è¦æŒ‰é¡ºåºè¿›è¡Œ (Generation must still proceed sequentially) => ä»ç„¶å¾ˆæ…¢ (still slow) [39:56]
    -   **PixelRNN: ç”Ÿæˆæ ·æœ¬ (Generated Samples)** [40:00]
        -   32x32 CIFAR-10 å’Œ 32x32 ImageNet çš„ç”Ÿæˆå›¾åƒ (examples shown) [40:00]
        -   å›¾ç‰‡çœ‹èµ·æ¥æ¨¡ç³Šï¼ŒåŒ…å«ä¸€äº›é«˜å±‚ç»“æ„ï¼Œä½†ç»†èŠ‚ä»æœ‰â€œåƒåœ¾â€ (Images look blurry, contain some high-level structures, but details are "garbage") [40:47]
    -   **è‡ªå›å½’æ¨¡å‹ï¼šPixelRNN å’Œ PixelCNN çš„ä¼˜ç¼ºç‚¹ (Autoregressive Models: PixelRNN and PixelCNN Pros & Cons)** [40:57]
        -   **ä¼˜ç‚¹ (Pros)**:
            -   å¯ä»¥æ˜¾å¼è®¡ç®— $p(x)$ çš„ä¼¼ç„¶ (Can explicitly compute likelihood $p(x)$) [41:06]
            -   è®­ç»ƒæ•°æ®çš„æ˜¾å¼ä¼¼ç„¶æä¾›äº†è‰¯å¥½çš„è¯„ä¼°æŒ‡æ ‡ (Explicit likelihood of training data gives good evaluation metric) [41:14]
            -   æ ·æœ¬è´¨é‡è‰¯å¥½ (Good samples) [41:50]
        -   **ç¼ºç‚¹ (Con)**:
            -   é¡ºåºç”Ÿæˆ (Sequential generation) => é€Ÿåº¦æ…¢ (slow) [43:40]
        -   **æ”¹è¿› PixelCNN æ€§èƒ½ (Improving PixelCNN performance)**:
            -   é—¨æ§å·ç§¯å±‚ (Gated convolutional layers)
            -   çŸ­è¿æ¥ (Short-cut connections)
            -   ç¦»æ•£é€»è¾‘æŸå¤± (Discretized logistic loss)
            -   å¤šå°ºåº¦ (Multi-scale)
            -   è®­ç»ƒæŠ€å·§ (Training tricks)
            -   ç­‰ç­‰ (Etc...)
        -   **ç›¸å…³è®ºæ–‡ (See)**:
            -   Van den Oord et al., "Pixel Recurrent Neural Networks", ICML 2016 [40:57]
            -   Salimans et al. 2017 (PixelCNN++) [40:57]

-   **å˜åˆ†è‡ªç¼–ç å™¨ (Variational Autoencoders - VAE)** [44:23]
    -   PixelRNN / PixelCNN æ˜¾å¼åœ°ä½¿ç”¨ç¥ç»ç½‘ç»œå‚æ•°åŒ–å¯†åº¦å‡½æ•°ï¼Œå› æ­¤å¯ä»¥è®­ç»ƒä»¥æœ€å¤§åŒ–è®­ç»ƒæ•°æ®çš„ä¼¼ç„¶ (PixelRNN / PixelCNN explicitly parameterizes density function with a neural network, so we can train to maximize likelihood of training data) [44:40]
    -   å˜åˆ†è‡ªç¼–ç å™¨ (VAE) å®šä¹‰äº†ä¸€ä¸ª**éš¾å¤„ç†çš„å¯†åº¦**ï¼Œæˆ‘ä»¬æ— æ³•æ˜¾å¼è®¡ç®—æˆ–ä¼˜åŒ–å®ƒ (Variational Autoencoders (VAE) define an **intractable density** that we cannot explicitly compute or optimize) [45:10]
    -   ä½†æˆ‘ä»¬å¯ä»¥ç›´æ¥ä¼˜åŒ–å¯†åº¦çš„**ä¸‹ç•Œ** (But we will be able to directly optimize a **lower bound** on the density) [45:24]

-   **å¸¸è§„ï¼ˆéå˜åˆ†ï¼‰è‡ªç¼–ç å™¨ (Regular, non-variational Autoencoders)** [46:20]
    -   ä¸€ç§æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºä»åŸå§‹æ•°æ® $x$ ä¸­å­¦ä¹ ç‰¹å¾å‘é‡ $z$ï¼Œæ— éœ€ä»»ä½•æ ‡ç­¾ (Unsupervised method for learning feature vectors from raw data x, without any labels) [46:21]
    -   ç‰¹å¾ (Features) åº”è¯¥æå–æœ‰ç”¨çš„ä¿¡æ¯ (should extract useful information) (ä¾‹å¦‚å¯¹è±¡èº«ä»½ã€å±æ€§ã€åœºæ™¯ç±»å‹ç­‰)ï¼Œä»¥ä¾¿ç”¨äºä¸‹æ¸¸ä»»åŠ¡ (that we can use for downstream tasks) [46:57]
    -   **ç¼–ç å™¨ (Encoder)**: ä»è¾“å…¥æ•°æ® $x$ åˆ°ç‰¹å¾ $z$ çš„æ˜ å°„ (map $x$ to $z$) [47:00]
        -   æœ€åˆæ˜¯çº¿æ€§ + éçº¿æ€§ï¼ˆsigmoidï¼‰(Originally: Linear + nonlinearity (sigmoid)) [47:58]
        -   åæ¥æ˜¯æ·±åº¦å…¨è¿æ¥ (Later: Deep, fully-connected) [48:07]
        -   å†åæ¥æ˜¯ ReLU CNN (Later: ReLU CNN) [48:16]
    -   **é—®é¢˜ (Problem)**: æˆ‘ä»¬å¦‚ä½•ä»åŸå§‹æ•°æ®ä¸­å­¦ä¹ è¿™ç§ç‰¹å¾è½¬æ¢ï¼Ÿæˆ‘ä»¬æ— æ³•è§‚å¯Ÿåˆ°ç‰¹å¾ï¼ (How can we learn this feature transform from raw data? But we can't observe features!) [47:33]
    -   **æ€è·¯ (Idea)**: ä½¿ç”¨ç‰¹å¾é€šè¿‡è§£ç å™¨ (decoder) é‡æ„è¾“å…¥æ•°æ® (Use the features to reconstruct the input data with a decoder) [48:26]
        -   â€œè‡ªç¼–ç  (Autoencoding)â€ = è‡ªæˆ‘ç¼–ç  (encoding itself) [48:32]
        -   **è§£ç å™¨ (Decoder)**: ä»ç‰¹å¾ $z$ åˆ°é‡æ„è¾“å…¥æ•°æ® $\hat{x}$ çš„æ˜ å°„ (map $z$ to $\hat{x}$) [48:45]
            -   æ¶æ„ç±»ä¼¼ç¼–ç å™¨ï¼Œä½†é€šå¸¸æ˜¯â€œç¿»è½¬â€çš„ç‰ˆæœ¬ï¼ˆä¾‹å¦‚ï¼ŒCNN ä½¿ç”¨ä¸Šé‡‡æ ·æˆ–è½¬ç½®å·ç§¯å±‚ (upconv layers)ï¼‰[48:58]
    -   **æŸå¤±å‡½æ•° (Loss Function)**: è¾“å…¥æ•°æ®å’Œé‡æ„æ•°æ®ä¹‹é—´çš„ L2 è·ç¦» (L2 distance between input and reconstructed data) [49:09]
        -   ä¸ä½¿ç”¨ä»»ä½•æ ‡ç­¾ï¼ä»…åŸå§‹æ•°æ®ï¼ (Does not use any labels! Just raw data!) [49:10]
        -   $\text{Loss Function} = ||\hat{x} - x||^2_2$ [49:09]
    -   **è®­ç»ƒå (After training)**: æ‰”æ‰è§£ç å™¨ (throw away decoder) å¹¶ä½¿ç”¨ç¼–ç å™¨è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡ (use encoder for a downstream task) [51:21]
        -   ç¼–ç å™¨å¯ä»¥ç”¨äºåˆå§‹åŒ–ç›‘ç£æ¨¡å‹ (Encoder can be used to initialize a supervised model) [51:31]
        -   åœ¨å°‘é‡æ•°æ®ä¸Šè®­ç»ƒæœ€ç»ˆä»»åŠ¡ (Train for final task (sometimes with small data)) [51:41]
    -   **å±€é™æ€§ (Limitations)**:
        -   è‡ªç¼–ç å™¨å­¦ä¹ æ•°æ®çš„**æ½œåœ¨ç‰¹å¾** (Autoencoders learn **latent features** for data without any labels!) [53:27]
        -   å¯ä»¥ç”¨ç‰¹å¾åˆå§‹åŒ–ç›‘ç£æ¨¡å‹ (Can use features to initialize a supervised model) [53:28]
        -   **éæ¦‚ç‡æ€§ (Not probabilistic)**: æ— æ³•ä»å­¦ä¹ åˆ°çš„æ¨¡å‹ä¸­é‡‡æ ·æ–°æ•°æ® (No way to sample new data from learned model) [53:38]

-   **å˜åˆ†è‡ªç¼–ç å™¨ (Variational Autoencoders - VAE)** [54:52]
    -   è‡ªç¼–ç å™¨çš„æ¦‚ç‡åŒ–ç‰ˆæœ¬ (Probabilistic spin on autoencoders) [54:53]
    -   1. ä»åŸå§‹æ•°æ®ä¸­å­¦ä¹ æ½œåœ¨ç‰¹å¾ $z$ (Learn latent features z from raw data) [55:08]
    -   2. ä»æ¨¡å‹ä¸­é‡‡æ ·ä»¥ç”Ÿæˆæ–°æ•°æ® (Sample from the model to generate new data) [55:16]
    -   **ç›´è§‰ (Intuition)**: $x$ æ˜¯ä¸€å¼ å›¾åƒï¼Œ$z$ æ˜¯ç”¨äºç”Ÿæˆ $x$ çš„æ½œåœ¨å› å­ (x is an image, z is latent factors used to generate x): å±æ€§ã€æ–¹å‘ç­‰ (attributes, orientation, etc.) [55:38]
    -   å‡è®¾è®­ç»ƒæ•°æ® $\{x^{(i)}\}_{i=1}^N$ æ˜¯ç”±æœªè§‚å¯Ÿåˆ°çš„ï¼ˆæ½œåœ¨ï¼‰è¡¨ç¤º $z$ ç”Ÿæˆçš„ (Assume training data is generated from unobserved (latent) representation z) [55:24]
    -   å‡è®¾ä¸€ä¸ªç®€å•çš„å…ˆéªŒ $p(z)$ï¼Œä¾‹å¦‚é«˜æ–¯åˆ†å¸ƒ (Assume simple prior p(z), e.g. Gaussian) [56:39]
    -   ç”¨ç¥ç»ç½‘ç»œè¡¨ç¤ºæ¡ä»¶æ¦‚ç‡ $p(x|z)$ (Represent $p(x|z)$ with a neural network) [56:58]
        -   ç±»ä¼¼äºè‡ªç¼–ç å™¨ä¸­çš„è§£ç å™¨ (Similar to decoder from autoencoder) [57:00]
        -   è§£ç å™¨ (Decoder) å¿…é¡»æ˜¯**æ¦‚ç‡æ€§**çš„ (must be probabilistic) [57:21]
        -   è§£ç å™¨è¾“å…¥ $z$ï¼Œè¾“å‡ºé«˜æ–¯åˆ†å¸ƒçš„å‡å€¼ $\mu_{x|z}$ å’Œï¼ˆå¯¹è§’çº¿ï¼‰åæ–¹å·® $\Sigma_{x|z}$ (Decoder inputs z, outputs mean $\mu_{x|z}$ and (diagonal) covariance $\Sigma_{x|z}$) [57:39]
        -   ä»é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ · $x$ï¼Œå…¶å‡å€¼å’Œåæ–¹å·®ç”±è§£ç å™¨ç½‘ç»œè¾“å‡º (Sample $x$ from Gaussian with mean $\mu_{x|z}$ and (diagonal) covariance $\Sigma_{x|z}$) [57:46]
    -   **å¦‚ä½•è®­ç»ƒè¿™ä¸ªæ¨¡å‹ï¼Ÿ(How to train this model?)** [1:00:04]
        -   åŸºæœ¬æ€æƒ³: æœ€å¤§åŒ–æ•°æ®çš„ä¼¼ç„¶ (Basic idea: maximize likelihood of data) [1:00:11]
        -   æˆ‘ä»¬æ²¡æœ‰è§‚å¯Ÿåˆ° $z$ï¼Œæ‰€ä»¥éœ€è¦è¿›è¡Œè¾¹ç¼˜åŒ– (We don't observe $z$, so need to marginalize):
            $$p_\theta(x) = \int p_\theta(x,z)dz = \int p_\theta(x|z)p_\theta(z)dz$$
            
            -   $p_\theta(x|z)$ ç”±è§£ç å™¨ç½‘ç»œè®¡ç®— (computed by decoder network) [1:01:29]
            -   $p_\theta(z)$ æ˜¯æˆ‘ä»¬å‡è®¾çš„å…ˆéªŒé«˜æ–¯åˆ†å¸ƒ (assumed Gaussian prior) [1:01:33]
        -   **é—®é¢˜ (Problem)**: è¿™ä¸ªç§¯åˆ†æ— æ³•è®¡ç®— (Impossible to integrate over all z!) [1:01:37]
    -   **å¦ä¸€ä¸ªæ€è·¯: å°è¯•è´å¶æ–¯æ³•åˆ™ (Another idea: Try Bayes' Rule)** [1:01:50]
        -   $\log p_\theta(x) = \log \frac{p_\theta(x|z)p_\theta(z)}{p_\theta(z|x)}$ [1:02:15]
        -   ä¹˜ä»¥å¹¶é™¤ä»¥ $q_\phi(z|x)$ (Multiply top and bottom by $q_\phi(z|x)$) (è¿™æ˜¯ä¸€ä¸ªæ–°çš„ç½‘ç»œï¼Œç¼–ç å™¨ç½‘ç»œï¼Œå…¶å‚æ•°ä¸º $\phi$) [1:02:21]
        -   ç¼–ç å™¨ç½‘ç»œ $q_\phi(z|x)$ è¾“å…¥æ•°æ® $x$ï¼Œç»™å‡ºæ½œåœ¨ç¼–ç  $z$ çš„åˆ†å¸ƒ (Encoder network inputs data x, gives distribution over latent codes z) [1:04:26]
            -   $q_\phi(z|x) = N(\mu_{z|x}, \Sigma_{z|x})$ [1:04:31]
            -   ç¼–ç å™¨è¾“å…¥ $x$ï¼Œè¾“å‡ºé«˜æ–¯åˆ†å¸ƒçš„å‡å€¼ $\mu_{z|x}$ å’Œï¼ˆå¯¹è§’çº¿ï¼‰åæ–¹å·® $\Sigma_{z|x}$ (Encoder inputs x, outputs mean $\mu_{z|x}$ and (diagonal) covariance $\Sigma_{z|x}$) [1:04:31]
        -   å¦‚æœæˆ‘ä»¬å¯ä»¥ç¡®ä¿ $q_\phi(z|x) \approx p_\theta(z|x)$ (If we can ensure that $q_\phi(z|x) \approx p_\theta(z|x)$)ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è¿‘ä¼¼ (then we can approximate):
            $$p_\theta(x) \approx \frac{p_\theta(x|z)p_\theta(z)}{q_\phi(z|x)}$$

        -   æˆ‘ä»¬å¯ä»¥ç”¨æœŸæœ›å€¼æ¥é‡å†™è¿™ä¸ªå¯¹æ•°ä¼¼ç„¶é¡¹ (We can rewrite this log-likelihood term as an expectation) [1:06:01]
            -   $\log p_\theta(x) = E_{z \sim q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) || p(z)) + D_{KL}(q_\phi(z|x) || p_\theta(z|x))$ [1:07:15]
            -   $D_{KL}(P || Q)$ æ˜¯ KL æ•£åº¦ (KL divergence) [1:07:30]
        -   æˆ‘ä»¬çŸ¥é“ $D_{KL}(Q || P) \ge 0$ (We know $D_{KL}(Q || P) \ge 0$) [1:08:21]
        -   å› æ­¤ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ª**æ•°æ®ä¼¼ç„¶çš„ä¸‹ç•Œ** (Therefore, we get a lower bound on the data likelihood):
            $$\log p_\theta(x) \ge E_{z \sim q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) || p(z))$$ [1:08:44]
            -   è¿™ä¸ªä¸‹ç•Œè¢«ç§°ä¸º**å˜åˆ†ä¸‹ç•Œ** (variational lower bound) [1:09:27]
            -   ç¬¬ä¸€é¡¹æ˜¯**æ•°æ®é‡æ„**é¡¹ (Data reconstruction) [1:09:02]
            -   ç¬¬äºŒé¡¹æ˜¯**KL æ•£åº¦** (KL divergence) [1:09:02]
    -   **è”åˆè®­ç»ƒç¼–ç å™¨ q å’Œè§£ç å™¨ p ä»¥æœ€å¤§åŒ–å˜åˆ†ä¸‹ç•Œ (Jointly train encoder q and decoder p to maximize the variational lower bound)** [1:09:12]
        -   åœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£ä¸­ï¼Œæˆ‘ä»¬å¯¹ $z$ ä» $q_\phi(z|x)$ ä¸­è¿›è¡Œé‡‡æ ·ï¼Œå¹¶ä½¿ç”¨è¿™äº›é‡‡æ ·å€¼è®¡ç®—æŸå¤±ï¼Œç„¶åé€šè¿‡åå‘ä¼ æ’­æ›´æ–° $\theta$ å’Œ $\phi$ã€‚
        -   è¿™ä½¿ç¼–ç å™¨èƒ½å¤Ÿå­¦ä¹ å¦‚ä½•å°†æ•°æ®æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ï¼ŒåŒæ—¶è§£ç å™¨å­¦ä¹ å¦‚ä½•ä»æ½œåœ¨ç©ºé—´ç”Ÿæˆæ•°æ®ã€‚
        -   **å…³é”® (Key)**: è¿™ç§æ–¹æ³•ä½¿å¾—åœ¨éš¾å¤„ç†çš„æ¦‚ç‡æ¨¡å‹ä¸Šè¿›è¡Œä¼˜åŒ–æˆä¸ºå¯èƒ½ï¼ŒåŒæ—¶èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„æ ·æœ¬ã€‚
        -   **ä¼˜ç¼ºç‚¹ (Pros & Cons)**:
            -   VAE çš„æ ·æœ¬é€šå¸¸æ¯” GAN æ¨¡ç³Š (VAEs samples are usually blurrier than GANs)
            -   ç„¶è€Œï¼Œå®ƒä»¬æä¾›äº†ä¼¼ç„¶ä¼°è®¡ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„è¯„ä¼°æŒ‡æ ‡ (But they provide likelihood estimation, which is an important evaluation metric)

æœ¬æ¬¡è§†é¢‘é€šè¿‡æ•°å­¦å…¬å¼å’Œæ¦‚å¿µæ¨å¯¼å±•ç¤ºäº†ç®—æ³•çš„æ ¸å¿ƒé€»è¾‘ï¼Œä½†æœªåŒ…å«å…·ä½“çš„ Python ä»£ç ç¤ºä¾‹ã€‚

### äºŒã€å…³é”®æœ¯è¯­å®šä¹‰ (Key Term Definitions)
-   **ç”Ÿæˆæ¨¡å‹ (Generative Models)**: ä¸€ç±»æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨å­¦ä¹ è®­ç»ƒæ•°æ®çš„åº•å±‚åˆ†å¸ƒï¼Œä»è€Œèƒ½å¤Ÿç”Ÿæˆä¸è®­ç»ƒæ•°æ®ç›¸ä¼¼çš„æ–°æ•°æ®æ ·æœ¬ã€‚
-   **ç›‘ç£å­¦ä¹  (Supervised Learning)**: ä¸€ç§æœºå™¨å­¦ä¹ èŒƒå¼ï¼Œæ¨¡å‹é€šè¿‡å¸¦æœ‰è¾“å…¥-è¾“å‡ºå¯¹ï¼ˆæ•°æ®å’Œå¯¹åº”æ ‡ç­¾ï¼‰çš„æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥å­¦ä¹ ä»è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„å…³ç³»ã€‚
-   **æ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)**: ä¸€ç§æœºå™¨å­¦ä¹ èŒƒå¼ï¼Œæ¨¡å‹åœ¨æ²¡æœ‰æ˜ç¡®æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è¯†åˆ«æ•°æ®ä¸­çš„æ¨¡å¼å’Œç»“æ„è¿›è¡Œå­¦ä¹ ã€‚
-   **å›¾åƒåˆ†ç±» (Image Classification)**: ç›‘ç£å­¦ä¹ çš„ä¸€ä¸ªåº”ç”¨ï¼Œç›®æ ‡æ˜¯å°†å›¾åƒå½’ç±»åˆ°é¢„å®šä¹‰çš„ç±»åˆ«ä¸­ã€‚
-   **å›å½’ (Regression)**: ç›‘ç£å­¦ä¹ çš„ä¸€ä¸ªåº”ç”¨ï¼Œç›®æ ‡æ˜¯é¢„æµ‹ä¸€ä¸ªè¿ç»­çš„è¾“å‡ºå€¼ã€‚
-   **ç›®æ ‡æ£€æµ‹ (Object Detection)**: ç›‘ç£å­¦ä¹ çš„ä¸€ä¸ªåº”ç”¨ï¼Œç›®æ ‡æ˜¯åœ¨å›¾åƒä¸­è¯†åˆ«å¹¶å®šä½ç‰©ä½“ã€‚
-   **è¯­ä¹‰åˆ†å‰² (Semantic Segmentation)**: ç›‘ç£å­¦ä¹ çš„ä¸€ä¸ªåº”ç”¨ï¼Œç›®æ ‡æ˜¯å¯¹å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ è¿›è¡Œåˆ†ç±»ã€‚
-   **å›¾åƒæ ‡æ³¨ (Image Captioning)**: ç›‘ç£å­¦ä¹ çš„ä¸€ä¸ªåº”ç”¨ï¼Œç›®æ ‡æ˜¯ä¸ºå›¾åƒç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ã€‚
-   **èšç±» (Clustering)**: æ— ç›‘ç£å­¦ä¹ çš„ä¸€ä¸ªåº”ç”¨ï¼Œç›®æ ‡æ˜¯å°†æ•°æ®ç‚¹åˆ†ç»„ï¼Œä½¿å¾—åŒç»„å†…çš„æ•°æ®ç‚¹ç›¸ä¼¼åº¦é«˜ï¼Œä¸åŒç»„é—´ç›¸ä¼¼åº¦ä½ã€‚
-   **é™ç»´ (Dimensionality Reduction)**: æ— ç›‘ç£å­¦ä¹ çš„ä¸€ä¸ªåº”ç”¨ï¼Œç›®æ ‡æ˜¯å°†é«˜ç»´æ•°æ®æ˜ å°„åˆ°ä½ç»´ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å…¶å¤§éƒ¨åˆ†é‡è¦ä¿¡æ¯ã€‚
-   **ç‰¹å¾å­¦ä¹  (Feature Learning)**: æ— ç›‘ç£å­¦ä¹ çš„ä¸€ä¸ªåº”ç”¨ï¼Œç›®æ ‡æ˜¯è‡ªåŠ¨å‘ç°æ•°æ®ä¸­çš„æœ‰æ•ˆè¡¨ç¤ºæˆ–ç‰¹å¾ã€‚
-   **å¯†åº¦ä¼°è®¡ (Density Estimation)**: æ— ç›‘ç£å­¦ä¹ çš„ä¸€ä¸ªåº”ç”¨ï¼Œç›®æ ‡æ˜¯ä¼°è®¡æ•°æ®ç‚¹çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚
-   **åˆ¤åˆ«æ¨¡å‹ (Discriminative Model)**: æœºå™¨å­¦ä¹ æ¨¡å‹çš„ä¸€ç§ï¼Œç›´æ¥å­¦ä¹ æ¡ä»¶æ¦‚ç‡ $P(y|x)$ï¼Œç”¨äºåŒºåˆ†ä¸åŒç±»åˆ«ï¼Œè€Œä¸æ˜¾å¼å»ºæ¨¡æ•°æ®çš„ç”Ÿæˆè¿‡ç¨‹ã€‚
-   **æ¡ä»¶ç”Ÿæˆæ¨¡å‹ (Conditional Generative Model)**: ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå­¦ä¹ ç»™å®šç‰¹å®šæ¡ä»¶ï¼ˆå¦‚æ ‡ç­¾ï¼‰ä¸‹æ•°æ®çš„æ¦‚ç‡åˆ†å¸ƒ $P(x|y)$ã€‚
-   **å¯†åº¦å‡½æ•° (Density Function)**: æ¦‚ç‡è®ºä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒæè¿°äº†éšæœºå˜é‡åœ¨ç»™å®šç‚¹é™„è¿‘çš„ç›¸å¯¹å¯èƒ½æ€§ã€‚å¯¹äºè¿ç»­å˜é‡ï¼Œå®ƒè¡¨ç¤ºæ¦‚ç‡å¯†åº¦ï¼Œå…¶ç§¯åˆ†åœ¨æ•´ä¸ªç©ºé—´ä¸Šç­‰äº1ã€‚
-   **è´å¶æ–¯å®šç† (Bayes' Rule)**: ä¸€æ¡åœ¨æ¦‚ç‡è®ºä¸­ç”¨äºè®¡ç®—æ¡ä»¶æ¦‚ç‡çš„å®šç†ï¼Œå…¶å…¬å¼ä¸º $P(A|B) = \frac{P(B|A)P(A)}{P(B)}$ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­å¸¸ç”¨äºè¿æ¥ä¸åŒç±»å‹çš„æ¨¡å‹ã€‚
-   **æ˜¾å¼å¯†åº¦ (Explicit Density)**: æŒ‡é‚£äº›èƒ½å¤Ÿæ˜ç¡®è®¡ç®—å‡ºå…¶æ¦‚ç‡å¯†åº¦å‡½æ•° $p(x)$ çš„ç”Ÿæˆæ¨¡å‹ã€‚
-   **éšå¼å¯†åº¦ (Implicit Density)**: æŒ‡é‚£äº›ä¸èƒ½ç›´æ¥è®¡ç®—å…¶æ¦‚ç‡å¯†åº¦å‡½æ•° $p(x)$ï¼Œä½†å¯ä»¥ä»ä¸­è¿›è¡Œé‡‡æ ·çš„ç”Ÿæˆæ¨¡å‹ã€‚
-   **å¯å¤„ç†å¯†åº¦ (Tractable Density)**: æ˜¾å¼å¯†åº¦æ¨¡å‹çš„ä¸€ä¸ªå­ç±»åˆ«ï¼Œå…¶æ¦‚ç‡å¯†åº¦å‡½æ•° $p(x)$ å¯ä»¥é«˜æ•ˆç²¾ç¡®åœ°è®¡ç®—ã€‚
-   **è¿‘ä¼¼å¯†åº¦ (Approximate Density)**: æ˜¾å¼å¯†åº¦æ¨¡å‹çš„ä¸€ä¸ªå­ç±»åˆ«ï¼Œå…¶æ¦‚ç‡å¯†åº¦å‡½æ•° $p(x)$ åªèƒ½é€šè¿‡è¿‘ä¼¼æ–¹æ³•æ¥è®¡ç®—ã€‚
-   **å˜åˆ†æ–¹æ³• (Variational Methods)**: ç”¨äºè¿‘ä¼¼éš¾ä»¥å¤„ç†çš„æ¦‚ç‡åˆ†å¸ƒæˆ–æ¨æ–­çš„æ–¹æ³•ï¼Œé€šå¸¸é€šè¿‡ä¼˜åŒ–ä¸€ä¸ªä¸‹ç•Œæ¥å®ç°ã€‚
-   **å˜åˆ†è‡ªç¼–ç å™¨ (Variational Autoencoder - VAE)**: ä¸€ç§åŸºäºå˜åˆ†æ–¹æ³•çš„ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒå­¦ä¹ æ•°æ®çš„æ½œåœ¨è¡¨ç¤ºå¹¶èƒ½ä»ä¸­ç”Ÿæˆæ–°çš„æ•°æ®ã€‚
-   **é©¬å°”å¯å¤«é“¾ (Markov Chain)**: ä¸€ç§éšæœºè¿‡ç¨‹ï¼Œå…¶ä¸­æ¯ä¸ªçŠ¶æ€çš„æ¦‚ç‡åªå–å†³äºå‰ä¸€ä¸ªçŠ¶æ€ã€‚åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ç”¨äºå»ºæ¨¡åºåˆ—æ•°æ®æˆ–é‡‡æ ·ã€‚
-   **Boltzmann Machine (ç»å°”å…¹æ›¼æœº)**: ä¸€ç§åŸºäºé©¬å°”å¯å¤«é“¾çš„èƒ½é‡æ¨¡å‹ï¼Œç”¨äºå­¦ä¹ å¤æ‚æ•°æ®çš„åˆ†å¸ƒã€‚
-   **ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (Generative Adversarial Networks - GANs)**: ä¸€ç§éšå¼å¯†åº¦æ¨¡å‹ï¼Œç”±ä¸€ä¸ªç”Ÿæˆå™¨å’Œä¸€ä¸ªåˆ¤åˆ«å™¨ç»„æˆï¼Œé€šè¿‡å¯¹æŠ—è®­ç»ƒå­¦ä¹ ç”Ÿæˆé€¼çœŸçš„æ•°æ®ã€‚
-   **è‡ªå›å½’æ¨¡å‹ (Autoregressive Models)**: ä¸€ç§å¯å¤„ç†å¯†åº¦çš„ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒé€šè¿‡å°†æ•°æ®çš„è”åˆæ¦‚ç‡åˆ†è§£ä¸ºä¸€ç³»åˆ—æ¡ä»¶æ¦‚ç‡çš„ä¹˜ç§¯æ¥å»ºæ¨¡æ•°æ®ï¼Œå…¶ä¸­æ¯ä¸ªæ•°æ®ç‚¹éƒ½ä»¥å‰é¢çš„æ•°æ®ç‚¹ä¸ºæ¡ä»¶ã€‚
-   **NADE (Neural Autoregressive Distribution Estimator)**: ä¸€ç§ä½¿ç”¨ç¥ç»ç½‘ç»œå®ç°è‡ªå›å½’æ¨¡å‹çš„æ˜¾å¼å¯†åº¦ä¼°è®¡å™¨ã€‚
-   **MADE (Masked Autoencoder for Density Estimation)**: å¦ä¸€ç§ä½¿ç”¨æ©ç è‡ªç¼–ç å™¨å®ç°è‡ªå›å½’æ¨¡å‹çš„æ˜¾å¼å¯†åº¦ä¼°è®¡å™¨ã€‚
-   **NICE (Non-linear Independent Components Estimation)**: ä¸€ç§ä½¿ç”¨å¯é€†å˜æ¢çš„æ˜¾å¼å¯†åº¦æ¨¡å‹ï¼Œå¯ä»¥ç²¾ç¡®è®¡ç®—å¯†åº¦å’Œé‡‡æ ·ã€‚
-   **RealNVP (Real NVP)**: NICE çš„æ”¹è¿›ç‰ˆæœ¬ï¼ŒåŒæ ·ä½¿ç”¨å¯é€†å˜æ¢æ¥å®ç°ç²¾ç¡®çš„å¯†åº¦ä¼°è®¡å’Œé‡‡æ ·ã€‚
-   **Glow (Generative Flow with Invertible 1x1 Convolutions)**: åŸºäºæµæ¨¡å‹ (Flow-based model) çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥å®ç°å¯é€†çš„ç”Ÿæˆå’Œå¯†åº¦ä¼°è®¡ã€‚
-   **FFjord (Free-form Jacobian of Flows with Ordinary Differential Equations)**: ä¸€ç§åŸºäºODEçš„æµæ¨¡å‹ï¼Œèƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„æ¦‚ç‡åˆ†å¸ƒã€‚
-   **æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (Maximum Likelihood Estimation)**: ä¸€ç§ç”¨äºä¼°è®¡æ¨¡å‹å‚æ•°çš„æ–¹æ³•ï¼Œé€šè¿‡æ‰¾åˆ°ä½¿è§‚æµ‹æ•°æ®çš„æ¦‚ç‡ï¼ˆæˆ–ä¼¼ç„¶ï¼‰æœ€å¤§åŒ–çš„å‚æ•°å€¼ã€‚
-   **Log Trick (å¯¹æ•°æŠ€å·§)**: åœ¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä¸­ï¼Œä¸ºäº†å°†ä¹˜ç§¯è½¬åŒ–ä¸ºå’Œï¼Œä»è€Œç®€åŒ–è®¡ç®—å’Œé¿å…æ•°å€¼ä¸‹æº¢ï¼Œå¸¸å¯¹ä¼¼ç„¶å‡½æ•°å–å¯¹æ•°ã€‚
-   **é“¾å¼æ³•åˆ™ (Chain Rule)**: æ¦‚ç‡è®ºä¸­ç”¨äºåˆ†è§£è”åˆæ¦‚ç‡åˆ†å¸ƒçš„è§„åˆ™ï¼Œå³å°†ä¸€ä¸ªè”åˆåˆ†å¸ƒè¡¨ç¤ºä¸ºä¸€ç³»åˆ—æ¡ä»¶åˆ†å¸ƒçš„ä¹˜ç§¯ã€‚
-   **å¾ªç¯ç¥ç»ç½‘ç»œ (Recurrent Neural Network - RNN)**: ä¸€ç§é€‚ç”¨äºåºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œå…¶ç‰¹ç‚¹æ˜¯ä¿¡æ¯å¯ä»¥åœ¨ç½‘ç»œä¸­å¾ªç¯ï¼Œä½¿å¾—å½“å‰è¾“å‡ºä¾èµ–äºè¿‡å»çš„è¾“å…¥ã€‚
-   **PixelRNN**: ä¸€ç§è‡ªå›å½’çš„æ˜¾å¼å¯†åº¦æ¨¡å‹ï¼Œé€šè¿‡é¡ºåºé¢„æµ‹å›¾åƒåƒç´ ï¼ˆåŒ…æ‹¬ RGB é€šé“ï¼‰ï¼Œä»¥å…¶å·¦ä¾§å’Œä¸Šæ–¹åƒç´ ä¸ºæ¡ä»¶æ¥ç”Ÿæˆå›¾åƒã€‚
-   **PixelCNN**: PixelRNN çš„å˜ä½“ï¼Œåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¥å»ºæ¨¡åƒç´ é—´çš„ä¾èµ–å…³ç³»ï¼Œå°¤å…¶æ˜¯åœ¨ä¸Šä¸‹æ–‡åŒºåŸŸï¼Œä»¥å®ç°æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ã€‚
-   **è‡ªç¼–ç å™¨ (Autoencoder)**: ä¸€ç§æ— ç›‘ç£ç¥ç»ç½‘ç»œï¼Œæ—¨åœ¨å­¦ä¹ æ•°æ®çš„æœ‰æ•ˆç¼–ç ï¼ˆæ½œåœ¨ç‰¹å¾ï¼‰ï¼Œé€šè¿‡å°è¯•å°†è¾“å…¥é‡æ„ä¸ºè¾“å‡ºã€‚
-   **ç¼–ç å™¨ (Encoder)**: è‡ªç¼–ç å™¨çš„ä¸€éƒ¨åˆ†ï¼Œè´Ÿè´£å°†è¾“å…¥æ•°æ®æ˜ å°„åˆ°æ½œåœ¨ç‰¹å¾ç©ºé—´ã€‚
-   **è§£ç å™¨ (Decoder)**: è‡ªç¼–ç å™¨çš„ä¸€éƒ¨åˆ†ï¼Œè´Ÿè´£å°†æ½œåœ¨ç‰¹å¾æ˜ å°„å›åŸå§‹æ•°æ®ç©ºé—´ï¼Œå°è¯•é‡æ„è¾“å…¥ã€‚
-   **å˜åˆ†ä¸‹ç•Œ (Variational Lower Bound - ELBO)**: å˜åˆ†è‡ªç¼–ç å™¨ä¸­ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°ï¼Œå®ƒæ˜¯æ•°æ®ä¼¼ç„¶çš„ä¸€ä¸ªä¸‹ç•Œï¼Œé€šè¿‡æœ€å¤§åŒ–æ­¤ä¸‹ç•Œæ¥é—´æ¥æœ€å¤§åŒ–æ•°æ®çš„ä¼¼ç„¶ã€‚
-   **KL æ•£åº¦ (KL Divergence)**: è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´å·®å¼‚çš„éè´Ÿåº¦é‡ã€‚åœ¨ VAE ä¸­ï¼Œå®ƒé€šå¸¸ç”¨äºè¡¡é‡è¿‘ä¼¼åéªŒåˆ†å¸ƒä¸å…ˆéªŒåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚
-   **å˜åˆ†æ¨æ–­ (Variational Inference)**: ä¸€ç§ç”¨äºè¿‘ä¼¼å¤æ‚æ¦‚ç‡åˆ†å¸ƒæˆ–éš¾ä»¥å¤„ç†çš„æ¨æ–­é—®é¢˜çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ä¸€ä¸ªæ›´ç®€å•çš„å˜åˆ†åˆ†å¸ƒæ¥è¿‘ä¼¼ç›®æ ‡åˆ†å¸ƒï¼Œå¹¶ä¼˜åŒ–å…¶å‚æ•°ã€‚

### ä¸‰ã€æ ¸å¿ƒç®—æ³•ä¸ä»£ç ç‰‡æ®µ (Core Algorithms & Code Snippets)

-   **æ˜¾å¼å¯†åº¦ä¼°è®¡ (Explicit Density Estimation)**
    -   **ç›®æ ‡ (Goal)**: å†™å‡º $p(x) = f(x, W)$ çš„æ˜¾å¼å‡½æ•°ã€‚
    -   **è®­ç»ƒæ–¹æ³• (Training Method)**: ç»™å®šæ•°æ®é›† $x^{(1)}, x^{(2)}, ..., x^{(N)}$ï¼Œé€šè¿‡è§£å†³ä»¥ä¸‹ä¼˜åŒ–é—®é¢˜æ¥è®­ç»ƒæ¨¡å‹ï¼š
        $$W^* = \arg \max_W \prod_i p(x^{(i)})$$
        -   è¿™æ—¨åœ¨æœ€å¤§åŒ–è®­ç»ƒæ•°æ®çš„æ¦‚ç‡ï¼ˆå³æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ŒMaximum Likelihood Estimationï¼‰ã€‚
        -   ä¸ºäº†ä¾¿äºä¼˜åŒ–ï¼Œé€šå¸¸ä½¿ç”¨å¯¹æ•°æŠ€å·§ (Log trick) å°†ä¹˜ç§¯è½¬æ¢ä¸ºå’Œï¼š
        $$W^* = \arg \max_W \sum_i \log p(x^{(i)})$$
        -   å°† $p(x^{(i)})$ æ›¿æ¢ä¸ºæˆ‘ä»¬çš„å‚æ•°åŒ–å‡½æ•° $f(x^{(i)}, W)$ï¼š
        $$W^* = \arg \max_W \sum_i \log f(x^{(i)}, W)$$
        -   è¿™ä¸ªè¡¨è¾¾å¼å°†æˆä¸ºæˆ‘ä»¬çš„æŸå¤±å‡½æ•° (loss function)ï¼Œå¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™ (gradient descent) è¿›è¡Œè®­ç»ƒã€‚

-   **è‡ªå›å½’æ¨¡å‹ (Autoregressive Models)**
    -   **æ ¸å¿ƒæ€æƒ³**: å‡è®¾æ¯ä¸ªæ•°æ®ç‚¹ $x$ ç”±å¤šä¸ªå­éƒ¨åˆ†ç»„æˆï¼š$x = (x_1, x_2, x_3, ..., x_T)$ã€‚
    -   **æ¦‚ç‡åˆ†è§£ (Probability Breakdown)**: åˆ©ç”¨é“¾å¼æ³•åˆ™ (Chain rule) å°†è”åˆæ¦‚ç‡åˆ†è§£ä¸ºæ¡ä»¶æ¦‚ç‡çš„ä¹˜ç§¯ï¼š
        $$p(x) = p(x_1, x_2, x_3, ..., x_T) = p(x_1)p(x_2 | x_1)p(x_3 | x_1, x_2)...$$
        $$= \prod_{t=1}^{T} p(x_t | x_1, ..., x_{t-1})$$
        -   è¿™è¡¨ç¤ºä¸‹ä¸€ä¸ªå­éƒ¨åˆ†ç»™å®šæ‰€æœ‰å‰ä¸€ä¸ªå­éƒ¨åˆ†çš„æ¦‚ç‡ã€‚
    -   **PixelRNN åƒç´ ç”Ÿæˆè¿‡ç¨‹**:
        -   ä»å·¦ä¸Šè§’å¼€å§‹ï¼Œä¾æ¬¡ç”Ÿæˆæ¯ä¸ªåƒç´ çš„ RGB å€¼ã€‚
        -   å½“å‰åƒç´ çš„éšè—çŠ¶æ€ $h_{x,y}$ ä¾èµ–äºå·¦ä¾§å’Œä¸Šæ–¹åƒç´ çš„éšè—çŠ¶æ€ $h_{x-1,y}$ å’Œ $h_{x,y-1}$ï¼Œä»¥åŠå®ƒä»¬å¯¹åº”çš„ RGB å€¼ã€‚
        -   $h_{x,y} = f(h_{x-1,y}, h_{x,y-1}, W)$
        -   æ¯ä¸ªåƒç´ çš„ RGB å€¼é€šè¿‡ Softmax è¾“å‡ºå…¶åœ¨ èŒƒå›´å†…çš„ç¦»æ•£æ¦‚ç‡åˆ†å¸ƒã€‚
        -   è¿™ç§ä¾èµ–å…³ç³»é€šè¿‡ RNN çš„åºåˆ—ç‰¹æ€§è‡ªç„¶åœ°å®ç°äº†å¯¹æ‰€æœ‰å…ˆå‰åƒç´ çš„éšå¼æ¡ä»¶ä¾èµ–ã€‚
    -   **PixelCNN åƒç´ ç”Ÿæˆè¿‡ç¨‹**:
        -   ä¸ PixelRNN ç±»ä¼¼ï¼Œä½†ä½¿ç”¨æ©ç å·ç§¯ (masked convolutions) æ¥å¼ºåˆ¶ä¾èµ–å…³ç³»ï¼Œåªè€ƒè™‘å·¦ä¾§å’Œä¸Šæ–¹çš„åƒç´ ä¿¡æ¯ã€‚
        -   è¿™å…è®¸åœ¨è®­ç»ƒæ—¶å¹¶è¡Œè®¡ç®—æŸäº›å·ç§¯æ“ä½œï¼Œä»è€ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œä½†ç”Ÿæˆè¿‡ç¨‹ä»ç„¶æ˜¯é¡ºåºçš„ã€‚

-   **å˜åˆ†è‡ªç¼–ç å™¨ (Variational Autoencoders - VAE)**
    -   **ç›®æ ‡**: å­¦ä¹ æ•°æ®çš„æ½œåœ¨ç‰¹å¾ $z$ å¹¶èƒ½å¤Ÿä»æ¨¡å‹ä¸­é‡‡æ ·ä»¥ç”Ÿæˆæ–°æ•°æ®ã€‚
    -   **æ¨¡å‹ç»“æ„**:
        -   **è§£ç å™¨ç½‘ç»œ (Decoder Network)**ï¼šè¾“å…¥æ½œåœ¨ä»£ç  $z$ï¼Œè¾“å‡ºæ•°æ® $x$ çš„æ¦‚ç‡åˆ†å¸ƒã€‚
            -   $p_\theta(x|z) = N(\mu_{x|z}, \Sigma_{x|z})$
            -   è§£ç å™¨ç½‘ç»œè¾“å‡ºé«˜æ–¯åˆ†å¸ƒçš„å‡å€¼ $\mu_{x|z}$ å’Œï¼ˆå¯¹è§’çº¿ï¼‰åæ–¹å·® $\Sigma_{x|z}$ã€‚
            -   ç„¶åä»è¿™ä¸ªé«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ · $x$ã€‚
        -   **ç¼–ç å™¨ç½‘ç»œ (Encoder Network)**ï¼ˆåˆç§°**å˜åˆ†æ¨æ–­ç½‘ç»œ**ï¼‰ï¼šè¾“å…¥æ•°æ® $x$ï¼Œè¾“å‡ºæ½œåœ¨ä»£ç  $z$ çš„åˆ†å¸ƒã€‚
            -   $q_\phi(z|x) = N(\mu_{z|x}, \Sigma_{z|x})$
            -   ç¼–ç å™¨ç½‘ç»œè¾“å‡ºé«˜æ–¯åˆ†å¸ƒçš„å‡å€¼ $\mu_{z|x}$ å’Œï¼ˆå¯¹è§’çº¿ï¼‰åæ–¹å·® $\Sigma_{z|x}$ã€‚
            -   ç„¶åä»è¿™ä¸ªé«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ · $z$ã€‚
    -   **è®­ç»ƒç›®æ ‡**: æœ€å¤§åŒ–æ•°æ®çš„ä¼¼ç„¶ $p_\theta(x)$ã€‚ç”±äºç›´æ¥è®¡ç®—æ­¤ç§¯åˆ†éš¾å¤„ç†ï¼Œæˆ‘ä»¬è½¬è€Œæœ€å¤§åŒ–å…¶**å˜åˆ†ä¸‹ç•Œ (Variational Lower Bound - ELBO)**ã€‚
        -   **æ•°æ®ä¼¼ç„¶çš„åˆ†è§£**:
            $$ \log p_\theta(x) = E_{z \sim q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) || p_\theta(z)) + D_{KL}(q_\phi(z|x) || p_\theta(z|x)) $$
            -   $E_{z \sim q_\phi(z|x)}[\log p_\theta(x|z)]$ï¼šæ•°æ®é‡æ„é¡¹ (Data reconstruction term)ï¼Œè¡¡é‡è§£ç å™¨ä»æ½œåœ¨ä»£ç é‡æ„è¾“å…¥æ•°æ®çš„èƒ½åŠ›ã€‚
            -   $D_{KL}(q_\phi(z|x) || p_\theta(z))$ï¼šKL æ•£åº¦é¡¹ï¼Œè¡¡é‡ç¼–ç å™¨è¾“å‡ºçš„è¿‘ä¼¼åéªŒåˆ†å¸ƒä¸æ½œåœ¨å˜é‡å…ˆéªŒåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚
            -   $D_{KL}(q_\phi(z|x) || p_\theta(z|x))$ï¼šKL æ•£åº¦é¡¹ï¼Œè¡¡é‡ç¼–ç å™¨è¾“å‡ºçš„è¿‘ä¼¼åéªŒåˆ†å¸ƒä¸çœŸå®åéªŒåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚æ­¤é¡¹é€šå¸¸æ— æ³•ç›´æ¥è®¡ç®—ï¼Œä½†å› å…¶éè´Ÿæ€§ï¼Œå¯ä»¥è¢«çœç•¥ä»¥è·å¾—ä¸‹ç•Œã€‚
        -   **å˜åˆ†ä¸‹ç•Œ (ELBO)**:
            $$ \log p_\theta(x) \ge E_{z \sim q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) || p(z)) $$
            -   æˆ‘ä»¬é€šè¿‡è”åˆè®­ç»ƒç¼–ç å™¨å’Œè§£ç å™¨æ¥æœ€å¤§åŒ–è¿™ä¸ªä¸‹ç•Œã€‚

æœ¬æ¬¡è§†é¢‘æœªåŒ…å«å…·ä½“çš„ Python ä»£ç ç¤ºä¾‹ã€‚

### å››ã€è®²å¸ˆæå‡ºçš„æ€è€ƒé¢˜ (Questions Posed by the Instructor)
-   â€œå…³äºç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ çš„åŒºåˆ«ï¼Œå¤§å®¶æ¸…æ¥šäº†å—ï¼Ÿæœ‰ä»€ä¹ˆé—®é¢˜å—ï¼Ÿâ€ (Was this clear to everyone? Was any kind of questions on this supervised versus unsupervised uh the distinction?) [8:56]
-   â€œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ç›‘ç£å­¦ä¹ æ¥è®­ç»ƒæ¡ä»¶ç”Ÿæˆæ¨¡å‹å—ï¼Ÿâ€ (Do we need to use supervised learning to learn the conditional generative model?) [11:39]
-   â€œæˆ‘ä»¬å¦‚ä½•åˆ¤æ–­ä¸€ä¸ªç”Ÿæˆæ¨¡å‹çš„å¥½åï¼Ÿâ€ (How can we tell how good is a generative model?) [19:16]
-   â€œè¿™çœ‹èµ·æ¥åƒæ˜¯ä»€ä¹ˆä¸œè¥¿ï¼Ÿâ€ (Can anyone guess what this reminds you of?) [34:01] (æŒ‡é“¾å¼æ³•åˆ™åˆ†è§£æ¦‚ç‡çš„æ¨¡å¼)
-   â€œå¯¹äºè¿™ç§ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªåƒç´ çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¿…é¡»ç”Ÿæˆç¬¬ä¸€ä¸ªåƒç´ å—ï¼Ÿâ€ (For these kinds of models where we're generating one pixel at a time, do we have to generate the first pixel?) [42:17]
-   â€œè¿™äº›æ¨¡å‹èƒ½æ¨å¹¿åˆ°ä¸åŒçš„å›¾åƒåˆ†è¾¨ç‡å—ï¼Ÿâ€ (Can these models generalize to different image resolutions?) [43:51]
-   â€œå¦‚æœæœ‰ä¸€ä¸ªç¼–ç å™¨ä½†æœ‰ä¸åŒçš„è§£ç å™¨ï¼Œé‚£ä¼šæ€ä¹ˆæ ·ï¼Ÿâ€ (If you had one encoder but different decoders, what would that be?) [52:43]
-   â€œå¦‚æœæˆ‘ä»¬æœ‰1ä¸ªç¼–ç å™¨ä½†æœ‰ä¸åŒçš„è§£ç å™¨ï¼Œé‚£ä¼šæ€ä¹ˆæ ·ï¼Ÿâ€ (What if you have one encoder but multiple decoders?) [52:43]

---