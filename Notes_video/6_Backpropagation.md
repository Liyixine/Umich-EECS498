### [ğŸ“š] è§†é¢‘å­¦ä¹ è„šæ‰‹æ¶: CS231n Lecture 6: ç¥ç»ç½‘ç»œä¸åå‘ä¼ æ’­ (Neural Networks & Backpropagation)

### ä¸€ã€æ ¸å¿ƒå†…å®¹å¤§çº² (Core Content Outline)
-   **ä¸ŠèŠ‚è¯¾å›é¡¾: ç¥ç»ç½‘ç»œ (Last time: Neural Networks)**
    -   ä»çº¿æ€§åˆ†ç±»å™¨åˆ°å…¨è¿æ¥ç½‘ç»œ
    -   æ ¸å¿ƒæ¦‚å¿µ:
        -   ç©ºé—´æ‰­æ›² (Space Warping)
        -   é€šç”¨è¿‘ä¼¼å®šç† (Universal Approximation)
        -   éå‡¸æ€§ (Nonconvex)
-   **æ ¸å¿ƒé—®é¢˜: å¦‚ä½•è®¡ç®—æ¢¯åº¦ (Problem: How to compute gradients?)**
    -   ç›®æ ‡: è®¡ç®—æ€»æŸå¤± (Total loss) L ç›¸å¯¹äºæƒé‡ W1 å’Œ W2 çš„åå¯¼æ•°ã€‚
    -   æ€»æŸå¤± = æ•°æ®æŸå¤± (data loss) + æ­£åˆ™åŒ– (regularization)
-   **ï¼ˆç³Ÿç³•çš„ï¼‰æƒ³æ³•: åœ¨çº¸ä¸Šæ‰‹åŠ¨æ¨å¯¼æ¢¯åº¦ (Bad Idea: Derive âˆ‡L on paper)**
    -   å±•å¼€ SVM æŸå¤±å‡½æ•°çš„å®Œæ•´è¡¨è¾¾å¼ã€‚
    -   è¿™ç§æ–¹æ³•å­˜åœ¨çš„é—®é¢˜:
        -   **é—®é¢˜1**: éå¸¸ç¹çï¼Œéœ€è¦å¤§é‡çŸ©é˜µå¾®ç§¯åˆ†å’Œçº¸å¼ ã€‚
        -   **é—®é¢˜2**: éæ¨¡å—åŒ–ã€‚å¦‚æœæƒ³æ›´æ¢æŸå¤±å‡½æ•°ï¼ˆå¦‚ç”¨ Softmax æ›¿æ¢ SVMï¼‰ï¼Œéœ€è¦ä»å¤´é‡æ–°æ¨å¯¼ã€‚
        -   **é—®é¢˜3**: å¯¹äºéå¸¸å¤æ‚çš„æ¨¡å‹ï¼Œæ­¤æ–¹æ³•ä¸å¯è¡Œã€‚
-   **æ›´å¥½çš„æƒ³æ³•: è®¡ç®—å›¾ (Better Idea: Computational Graphs)**
    -   å°†ä»»æ„å¤æ‚å‡½æ•°è¡¨ç¤ºä¸ºä¸€ä¸ªæœ‰å‘å›¾ã€‚
    -   å›¾ä¸­çš„èŠ‚ç‚¹ä»£è¡¨åŸºæœ¬è¿ç®—æˆ–å˜é‡ã€‚
    -   ç¤ºä¾‹:
        -   ç®€å•çš„çº¿æ€§åˆ†ç±»å™¨ (`f = Wx`) ä¸ Hinge Lossã€‚
        -   å¤æ‚çš„æ·±åº¦ç½‘ç»œï¼Œå¦‚ AlexNetã€‚
        -   æå…¶å¤æ‚çš„æ¨¡å‹ï¼Œå¦‚ç¥ç»å›¾çµæœº (Neural Turing Machine)ã€‚
-   **åå‘ä¼ æ’­ (Backpropagation): åœ¨è®¡ç®—å›¾ä¸Šè®¡ç®—æ¢¯åº¦**
    -   **ç®€å•æ ‡é‡ç¤ºä¾‹: `f(x, y, z) = (x + y)z`**
        -   **å‰å‘ä¼ æ’­ (Forward pass)**: ä»è¾“å…¥åˆ°è¾“å‡ºï¼Œè®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºå€¼ã€‚
        -   **åå‘ä¼ æ’­ (Backward pass)**: ä»æœ€ç»ˆè¾“å‡ºå¼€å§‹ï¼Œåå‘è®¡ç®—æ¢¯åº¦ã€‚
            -   åˆ©ç”¨é“¾å¼æ³•åˆ™ (Chain Rule)ã€‚
            -   å¼•å…¥æ¦‚å¿µ: ä¸Šæ¸¸æ¢¯åº¦ (Upstream Gradient), å±€éƒ¨æ¢¯åº¦ (Local Gradient), ä¸‹æ¸¸æ¢¯åº¦ (Downstream Gradient)ã€‚
            -   **ä¸‹æ¸¸æ¢¯åº¦ = å±€éƒ¨æ¢¯åº¦ Ã— ä¸Šæ¸¸æ¢¯åº¦**
    -   **é€šç”¨åŒ–: æ¨¡å—åŒ–è§†è§’**
        -   å°†æ¯ä¸ªè¿ç®—èŠ‚ç‚¹è§†ä¸ºä¸€ä¸ªç‹¬ç«‹çš„æ¨¡å— (gate)ã€‚
        -   æ¯ä¸ªèŠ‚ç‚¹æ¥æ”¶ä¸Šæ¸¸æ¢¯åº¦ï¼Œå¹¶æ ¹æ®å…¶å±€éƒ¨æ¢¯åº¦è®¡ç®—ä¸‹æ¸¸æ¢¯åº¦ã€‚
-   **æ¢¯åº¦æµä¸­çš„æ¨¡å¼ (Patterns in Gradient Flow)**
    -   **åŠ æ³•é—¨ (add gate)**: æ‰®æ¼”â€œæ¢¯åº¦åˆ†é…å™¨â€ (gradient distributor) çš„è§’è‰²ï¼Œå°†ä¸Šæ¸¸æ¢¯åº¦ç­‰å€¼åœ°åˆ†é…ç»™æ‰€æœ‰è¾“å…¥ã€‚
    -   **æ‹·è´é—¨ (copy gate)**: æ‰®æ¼”â€œæ¢¯åº¦åŠ æ³•å™¨â€ (gradient adder) çš„è§’è‰²ï¼Œå°†æ¥è‡ªä¸åŒåˆ†æ”¯çš„æ¢¯åº¦ç›¸åŠ ã€‚
    -   **ä¹˜æ³•é—¨ (mul gate)**: æ‰®æ¼”â€œäº¤æ¢ä¹˜æ³•å™¨â€ (swap multiplier) çš„è§’è‰²ï¼Œä¸€ä¸ªè¾“å…¥çš„æ¢¯åº¦æ˜¯ä¸Šæ¸¸æ¢¯åº¦ä¹˜ä»¥å¦ä¸€ä¸ªè¾“å…¥çš„å€¼ã€‚
    -   **æœ€å¤§å€¼é—¨ (max gate)**: æ‰®æ¼”â€œæ¢¯åº¦è·¯ç”±å™¨â€ (gradient router) çš„è§’è‰²ï¼Œå°†æ¢¯åº¦åªè·¯ç”±åˆ°å€¼è¾ƒå¤§çš„é‚£ä¸ªè¾“å…¥ï¼Œå¦ä¸€ä¸ªè¾“å…¥çš„æ¢¯åº¦ä¸º0ã€‚
-   **å¤„ç†å‘é‡ (Backprop with Vectors)**
    -   å›é¡¾å‘é‡å¯¼æ•°:
        -   æ ‡é‡å¯¹å‘é‡æ±‚å¯¼ -> æ¢¯åº¦ (Gradient)
        -   å‘é‡å¯¹å‘é‡æ±‚å¯¼ -> é›…å¯æ¯”çŸ©é˜µ (Jacobian Matrix)
    -   é›…å¯æ¯”çŸ©é˜µé€šå¸¸æ˜¯ç¨€ç–çš„ (sparse)ï¼Œå°¤å…¶æ˜¯å¯¹äºå…ƒç´ çº§ (elementwise) æ“ä½œï¼ˆæ­¤æ—¶ä¸ºå¯¹è§’çŸ©é˜µï¼‰ã€‚
    -   **æ ¸å¿ƒæ€æƒ³**: æ°¸è¿œä¸è¦æ˜¾å¼åœ°æ„å»ºå·¨å¤§çš„é›…å¯æ¯”çŸ©é˜µï¼Œè€Œæ˜¯åˆ©ç”¨å…¶ç»“æ„è¿›è¡Œéšå¼ä¹˜æ³• (implicit multiplication)ã€‚
    -   ç¤ºä¾‹: å¯¹ ReLU å‡½æ•°è¿›è¡Œå‘é‡åŒ–çš„åå‘ä¼ æ’­ã€‚
-   **å¤„ç†çŸ©é˜µæˆ–å¼ é‡ (Backprop with Matrices or Tensors)**
    -   æ¦‚å¿µä¸å‘é‡ç±»ä¼¼ï¼Œä½†é›…å¯æ¯”çŸ©é˜µä¼šå˜æˆæ›´é«˜é˜¶çš„å¼ é‡ã€‚
    -   **å…³é”®**: å°†é«˜é˜¶å¼ é‡è¿ç®—â€œæ‰å¹³åŒ–â€ (flatten) ä¸ºçŸ©é˜µ-å‘é‡ä¹˜æ³•æ¥ç†è§£ã€‚
    -   ç¤ºä¾‹: çŸ©é˜µä¹˜æ³• `y = xw` çš„åå‘ä¼ æ’­æ¨å¯¼ã€‚
        -   é€šè¿‡åˆ†æå•ä¸ªå…ƒç´ çš„æ¢¯åº¦æ¥æ¨å¯¼æ•´ä½“çš„æ¢¯åº¦è¡¨è¾¾å¼ã€‚
        -   æœ€ç»ˆçš„æ¢¯åº¦è®¡ç®—å¯ä»¥ç®€åŒ–ä¸ºä¸¤ä¸ªçŸ©é˜µä¹˜æ³•ã€‚
-   **å®ç°ç­–ç•¥ (Implementation Strategies)**
    -   **â€œæ‰å¹³åŒ–â€åå‘ä¼ æ’­ (Flat Backprop)**
        -   å°†å‰å‘ä¼ æ’­çš„ä»£ç â€œåå‘â€è¿‡æ¥å†™ã€‚
        -   é€‚ç”¨äºå›ºå®šç»“æ„çš„ç®€å•æ¨¡å‹ï¼Œä¾‹å¦‚ä½œä¸š2ä¸­çš„è¦æ±‚ã€‚
        -   ç¼ºç‚¹: éæ¨¡å—åŒ–ï¼Œä¿®æ”¹æ¨¡å‹ç»“æ„éœ€è¦é‡å†™å¤§é‡ä»£ç ã€‚
    -   **æ¨¡å—åŒ– API (Modular API)**
        -   å°†è®¡ç®—å›¾å’ŒèŠ‚ç‚¹éƒ½å®ç°ä¸ºå¯¹è±¡ (object)ã€‚
        -   æ¯ä¸ªèŠ‚ç‚¹å¯¹è±¡å®ç°è‡ªå·±çš„ `forward` å’Œ `backward` æ–¹æ³•ã€‚
        -   å›¾å¯¹è±¡è´Ÿè´£æŒ‰æ‹“æ‰‘é¡ºåºè°ƒç”¨èŠ‚ç‚¹çš„ `forward` å’Œ `backward` æ–¹æ³•ã€‚
        -   è¿™æ˜¯ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚ PyTorchï¼‰çš„å®ç°æ–¹å¼ã€‚
    -   **PyTorch Autograd ç¤ºä¾‹**:
        -   é€šè¿‡ç»§æ‰¿ `torch.autograd.Function` æ¥å®šä¹‰è‡ªå·±çš„è¿ç®—ã€‚
        -   å®ç°é™æ€çš„ `forward` å’Œ `backward` æ–¹æ³•ã€‚

### äºŒã€å…³é”®æœ¯è¯­å®šä¹‰ (Key Term Definitions)
-   **è®¡ç®—å›¾ (Computational Graph)**: ä¸€ç§å°†ä»»æ„æ•°å­¦è¡¨è¾¾å¼è¡¨ç¤ºä¸ºæœ‰å‘å›¾çš„æ•°æ®ç»“æ„ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨å˜é‡æˆ–æ“ä½œï¼Œè¾¹ä»£è¡¨å‡½æ•°å…³ç³»ã€‚
-   **åå‘ä¼ æ’­ (Backpropagation)**: ä¸€ç§åœ¨è®¡ç®—å›¾ä¸Šé«˜æ•ˆè®¡ç®—æ¢¯åº¦çš„ç®—æ³•ï¼Œé€šè¿‡é€’å½’åœ°åº”ç”¨é“¾å¼æ³•åˆ™ï¼Œä»æœ€ç»ˆè¾“å‡ºå¼€å§‹åå‘ä¼ æ’­æ¢¯åº¦ã€‚
-   **å‰å‘ä¼ æ’­ (Forward Pass)**: åœ¨è®¡ç®—å›¾ä¸­ï¼Œä»è¾“å…¥å¼€å§‹ï¼ŒæŒ‰æ‹“æ‰‘é¡ºåºè®¡ç®—å¹¶å­˜å‚¨æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºå€¼çš„è¿‡ç¨‹ã€‚
-   **åå‘ä¼ æ’­ (Backward Pass)**: åœ¨è®¡ç®—å›¾ä¸­ï¼Œä»æœ€ç»ˆè¾“å‡ºï¼ˆå…¶æ¢¯åº¦ä¸º1ï¼‰å¼€å§‹ï¼ŒæŒ‰æ‹“æ‰‘é¡ºåºçš„é€†åºï¼Œè®¡ç®—æ¯ä¸ªèŠ‚ç‚¹ç›¸å¯¹äºæœ€ç»ˆè¾“å‡ºçš„æ¢¯åº¦çš„è¿‡ç¨‹ã€‚
-   **ä¸Šæ¸¸æ¢¯åº¦ (Upstream Gradient)**: åœ¨åå‘ä¼ æ’­ä¸­ï¼Œä¸€ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºç›¸å¯¹äºæœ€ç»ˆæŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼ˆå³ä»å›¾çš„ä¸‹æ¸¸ä¼ æ¥çš„æ¢¯åº¦ï¼‰ã€‚
-   **å±€éƒ¨æ¢¯åº¦ (Local Gradient)**: ä¸€ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºç›¸å¯¹äºå…¶ç›´æ¥è¾“å…¥çš„åå¯¼æ•°ã€‚
-   **ä¸‹æ¸¸æ¢¯åº¦ (Downstream Gradient)**: åœ¨åå‘ä¼ æ’­ä¸­ï¼Œä¸€ä¸ªèŠ‚ç‚¹çš„è¾“å…¥ç›¸å¯¹äºæœ€ç»ˆæŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼ˆå³éœ€è¦ç»§ç»­å‘ä¸‹æ¸¸ä¼ æ’­çš„æ¢¯åº¦ï¼‰ã€‚
-   **é›…å¯æ¯”çŸ©é˜µ (Jacobian Matrix)**: ä¸€ä¸ªå‘é‡å€¼å‡½æ•°ç›¸å¯¹äºå…¶å‘é‡è¾“å…¥çš„åå¯¼æ•°çŸ©é˜µï¼Œæ¦‚æ‹¬äº†æ¯ä¸ªè¾“å…¥å…ƒç´ å¯¹æ¯ä¸ªè¾“å‡ºå…ƒç´ çš„å½±å“ã€‚

### ä¸‰ã€æ ¸å¿ƒç®—æ³•ä¸ä»£ç ç‰‡æ®µ (Core Algorithms & Code Snippets)
-   **ç®—æ³•: â€œæ‰å¹³åŒ–â€åå‘ä¼ æ’­ (Flat Backprop)**
    1.  **å‰å‘ä¼ æ’­**: ç¼–å†™ä»£ç ï¼Œè®¡ç®—ä»è¾“å…¥åˆ°æœ€ç»ˆè¾“å‡ºLçš„æ‰€æœ‰ä¸­é—´å˜é‡ã€‚
        ```python
        # Forward pass: Compute output
        s0 = w0 * x0
        s1 = w1 * x1
        s2 = s0 + s1
        s3 = s2 + w2
        L = sigmoid(s3)
        ```
    2.  **åå‘ä¼ æ’­**: ä»¥ç›¸åçš„é¡ºåºï¼Œä¸ºå‰å‘ä¼ æ’­ä¸­çš„æ¯ä¸€è¡Œä»£ç ï¼Œæ ¹æ®é“¾å¼æ³•åˆ™è®¡ç®—å¯¹åº”çš„æ¢¯åº¦ã€‚
        ```python
        # Backward pass: Compute grads
        grad_L = 1.0 # Base case
        grad_s3 = grad_L * (1 - L) * L # Backward through sigmoid
        grad_w2 = grad_s3 # Backward through add
        grad_s2 = grad_s3 # Backward through add
        grad_s0 = grad_s2 # Backward through add
        grad_s1 = grad_s2 # Backward through add
        grad_w1 = grad_s1 * x1 # Backward through multiply
        grad_x1 = grad_s1 * w1 # Backward through multiply
        grad_w0 = grad_s0 * x0 # Backward through multiply
        grad_x0 = grad_s0 * w0 # Backward through multiply
        ```
-   **ä»£ç ç¤ºä¾‹: PyTorch è‡ªå®šä¹‰ Autograd å‡½æ•° (PyTorch Autograd Functions)**
    ```python
    class Multiply(torch.autograd.Function):
        @staticmethod
        def forward(ctx, x, y):
            # Stash inputs for use in backward pass
            ctx.save_for_backward(x, y)
            z = x * y
            return z

        @staticmethod
        def backward(ctx, grad_z): # grad_z is the upstream gradient
            # Retrieve stashed inputs
            x, y = ctx.saved_tensors
            # Multiply upstream gradient by local gradients
            grad_x = y * grad_z # Local gradient for x is y
            grad_y = x * grad_z # Local gradient for y is x
            return grad_x, grad_y
    ```
-   **ä»£ç ç¤ºä¾‹: PyTorch åº•å±‚ Sigmoid å±‚å®ç° (PyTorch Sigmoid Layer)**
    ```c
    // (Simplified from the lecture's C/C++ snippet)
    // Forward pass computes the sigmoid function
    void THNN_Sigmoid_updateOutput(THNNState *state, THTensor *input, THTensor *output) {
        // ...
        // output = 1 / (1 + exp(-input))
    }

    // Backward pass computes the gradient
    void THNN_Sigmoid_updateGradInput(THNNState *state, THTensor *gradOutput, THTensor *gradInput, THTensor *output) {
        // ...
        // gradInput = gradOutput * (1 - output) * output;
    }
    ```

### å››ã€è®²å¸ˆæå‡ºçš„æ€è€ƒé¢˜ (Questions Posed by the Instructor)
-   `df/df` (få¯¹fçš„å¯¼æ•°) åº”è¯¥æ˜¯ä»€ä¹ˆï¼Ÿ (ç­”æ¡ˆ: 1)
-   åœ¨ä¸€ä¸ªçº¿æ€§æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å®é™…ä¸Šå·²ç»çœ‹åˆ°äº†è¿™äº›ï¼ˆè¾“å…¥å’Œæƒé‡ï¼‰ä»¥ä¸¤ç§æ–¹å¼è¢«ä½¿ç”¨ï¼šä¸€æ¬¡æ˜¯è®¡ç®—å¾—åˆ†ï¼Œå¦ä¸€æ¬¡æ˜¯è®¡ç®—æ­£åˆ™åŒ–é¡¹ã€‚é‚£ä¹ˆæˆ‘ä»¬åº”è¯¥å¦‚ä½•å¤„ç†è¿™ç§æƒ…å†µï¼Ÿ(æš—ç¤ºï¼šä½¿ç”¨æ‹·è´é—¨)