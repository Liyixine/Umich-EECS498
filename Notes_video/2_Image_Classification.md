### [ğŸ“š] è§†é¢‘å­¦ä¹ è„šæ‰‹æ¶: Lecture 2: Image Classification

### ä¸€ã€æ ¸å¿ƒå†…å®¹å¤§çº² (Core Content Outline)

-   **å¼•è¨€ (Introduction)**
    -   æœ¬æ¬¡è®²åº§ä¸ºç¬¬äºŒè®²ï¼Œä¸»é¢˜æ˜¯å›¾åƒåˆ†ç±» (This is Lecture 2, on Image Classification)ã€‚
    -   å›é¡¾ä¸Šæ¬¡è®²åº§å†…å®¹ï¼šè®¡ç®—æœºè§†è§‰ã€æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„å†å²æ¦‚è¿° (Recap: Historical overview of computer vision, deep learning, and machine learning)ã€‚

-   **å›¾åƒåˆ†ç±»ï¼šæ ¸å¿ƒè®¡ç®—æœºè§†è§‰ä»»åŠ¡ (Image Classification: A Core Computer Vision Task)**
    -   **å®šä¹‰ (Definition)**
        -   è¾“å…¥ (Input): å›¾åƒ (Image)ã€‚
        -   è¾“å‡º (Output): å°†å›¾åƒåˆ†é…åˆ°é¢„å®šä¹‰çš„å›ºå®šç±»åˆ«é›†åˆä¸­çš„ä¸€ä¸ª (Assign image to one of a fixed set of categories)ã€‚
        -   ç¤ºä¾‹ç±»åˆ« (Example categories): çŒ« (cat), é¸Ÿ (bird), é¹¿ (deer), ç‹— (dog), å¡è½¦ (truck)ã€‚
    -   **äººç±»æ„ŸçŸ¥ä¸æœºå™¨æ„ŸçŸ¥ (Human vs. Computer Perception)**
        -   äººç±» (Humans): å›¾åƒåˆ†ç±»æ˜¯å¾®ä¸è¶³é“çš„ä»»åŠ¡ï¼Œå‡ ä¹æ— éœ€æ€è€ƒå³å¯å®Œæˆ (Trivial task, immediately know it's a cat without thinking)ã€‚
        -   è®¡ç®—æœº (Computers): è¿œéæ˜“äº‹ (Not so easy)ã€‚
            -   è®¡ç®—æœºçœ‹åˆ°çš„æ˜¯ä»€ä¹ˆï¼Ÿå›¾åƒåªæ˜¯ä¸€ä¸ªç”±0åˆ°255ä¹‹é—´çš„æ•°å­—ç»„æˆçš„å·¨å¤§ç½‘æ ¼ (What the computer sees: A big grid of numbers between 0 and 255)ã€‚
            -   ä¾‹å¦‚ï¼š800 x 600 x 3 (3ä¸ªRGBé€šé“) (e.g., 800 x 600 x 3 (3 channels RGB))ã€‚
            -   æ²¡æœ‰æ˜æ˜¾çš„æ–¹æ³•å°†åŸå§‹åƒç´ å€¼ç½‘æ ¼è½¬æ¢ä¸ºå…·æœ‰è¯­ä¹‰æ„ä¹‰çš„ç±»åˆ«æ ‡ç­¾ (No obvious way to convert raw pixel values into semantically meaningful category labels)ã€‚

-   **å›¾åƒåˆ†ç±»çš„æŒ‘æˆ˜ (Challenges in Image Classification)**
    -   **è¯­ä¹‰é¸¿æ²Ÿ (Semantic Gap)**
        -   å›¾åƒçš„å¾®å°å˜åŒ–å¯èƒ½å¯¼è‡´åƒç´ å€¼å‘ç”Ÿå·¨å¤§å˜åŒ– (Small changes in images can drastically change pixel values)ã€‚
    -   **è§†ç‚¹å˜åŒ– (Viewpoint Variation)**
        -   ç›¸æœºç§»åŠ¨æ—¶ï¼Œæ‰€æœ‰åƒç´ éƒ½ä¼šæ”¹å˜ (All pixels change when the camera moves)ã€‚
        -   ç®—æ³•éœ€è¦å¯¹è¿™äº›å˜åŒ–å…·æœ‰é²æ£’æ€§ (Algorithms need to be robust to these changes)ã€‚
    -   **ç±»å†…å·®å¼‚ (Intraclass Variation)**
        -   åŒä¸€ç±»åˆ«å†…çš„ä¸åŒå¯¹è±¡ï¼ˆä¾‹å¦‚ï¼šä¸åŒçš„çŒ«ï¼‰çœ‹èµ·æ¥éå¸¸ä¸åŒ (Different instances of the same category look very different)ã€‚
        -   ç®—æ³•éœ€è¦å¯¹åŒä¸€ç±»åˆ«å†…å¯èƒ½å‘ç”Ÿçš„å·¨å¤§å˜åŒ–å…·æœ‰é²æ£’æ€§ (Algorithms need to be robust to massive variations within categories)ã€‚
    -   **ç»†ç²’åº¦ç±»åˆ« (Fine-Grained Categories)**
        -   è¯†åˆ«è§†è§‰ä¸Šéå¸¸ç›¸ä¼¼çš„ä¸åŒå­ç±»åˆ« (Recognizing different categories that appear very visually similar)ã€‚
        -   ä¾‹å¦‚ï¼šè¯†åˆ«ä¸åŒå“ç§çš„çŒ« (e.g., different breeds of cats like Maine Coon, Ragdoll, American Shorthair)ã€‚
    -   **èƒŒæ™¯æ‚ä¹± (Background Clutter)**
        -   å›¾åƒä¸­çš„ç‰©ä½“å¯èƒ½ä¸èƒŒæ™¯èåˆ (Objects in the image might blend into the background)ã€‚
        -   ä¾‹å¦‚ï¼šç”±äºè‡ªç„¶ä¼ªè£…æˆ–å…¶ä»–åœºæ™¯ä¸­çš„å¤æ‚æƒ…å†µ (e.g., due to natural camouflage or other crazy things in the scene)ã€‚
    -   **å…‰ç…§å˜åŒ– (Illumination Changes)**
        -   åœºæ™¯ä¸­çš„å…‰ç…§æ¡ä»¶å˜åŒ–ä¼šå¯¼è‡´åƒç´ å€¼å‘ç”Ÿå·¨å¤§å˜åŒ– (Lighting conditions change significantly)ã€‚
        -   ç®—æ³•éœ€è¦å¯¹ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„å·¨å¤§å˜åŒ–å…·æœ‰é²æ£’æ€§ (Algorithms should be robust to massive changes in different lighting conditions)ã€‚
    -   **å½¢å˜ (Deformation)**
        -   ç‰©ä½“å¯èƒ½ä»¥éå¸¸ä¸åŒçš„å§¿æ€æˆ–ä½ç½®å‡ºç° (Objects might appear in very different poses/positions)ã€‚
        -   ä¾‹å¦‚ï¼šçŒ«å¯ä»¥æ‘†å‡ºå„ç§å§¿åŠ¿ (e.g., cats in various poses)ã€‚
    -   **é®æŒ¡ (Occlusion)**
        -   ç‰©ä½“åœ¨å›¾åƒä¸­å¯èƒ½å‡ ä¹ä¸å¯è§ (The object we want to recognize might not be visible hardly at all)ã€‚
        -   è¯†åˆ«éœ€è¦å¤§é‡çš„å¸¸è¯†æ€§æ¨ç† (Recognition involves common-sense reasoning about the world, e.g., a tail sticking out from under a couch)ã€‚

-   **å›¾åƒåˆ†ç±»ï¼šéå¸¸æœ‰ç”¨ï¼(Image Classification: Very Useful!)**
    -   **ç§‘å­¦åº”ç”¨ (Scientific Applications)**
        -   åŒ»å­¦æˆåƒ (Medical Imaging): è¯Šæ–­è‰¯æ€§/æ¶æ€§è‚¿ç˜¤ (diagnosing benign/malignant tumors)ã€‚
        -   æ˜Ÿç³»åˆ†ç±» (Galaxy Classification): åˆ†ç±»æœ›è¿œé•œæ•°æ®ä¸­çš„å¤©ä½“ç°è±¡ (classifying celestial phenomena from telescope data)ã€‚
        -   é²¸é±¼è¯†åˆ« (Whale Recognition) åŠå…¶ä»–åŠ¨ç‰©åˆ†ç±» (and other animal classification)ã€‚
    -   **ä½œä¸ºå…¶ä»–ä»»åŠ¡çš„åŸºç¡€æ¨¡å— (Building Block for Other Tasks)**
        -   **ç›®æ ‡æ£€æµ‹ (Object Detection)**: ç»˜åˆ¶è¾¹ç•Œæ¡†å¹¶åˆ†ç±»å›¾åƒä¸­çš„ç‰©ä½“ (Draw boxes around objects and classify them)ã€‚
        -   **å›¾åƒæ ‡æ³¨ (Image Captioning)**: ç»™å®šè¾“å…¥å›¾åƒï¼Œç¼–å†™è‡ªç„¶è¯­è¨€å¥å­æè¿°å›¾åƒå†…å®¹ (Given an input image, write a natural language sentence to describe what is in the image)ã€‚
        -   **ç©å›´æ£‹ (Playing Go)**: è¾“å…¥æ˜¯æ£‹ç›˜å›¾åƒï¼Œè¾“å‡ºæ˜¯ä¸‹ä¸€ä¸ªè½å­çš„ä½ç½® (Input is an image of the game board, output is where to play the next stone)ã€‚

-   **å›¾åƒåˆ†ç±»å™¨ (An Image Classifier)**
    -   ä¸åƒå¯¹æ•°å­—åˆ—è¡¨è¿›è¡Œæ’åºï¼Œæ²¡æœ‰æ˜æ˜¾çš„æ–¹æ³•æ¥ç¡¬ç¼–ç è¯†åˆ«çŒ«æˆ–å…¶ä»–ç±»åˆ«çš„ç®—æ³• (Unlike sorting a list of numbers, there's no obvious way to hard-code the algorithm for recognizing a cat, or other classes)ã€‚
    -   **ä¼ ç»Ÿæ–¹æ³•å°è¯• (You could try...)**
        -   å¯»æ‰¾è¾¹ç¼˜ (Find edges)ã€‚
        -   å¯»æ‰¾è§’ç‚¹ (Find corners)ã€‚
        -   ç¡¬ç¼–ç è§„åˆ™ (Hard-code rules)ã€‚
        -   è¿™ç§æ–¹æ³•å¾ˆâ€œè„†å¼±â€ä¸”ä¸å¯æ‰©å±• (This approach is "brittle" and not scalable)ã€‚
    -   **æœºå™¨å­¦ä¹ ï¼šæ•°æ®é©±åŠ¨æ–¹æ³• (Machine Learning: Data-Driven Approach)**
        1.  æ”¶é›†å›¾åƒå’Œæ ‡ç­¾çš„æ•°æ®é›† (Collect a dataset of images and labels)ã€‚
        2.  ä½¿ç”¨æœºå™¨å­¦ä¹ è®­ç»ƒåˆ†ç±»å™¨ (Use Machine Learning to train a classifier)ã€‚
        3.  åœ¨æ–°å›¾åƒä¸Šè¯„ä¼°åˆ†ç±»å™¨ (Evaluate the classifier on new images)ã€‚
        -   è¿™ç§æ–¹æ³•é€šè¿‡æ•°æ®æ¥â€œç¼–ç¨‹â€è®¡ç®—æœº (Program the computer via the data)ã€‚

-   **å›¾åƒåˆ†ç±»æ•°æ®é›† (Image Classification Datasets)**
    -   **MNIST**: 10ä¸ªç±»åˆ«ï¼šæ•°å­—0åˆ°9 (Digits 0 to 9)ï¼›28x28ç°åº¦å›¾åƒ (28x28 grayscale images)ï¼›5ä¸‡å¼ è®­ç»ƒå›¾åƒ (50k training images), 1ä¸‡å¼ æµ‹è¯•å›¾åƒ (10k test images)ã€‚
    -   **CIFAR-10**: 10ä¸ªç±»åˆ«ï¼šé£æœºã€æ±½è½¦ã€é¸Ÿã€çŒ«ã€é¹¿ã€ç‹—ã€é’è›™ã€é©¬ã€èˆ¹ã€å¡è½¦ (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)ï¼›32x32 RGBå›¾åƒ (32x32 RGB images)ï¼›5ä¸‡å¼ è®­ç»ƒå›¾åƒ (5k per class), 1ä¸‡å¼ æµ‹è¯•å›¾åƒ (1k per class)ã€‚
    -   **CIFAR-100**: 100ä¸ªç±»åˆ« (100 classes)ï¼›32x32 RGBå›¾åƒ (32x32 RGB images)ï¼›5ä¸‡å¼ è®­ç»ƒå›¾åƒ (500 per class), 1ä¸‡å¼ æµ‹è¯•å›¾åƒ (100 per class)ï¼›æœ‰20ä¸ªè¶…ç±»åˆ«ï¼Œæ¯ä¸ªåŒ…å«5ä¸ªå­ç±»åˆ« (20 superclasses with 5 classes each)ã€‚
    -   **ImageNet**: 1000ä¸ªç±»åˆ« (1000 classes)ï¼›çº¦130ä¸‡å¼ è®­ç»ƒå›¾åƒ (~1.3M training images), 5ä¸‡å¼ éªŒè¯å›¾åƒ (50K validation images), 10ä¸‡å¼ æµ‹è¯•å›¾åƒ (100K test images)ï¼›æ€§èƒ½æŒ‡æ ‡ (Performance metric): Top-5 å‡†ç¡®ç‡ (Top-5 accuracy)ã€‚
    -   **MIT Places**: 365ä¸ªä¸åŒåœºæ™¯ç±»å‹ (365 classes of different scene types)ï¼›çº¦800ä¸‡å¼ è®­ç»ƒå›¾åƒ (8M training images), 1.825ä¸‡å¼ éªŒè¯å›¾åƒ (50 per class), 32.85ä¸‡å¼ æµ‹è¯•å›¾åƒ (900 per class)ã€‚
    -   **Omniglot**: 1623ä¸ªç±»åˆ«ï¼šæ¥è‡ª50ç§ä¸åŒå­—æ¯è¡¨çš„å­—ç¬¦ (1623 categories: characters from 50 different alphabets)ï¼›æ¯ä¸ªç±»åˆ«20å¼ å›¾åƒ (20 images per category)ï¼›æ—¨åœ¨æµ‹è¯•å°‘æ ·æœ¬å­¦ä¹  (Meant to test few-shot learning)ã€‚

-   **ç¬¬ä¸€ä¸ªåˆ†ç±»å™¨ï¼šæœ€è¿‘é‚» (First Classifier: Nearest Neighbor)**
    -   **è®­ç»ƒå‡½æ•° (train function)**
        -   è®­ç»ƒé€Ÿåº¦ (Training speed): O(1) (å¸¸æ•°æ—¶é—´)ã€‚
    -   **é¢„æµ‹å‡½æ•° (predict function)**
        -   é¢„æµ‹é€Ÿåº¦ (Testing speed): O(N) (çº¿æ€§æ—¶é—´)ã€‚
        -   **é—®é¢˜ (Problem)**: è®­ç»ƒå¿«ä½†æµ‹è¯•æ…¢æ˜¯ç³Ÿç³•çš„ï¼æˆ‘ä»¬éœ€è¦å¿«é€Ÿæµ‹è¯•ï¼(This is bad: We can afford slow training, but we need fast testing!)ã€‚
        -   å­˜åœ¨è®¸å¤šç”¨äºå¿«é€Ÿ/è¿‘ä¼¼æœ€è¿‘é‚»çš„æ–¹æ³• (There are many methods for fast / approximate nearest neighbors)ã€‚

-   **æœ€è¿‘é‚»åˆ†ç±»å™¨æ•ˆæœå¦‚ä½•ï¼Ÿ(What does Nearest Neighbor look like?)**
    -   **L1 è·ç¦»æ¥æ¯”è¾ƒå›¾åƒ (L1 Distance to Compare Images)**
        -   è®¡ç®—å¯¹åº”åƒç´ çš„ç»å¯¹å€¼å·®ä¹‹å’Œ ($d_1(I_1, I_2) = \sum_p |I_1^p - I_2^p|$ )ã€‚
    -   **è§†è§‰ç›¸ä¼¼æ€§ä¸è¯­ä¹‰ç›¸ä¼¼æ€§ (Visual vs. Semantic Similarity)**
        -   æœ€è¿‘é‚»å¾€å¾€æ˜¯è§†è§‰ä¸Šéå¸¸ç›¸ä¼¼çš„å›¾åƒ (Nearest neighbors tend to be very visually similar images)ã€‚
        -   ä½†è§†è§‰ç›¸ä¼¼æ€§å¹¶ä¸æ€»æ˜¯æ„å‘³ç€è¯­ä¹‰ç›¸ä¼¼æ€§ (But visual similarity does not always mean semantic similarity)ã€‚
        -   ä¾‹å¦‚ï¼šä¸€ä¸ªæ©™è‰²æ–‘ç‚¹å¯èƒ½æ˜¯é’è›™ï¼Œä½†å…¶æœ€è¿‘é‚»å¯èƒ½æ˜¯ä¸€åªçŒ« (e.g., an orange blob that is a frog, its nearest neighbor is a cat)ã€‚
    -   **æœ€è¿‘é‚»å†³ç­–è¾¹ç•Œ (Nearest Neighbor Decision Boundaries)**
        -   ç‚¹æ˜¯è®­ç»ƒæ ·æœ¬ï¼›é¢œè‰²ä»£è¡¨è®­ç»ƒæ ‡ç­¾ (Points are training examples; colors give training labels)ã€‚
        -   èƒŒæ™¯é¢œè‰²ä»£è¡¨æµ‹è¯•ç‚¹å°†è¢«åˆ†é…çš„ç±»åˆ« (Background colors give the category a test point would be assigned)ã€‚
        -   å†³ç­–è¾¹ç•Œ (Decision boundary): ä¸¤ä¸ªåˆ†ç±»åŒºåŸŸä¹‹é—´çš„è¾¹ç•Œ (Boundary between two classification regions)ã€‚
        -   å†³ç­–è¾¹ç•Œå¯èƒ½å˜ˆæ‚ï¼Œå—ç¦»ç¾¤ç‚¹å½±å“ (Decision boundaries can be noisy; affected by outliers)ã€‚
        -   å¦‚ä½•å¹³æ»‘å†³ç­–è¾¹ç•Œï¼Ÿä½¿ç”¨æ›´å¤šçš„é‚»å±…ï¼(How to smooth out decision boundaries? Use more neighbors!)ã€‚

-   **K-æœ€è¿‘é‚» (K-Nearest Neighbors)**
    -   ä¸å¤åˆ¶æœ€è¿‘é‚»çš„æ ‡ç­¾ï¼Œè€Œæ˜¯ä» K ä¸ªæœ€è¿‘ç‚¹ä¸­å–å¤šæ•°ç¥¨ (Instead of copying label from nearest neighbor, take majority vote from K closest points)ã€‚
    -   K=1 (åŸå§‹æœ€è¿‘é‚») vs. K=3 (æ›´å¹³æ»‘çš„å†³ç­–è¾¹ç•Œï¼Œå—å™ªå£°å½±å“å°) (K=1 (original nearest neighbor) vs. K=3 (smoother boundaries, less affected by noise))ã€‚
    -   å½“ K > 1 æ—¶ï¼Œç±»åˆ«ä¹‹é—´å¯èƒ½å­˜åœ¨å¹³å±€ï¼Œéœ€è¦æŸç§æ–¹æ³•æ¥æ‰“ç ´å¹³å±€ (When K > 1 there can be ties between classes. Need to break somehow!)ã€‚
    -   **è·ç¦»åº¦é‡ (Distance Metric)**
        -   L1 (æ›¼å“ˆé¡¿) è·ç¦» (L1 (Manhattan) distance): $d_1(I_1, I_2) = \sum_p |I_1^p - I_2^p|$
        -   L2 (æ¬§å‡ é‡Œå¾—) è·ç¦» (L2 (Euclidean) distance): $d_2(I_1, I_2) = \sqrt{\sum_p (I_1^p - I_2^p)^2}$
        -   é€šè¿‡é€‰æ‹©åˆé€‚çš„è·ç¦»åº¦é‡ï¼Œæˆ‘ä»¬å¯ä»¥å°† K-æœ€è¿‘é‚»åº”ç”¨äºä»»ä½•ç±»å‹çš„æ•°æ®ï¼(With the right choice of distance metric, we can apply K-Nearest Neighbor to any type of data!)ã€‚
        -   ç¤ºä¾‹ï¼šä½¿ç”¨tf-idfç›¸ä¼¼åº¦æ¯”è¾ƒç ”ç©¶è®ºæ–‡ (Example: Compare research papers using tf-idf similarity)ã€‚

-   **è¶…å‚æ•° (Hyperparameters)**
    -   ä»€ä¹ˆæ˜¯æœ€ä½³çš„ K å€¼ï¼Ÿ(What is the best value of K to use?)ã€‚
    -   ä»€ä¹ˆæ˜¯æœ€ä½³çš„è·ç¦»åº¦é‡ï¼Ÿ(What is the best distance metric to use?)ã€‚
    -   è¿™äº›æ˜¯è¶…å‚æ•°çš„ä¾‹å­ï¼šå…³äºæˆ‘ä»¬å­¦ä¹ ç®—æ³•çš„é€‰æ‹©ï¼Œæˆ‘ä»¬ä¸ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ ï¼›ç›¸åï¼Œæˆ‘ä»¬åœ¨å­¦ä¹ è¿‡ç¨‹å¼€å§‹æ—¶è®¾ç½®å®ƒä»¬ (These are examples of hyperparameters: choices about our learning algorithm that we don't learn from the training data; instead we set them at the start of the learning process)ã€‚
    -   å®ƒä»¬éå¸¸ä¾èµ–äºé—®é¢˜ (Very problem-dependent)ã€‚
    -   é€šå¸¸éœ€è¦å°è¯•æ‰€æœ‰è¿™äº›æ–¹æ³•ï¼Œçœ‹çœ‹å“ªç§æœ€é€‚åˆæˆ‘ä»¬çš„æ•°æ®/ä»»åŠ¡ (In general need to try them all and see what works best for our data / task)ã€‚

-   **è®¾ç½®è¶…å‚æ•° (Setting Hyperparameters)**
    -   **æƒ³æ³• #1**: é€‰æ‹©åœ¨æ•°æ®ä¸Šè¡¨ç°æœ€å¥½çš„è¶…å‚æ•° (Idea #1: Choose hyperparameters that work best on the data)ã€‚
        -   **ä¸å¥½**: K=1 æ€»æ˜¯èƒ½åœ¨è®­ç»ƒæ•°æ®ä¸Šå®Œç¾è¿è¡Œ (BAD: K=1 always works perfectly on training data)ã€‚
        -   è¿™ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå¯¹æ–°æ•°æ®æ²¡æœ‰æ³›åŒ–èƒ½åŠ› (This leads to overfitting and no generalization to new data)ã€‚
    -   **æƒ³æ³• #2**: å°†æ•°æ®åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œé€‰æ‹©åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°æœ€å¥½çš„è¶…å‚æ•° (Idea #2: Split data into train and test, choose hyperparameters that work best on test data)ã€‚
        -   **ä¸å¥½**: ä¸çŸ¥é“ç®—æ³•åœ¨æ–°æ•°æ®ä¸Šå°†å¦‚ä½•è¡¨ç° (BAD: No idea how algorithm will perform on new data)ã€‚
        -   å› ä¸ºæˆ‘ä»¬å·²ç»ä½¿ç”¨æµ‹è¯•é›†æ¥é€‰æ‹©è¶…å‚æ•°ï¼Œæµ‹è¯•é›†ä¸å†æ˜¯â€œæœªè§è¿‡â€çš„æ•°æ® (Because we used the test set to select hyperparameters, it is no longer unseen data)ã€‚
        -   è¿™æ˜¯ä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ä¸­â€œä½œå¼Šâ€çš„å¸¸è§é”™è¯¯ (This is a fundamental cardinal sin in machine learning models)ã€‚
    -   **æƒ³æ³• #3**: å°†æ•°æ®åˆ†å‰²ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼›åœ¨éªŒè¯é›†ä¸Šé€‰æ‹©è¶…å‚æ•°ï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼° (Idea #3: Split data into train, val, and test; choose hyperparameters on val and evaluate on test)ã€‚
        -   **æ›´å¥½ï¼(Better!)**
        -   è®­ç»ƒé›† (train): ç”¨äºè®­ç»ƒæ¨¡å‹ (Used to train the model)ã€‚
        -   éªŒè¯é›† (validation): ç”¨äºé€‰æ‹©è¶…å‚æ•° (Used to select hyperparameters)ã€‚
        -   æµ‹è¯•é›† (test): ä»…åœ¨æ‰€æœ‰å†³ç­–å®Œæˆåä½¿ç”¨ä¸€æ¬¡ (Used only once at the very end to evaluate the final model)ã€‚
    -   **æƒ³æ³• #4**: äº¤å‰éªŒè¯ (Cross-Validation)ã€‚å°†æ•°æ®åˆ†å‰²æˆå¤šä¸ªæŠ˜å  (folds)ï¼Œå°†æ¯ä¸ªæŠ˜å ä½œä¸ºéªŒè¯é›†è¿›è¡Œå°è¯•ï¼Œå¹¶å¹³å‡ç»“æœ (Split data into folds, try each fold as validation and average the results)ã€‚
        -   å¯¹å°å‹æ•°æ®é›†æœ‰ç”¨ (Useful for small datasets)ã€‚
        -   ä½†åœ¨æ·±åº¦å­¦ä¹ ä¸­ä¸å¹¸æ²¡æœ‰è¢«é¢‘ç¹ä½¿ç”¨ (But unfortunately not used too frequently in deep learning)ã€‚
        -   **ç¤ºä¾‹**: K å€¼çš„5æŠ˜äº¤å‰éªŒè¯ (Example of 5-fold cross-validation for the value of k)ã€‚
            -   æ¯ä¸ªç‚¹ï¼šå•ä¸ªç»“æœ (Each point: single outcome)ã€‚
            -   çº¿ç©¿è¿‡å¹³å‡å€¼ï¼Œæ¡å½¢è¡¨ç¤ºæ ‡å‡†å·® (The line goes through the mean, bars indicated standard deviation)ã€‚
            -   (ä¼¼ä¹ K~7 å¯¹æ­¤æ•°æ®æ•ˆæœæœ€å¥½) (Seems that K~7 works best for this data)ã€‚

-   **K-æœ€è¿‘é‚»ï¼šé€šç”¨é€¼è¿‘ (K-Nearest Neighbor: Universal Approximation)**
    -   éšç€è®­ç»ƒæ ·æœ¬æ•°é‡è¶‹äºæ— ç©·å¤§ï¼Œæœ€è¿‘é‚»å¯ä»¥è¡¨ç¤ºä»»ä½•(*)å‡½æ•°ï¼(As the number of training samples goes to infinity, nearest neighbor can represent any(*) function!)ã€‚
    -   (*) é¡»ç¬¦åˆè®¸å¤šæŠ€æœ¯æ¡ä»¶ã€‚ä»…åœ¨ç´§å‡‘åŸŸä¸Šçš„è¿ç»­å‡½æ•°ï¼›éœ€è¦å¯¹è®­ç»ƒç‚¹çš„é—´è·ç­‰åšå‡ºå‡è®¾ (Subject to many technical conditions. Only continuous functions on a compact domain; need to make assumptions about spacing of training points; etc.)ã€‚
    -   **é—®é¢˜ï¼šç»´åº¦è¯…å’’ (Problem: Curse of Dimensionality)**
        -   ä¸ºäº†å‡åŒ€è¦†ç›–ç©ºé—´ï¼Œæ‰€éœ€çš„è®­ç»ƒç‚¹æ•°é‡éšç»´åº¦å‘ˆæŒ‡æ•°å¢é•¿ (For uniform coverage of space, number of training points needed grows exponentially with dimension)ã€‚
        -   ç»´åº¦ = 1ï¼Œç‚¹ = 4 (Dimensions = 1, Points = 4)ã€‚
        -   ç»´åº¦ = 2ï¼Œç‚¹ = $4^2$ (Dimensions = 2, Points = $4^2$)ã€‚
        -   ç»´åº¦ = 3ï¼Œç‚¹ = $4^3$ (Dimensions = 3, Points = $4^3$)ã€‚
        -   32x32 äºŒå€¼å›¾åƒçš„å¯èƒ½æ•°é‡çº¦ä¸º $2^{32 \times 32} \approx 10^{308}$ (Number of possible 32x32 binary images: $2^{32 \times 32} \approx 10^{308}$)ã€‚
        -   å¯è§å®‡å®™ä¸­åŸºæœ¬ç²’å­çš„æ•°é‡çº¦ä¸º $10^{97}$ (Number of elementary particles in the visible universe: $\approx 10^{97}$)ã€‚
        -   è¿™æ„å‘³ç€æˆ‘ä»¬æ°¸è¿œæ— æ³•æ”¶é›†è¶³å¤Ÿçš„æ•°æ®æ¥å¯†é›†è¦†ç›–æ•´ä¸ªå›¾åƒç©ºé—´ (This means we can never collect enough data to densely cover the entire space of images)ã€‚
    -   **K-æœ€è¿‘é‚»åœ¨åŸå§‹åƒç´ ä¸Šå¾ˆå°‘ä½¿ç”¨ (K-Nearest Neighbor on raw pixels is seldom used)**
        -   åœ¨æµ‹è¯•æ—¶éå¸¸æ…¢ (Very slow at test time)ã€‚
        -   åƒç´ ä¸Šçš„è·ç¦»åº¦é‡ä¸å…·æœ‰ä¿¡æ¯æ€§ (Distance metrics on pixels are not informative)ã€‚
            -   åŸå§‹å›¾åƒä¸ä¿®æ”¹åçš„å›¾åƒåœ¨ L2 è·ç¦»ä¸Šå¯èƒ½ç›¸åŒï¼Œä½†è¯­ä¹‰ä¸Šå·®å¼‚å·¨å¤§ (Original image vs. boxed/shifted/tinted images have same L2 distance but are semantically different)ã€‚
        -   **æœ€è¿‘é‚»ä¸ ConvNet ç‰¹å¾ç»“åˆæ•ˆæœè‰¯å¥½ï¼(Nearest Neighbor with ConvNet features works well!)**
            -   ç¤ºä¾‹ï¼šå›¾åƒæ ‡æ³¨ä¸æœ€è¿‘é‚» (Example: Image Captioning with Nearest Neighbor)ã€‚
            -   é€šè¿‡æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ (ConvNet) æå–çš„ç‰¹å¾å‘é‡èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å›¾åƒçš„è¯­ä¹‰ç›¸ä¼¼æ€§ (Feature vectors computed from deep ConvNets can capture semantic similarity better)ã€‚

### äºŒã€å…³é”®æœ¯è¯­å®šä¹‰ (Key Term Definitions)

-   **å›¾åƒåˆ†ç±» (Image Classification)**: å°†ç»™å®šå›¾åƒåˆ†é…åˆ°é¢„å®šä¹‰çš„å›ºå®šç±»åˆ«é›†åˆä¸­çš„ä¸€ä¸ªè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚
-   **è¯­ä¹‰é¸¿æ²Ÿ (Semantic Gap)**: æŒ‡è®¡ç®—æœºå¤„ç†çš„åŸå§‹åƒç´ æ•°æ®ä¸äººç±»å¯¹å›¾åƒçš„è¯­ä¹‰ç†è§£ä¹‹é—´çš„å·®å¼‚ã€‚
-   **è§†ç‚¹å˜åŒ– (Viewpoint Variation)**: ç”±äºç›¸æœºè§’åº¦ã€ä½ç½®ç­‰å˜åŒ–å¯¼è‡´çš„åŒä¸€ç‰©ä½“åœ¨ä¸åŒå›¾åƒä¸­åƒç´ å€¼å·®å¼‚å¤§çš„é—®é¢˜ã€‚
-   **ç±»å†…å·®å¼‚ (Intraclass Variation)**: åŒä¸€ç±»åˆ«å†…ä¸åŒä¸ªä½“ï¼ˆä¾‹å¦‚ä¸åŒå“ç§çš„çŒ«ï¼‰åœ¨è§†è§‰å¤–è§‚ä¸Šå­˜åœ¨çš„å·¨å¤§å·®å¼‚ã€‚
-   **ç»†ç²’åº¦ç±»åˆ« (Fine-Grained Categories)**: è§†è§‰ä¸Šéå¸¸ç›¸ä¼¼ä½†å±äºä¸åŒå­ç±»åˆ«çš„ç‰©ä½“ï¼Œå¦‚ä¸åŒå“ç§çš„çŒ«æˆ–ç‹—ã€‚
-   **èƒŒæ™¯æ‚ä¹± (Background Clutter)**: å›¾åƒä¸­ç‰©ä½“ä¸èƒŒæ™¯èåˆï¼Œæˆ–èƒŒæ™¯å…ƒç´ å¹²æ‰°ç‰©ä½“è¯†åˆ«çš„æƒ…å†µã€‚
-   **å…‰ç…§å˜åŒ– (Illumination Changes)**: åœºæ™¯å…‰ç…§æ¡ä»¶æ”¹å˜å¯¼è‡´å›¾åƒåƒç´ å€¼å‘ç”Ÿå·¨å¤§å˜åŒ–ï¼Œä½†ç‰©ä½“æœ¬èº«è¯­ä¹‰ä¸å˜ã€‚
-   **å½¢å˜ (Deformation)**: ç‰©ä½“ä»¥ä¸åŒå§¿æ€æˆ–å½¢çŠ¶å‡ºç°åœ¨å›¾åƒä¸­ï¼Œä¿æŒå…¶ç±»åˆ«ä½†è§†è§‰è¡¨ç°å¤šæ ·ã€‚
-   **é®æŒ¡ (Occlusion)**: ç‰©ä½“éƒ¨åˆ†è¢«å…¶ä»–ç‰©ä½“é®æŒ¡ï¼Œå¯¼è‡´å…¶åœ¨å›¾åƒä¸­ä¸å®Œå…¨å¯è§ã€‚
-   **ç›®æ ‡æ£€æµ‹ (Object Detection)**: è¯†åˆ«å›¾åƒä¸­ç‰©ä½“çš„ä½ç½®ï¼ˆé€šå¸¸ç”¨è¾¹ç•Œæ¡†è¡¨ç¤ºï¼‰å¹¶åˆ†ç±»å®ƒä»¬ã€‚
-   **å›¾åƒæ ‡æ³¨ (Image Captioning)**: æ ¹æ®å›¾åƒå†…å®¹ç”Ÿæˆä¸€æ®µæè¿°æ€§çš„è‡ªç„¶è¯­è¨€æ–‡æœ¬ã€‚
-   **æ•°æ®é©±åŠ¨æ–¹æ³• (Data-Driven Approach)**: ä¸€ç§æœºå™¨å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡ä»å¤§é‡æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼æ¥è®­ç»ƒæ¨¡å‹ï¼Œè€Œä¸æ˜¯é€šè¿‡ç¡¬ç¼–ç è§„åˆ™ã€‚
-   **MNIST**: ä¸€ä¸ªåŒ…å«æ‰‹å†™æ•°å­—å›¾åƒçš„ç»å…¸å›¾åƒåˆ†ç±»æ•°æ®é›†ï¼Œå¸¸ç”¨äºæœºå™¨å­¦ä¹ ç®—æ³•çš„åˆæ­¥æµ‹è¯•ã€‚
-   **CIFAR-10**: ä¸€ä¸ªåŒ…å«10ä¸ªå¸¸è§ç‰©ä½“ç±»åˆ«çš„å½©è‰²å›¾åƒæ•°æ®é›†ï¼Œæ¯”MNISTæ›´å…·æŒ‘æˆ˜æ€§ã€‚
-   **CIFAR-100**: CIFAR-10çš„æ‰©å±•ç‰ˆæœ¬ï¼ŒåŒ…å«100ä¸ªç±»åˆ«ã€‚
-   **ImageNet**: ä¸€ä¸ªå¤§è§„æ¨¡å›¾åƒæ•°æ®åº“ï¼ŒåŒ…å«æ•°ç™¾ä¸‡å¼ å›¾åƒå’Œæ•°åƒä¸ªç±»åˆ«ï¼Œæ˜¯å›¾åƒåˆ†ç±»ä»»åŠ¡çš„é»„é‡‘æ ‡å‡†åŸºå‡†ã€‚
-   **MIT Places**: ä¸€ä¸ªä¸“æ³¨äºåœºæ™¯è¯†åˆ«çš„å¤§å‹å›¾åƒæ•°æ®é›†ã€‚
-   **Omniglot**: ä¸€ä¸ªæ—¨åœ¨æµ‹è¯•å°‘æ ·æœ¬å­¦ä¹ çš„æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªå¤šç§è¯­è¨€çš„å­—ç¬¦ã€‚
-   **Top-5 å‡†ç¡®ç‡ (Top-5 Accuracy)**: ä¸€ç§è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚æœç®—æ³•å¯¹å›¾åƒé¢„æµ‹çš„å‰5ä¸ªæ ‡ç­¾ä¸­åŒ…å«æ­£ç¡®æ ‡ç­¾ï¼Œåˆ™è®¤ä¸ºé¢„æµ‹æ­£ç¡®ã€‚
-   **å°‘æ ·æœ¬å­¦ä¹  (Few-Shot Learning)**: æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªç ”ç©¶é¢†åŸŸï¼Œæ—¨åœ¨ä½¿ç®—æ³•èƒ½å¤Ÿä»æ¯ä¸ªç±»åˆ«å¾ˆå°‘çš„è®­ç»ƒæ ·æœ¬ä¸­å­¦ä¹ å’Œæ³›åŒ–ã€‚
-   **æœ€è¿‘é‚» (Nearest Neighbor)**: ä¸€ç§ç®€å•çš„åˆ†ç±»ç®—æ³•ï¼Œé€šè¿‡æŸ¥æ‰¾æµ‹è¯•æ ·æœ¬åœ¨è®­ç»ƒé›†ä¸­æœ€ç›¸ä¼¼çš„æ ·æœ¬çš„æ ‡ç­¾æ¥è¿›è¡Œé¢„æµ‹ã€‚
-   **L1 è·ç¦» (L1 Distance)**: ä¹Ÿç§°ä¸ºæ›¼å“ˆé¡¿è·ç¦»ï¼Œç”¨äºæ¯”è¾ƒä¸¤ä¸ªå›¾åƒçš„è·ç¦»ï¼Œè®¡ç®—å¯¹åº”åƒç´ ç»å¯¹å€¼å·®çš„æ€»å’Œã€‚
-   **L2 è·ç¦» (L2 Distance)**: ä¹Ÿç§°ä¸ºæ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œç”¨äºæ¯”è¾ƒä¸¤ä¸ªå›¾åƒçš„è·ç¦»ï¼Œè®¡ç®—å¯¹åº”åƒç´ å·®çš„å¹³æ–¹å’Œçš„å¹³æ–¹æ ¹ã€‚
-   **å†³ç­–è¾¹ç•Œ (Decision Boundary)**: åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä¸åŒç±»åˆ«é¢„æµ‹åŒºåŸŸä¹‹é—´çš„ç•Œé™ã€‚
-   **K-æœ€è¿‘é‚» (K-Nearest Neighbors)**: æœ€è¿‘é‚»ç®—æ³•çš„æ‰©å±•ï¼Œé€šè¿‡ä» K ä¸ªæœ€è¿‘ç‚¹çš„å¤šæ•°ç¥¨æ¥é¢„æµ‹æ ‡ç­¾ï¼Œæœ‰åŠ©äºå¹³æ»‘å†³ç­–è¾¹ç•Œå’Œå‡å°‘ç¦»ç¾¤ç‚¹å½±å“ã€‚
-   **è¶…å‚æ•° (Hyperparameters)**: åœ¨å­¦ä¹ è¿‡ç¨‹å¼€å§‹æ—¶è®¾å®šçš„ç®—æ³•å‚æ•°ï¼Œä¸èƒ½ç›´æ¥ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ ã€‚
-   **éªŒè¯é›† (Validation Set)**: æ•°æ®é›†ä¸­ç”¨äºè°ƒæ•´æ¨¡å‹è¶…å‚æ•°çš„éƒ¨åˆ†ï¼Œç‹¬ç«‹äºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
-   **äº¤å‰éªŒè¯ (Cross-Validation)**: ä¸€ç§æ›´ç¨³å¥çš„è¶…å‚æ•°é€‰æ‹©å’Œæ¨¡å‹è¯„ä¼°æŠ€æœ¯ï¼Œé€šè¿‡å°†æ•°æ®åˆ†å‰²æˆå¤šä¸ªæŠ˜å å¹¶è¿­ä»£ä½¿ç”¨ä¸åŒæŠ˜å ä½œä¸ºéªŒè¯é›†æ¥å¹³å‡ç»“æœã€‚
-   **ç»´åº¦è¯…å’’ (Curse of Dimensionality)**: åœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œä¸ºäº†å‡åŒ€è¦†ç›–ç©ºé—´ï¼Œæ‰€éœ€è®­ç»ƒæ ·æœ¬æ•°é‡å‘ˆæŒ‡æ•°çº§å¢é•¿çš„é—®é¢˜ã€‚
-   **tf-idf (term frequency-inverse document frequency)**: ä¸€ç§ç”¨äºæ–‡æœ¬åˆ†æçš„ç›¸ä¼¼åº¦åº¦é‡ï¼Œé€šè¿‡è¯é¢‘å’Œé€†æ–‡æ¡£é¢‘ç‡æ¥è¯„ä¼°è¯è¯­çš„é‡è¦æ€§ã€‚

### ä¸‰ã€æ ¸å¿ƒç®—æ³•ä¸ä»£ç ç‰‡æ®µ (Core Algorithms & Code Snippets)

-   **æœºå™¨å­¦ä¹ ç³»ç»Ÿé€šç”¨æ¡†æ¶ (General Framework for Machine Learning Systems)**:
    1.  æ”¶é›†å›¾åƒå’Œæ ‡ç­¾çš„æ•°æ®é›† (Collect a dataset of images and labels)ã€‚
    2.  ä½¿ç”¨æœºå™¨å­¦ä¹ è®­ç»ƒåˆ†ç±»å™¨ (Use Machine Learning to train a classifier)ã€‚
    3.  åœ¨æ–°å›¾åƒä¸Šè¯„ä¼°åˆ†ç±»å™¨ (Evaluate the classifier on new images)ã€‚

-   **è®­ç»ƒå‡½æ•°ç­¾å (Train Function Signature)**:
    ```python
    def train(images, labels):
        # æœºå™¨å­¦ä¹ ç®—æ³•çš„æ ¸å¿ƒé€»è¾‘
        # Machine learning!
        return model # è¿”å›è®­ç»ƒå¥½çš„æ¨¡å‹
    ```

-   **é¢„æµ‹å‡½æ•°ç­¾å (Predict Function Signature)**:
    ```python
    def predict(model, test_images):
        # ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
        # Use model to predict labels
        return test_labels # è¿”å›é¢„æµ‹çš„æ ‡ç­¾
    ```

-   **L1 è·ç¦»è®¡ç®— (L1 Distance Calculation)**:
    -   $d_1(I_1, I_2) = \sum_p |I_1^p - I_2^p|$
    -   å°†æµ‹è¯•å›¾åƒå’Œè®­ç»ƒå›¾åƒçš„å¯¹åº”åƒç´ å€¼ç›¸å‡ï¼Œå–ç»å¯¹å€¼ï¼Œç„¶åå°†æ‰€æœ‰ç»“æœç›¸åŠ ã€‚

-   **L2 è·ç¦»è®¡ç®— (L2 Distance Calculation)**:
    -   $d_2(I_1, I_2) = \sqrt{\sum_p (I_1^p - I_2^p)^2}$
    -   å°†æµ‹è¯•å›¾åƒå’Œè®­ç»ƒå›¾åƒçš„å¯¹åº”åƒç´ å€¼ç›¸å‡ï¼Œå–å¹³æ–¹ï¼Œç„¶åå°†æ‰€æœ‰ç»“æœç›¸åŠ ï¼Œæœ€åå¼€å¹³æ–¹ã€‚

-   **æœ€è¿‘é‚»åˆ†ç±»å™¨å®ç° (Nearest Neighbor Classifier Implementation)**:
    ```python
    import numpy as np

    class NearestNeighbor:
        def __init__(self):
            pass

        def train(self, X, y):
            """ X is N x D where each row is an example. Y is 1-dimension of size N
                *** the nearest neighbor classifier simply remembers all the training data ***
            """
            self.Xtr = X
            self.ytr = y

        def predict(self, X):
            """ X is N x D where each row is an example we wish to predict label for """
            num_test = X.shape[0]
            # ç¡®ä¿è¾“å‡ºç±»å‹ä¸è¾“å…¥ç±»å‹åŒ¹é…
            # lets make sure that the output type matches the input type
            Ypred = np.zeros(num_test, dtype=self.ytr.dtype)

            # éå†æ‰€æœ‰æµ‹è¯•è¡Œ
            # loop over all test rows
            for i in xrange(num_test):
                # ä½¿ç”¨L1è·ç¦»(ç»å¯¹å€¼å·®ä¹‹å’Œ)æ‰¾åˆ°ç¬¬iä¸ªæµ‹è¯•å›¾åƒçš„æœ€è¿‘è®­ç»ƒå›¾åƒ
                # find the nearest training image to the i'th test image
                # using the L1 distance (sum of absolute value differences)
                distances = np.sum(np.abs(self.Xtr - X[i, :]), axis=1)
                min_index = np.argmin(distances) # è·å–è·ç¦»æœ€å°çš„ç´¢å¼•
                Ypred[i] = self.ytr[min_index] # é¢„æµ‹æœ€è¿‘é‚»æ ·æœ¬çš„æ ‡ç­¾

            return Ypred
    ```

### å››ã€è®²å¸ˆæå‡ºçš„æ€è€ƒé¢˜ (Questions Posed by the Instructor)

-   æœ‰ N ä¸ªæ ·æœ¬æ—¶ï¼Œè®­ç»ƒé€Ÿåº¦æœ‰å¤šå¿«ï¼Ÿ(With N examples, how fast is training?)
-   æœ‰ N ä¸ªæ ·æœ¬æ—¶ï¼Œæµ‹è¯•é€Ÿåº¦æœ‰å¤šå¿«ï¼Ÿ(With N examples, how fast is testing?)
-   å¦‚ä½•å¹³æ»‘å†³ç­–è¾¹ç•Œï¼Ÿ(How to smooth out decision boundaries?)
-   ä»€ä¹ˆæ˜¯æœ€ä½³çš„ K å€¼ï¼Ÿ(What is the best value of K to use?)
-   ä»€ä¹ˆæ˜¯æœ€ä½³çš„è·ç¦»åº¦é‡ï¼Ÿ(What is the best distance metric to use?)